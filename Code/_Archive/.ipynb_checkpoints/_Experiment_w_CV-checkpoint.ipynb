{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3d67cd-e230-495a-a031-1e5a7ebc4dd7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b518e25-3378-4cde-9fca-4ea5cccd902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Weights Model\n",
    "from WeightsModel import PreProcessing\n",
    "from WeightsModel import MaxQFeatureScaler\n",
    "from WeightsModel import MaxQDemandScaler\n",
    "from WeightsModel import RandomForestWeightsModel\n",
    "\n",
    "# Import Weighted SAA Model\n",
    "from WeightedSAA import WeightedSAA\n",
    "from WeightedSAA import RobustWeightedSAA\n",
    "from WeightedSAA import RollingHorizonOptimization\n",
    "\n",
    "# Import Experiment\n",
    "from Experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8cb12-853b-457f-91ff-8e41687c3862",
   "metadata": {},
   "source": [
    "# Setup the experiment\n",
    "\n",
    "**Experiment data** consists of four data sets.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ of product $j$ at time $n$\n",
    "- **Y_Data** (pd.DataFrame) is the demand time series data, i.e., each row is a demand observations $d_{j,n}$ of product $j$ at time $n$\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets\n",
    "\n",
    "Data is loaded before each experiment using **load_data()**.\n",
    "\n",
    "We run an experiment for all given products (SKUs) $k=1,...,M$ over a test planning horizon $t=1,...,T$ with $T=13$ for three different cost parameter settings $\\{K, u, h, b\\}$ that vary the critical ratio ($CR=\\frac{b}{b+h}$) of holding and backlogging yielding\n",
    "- $CR=0.50$: $\\{K=100, u=0.5, h=1, b=1\\}$\n",
    "- $CR=0.75$: $\\{K=100, u=0.5, h=1, b=3\\}$\n",
    "- $CR=0.90$: $\\{K=100, u=0.5, h=1, b=9\\}$\n",
    "\n",
    "Experiments are run for different choices of the look-ahead $\\tau=0,...,4$. For robust models, we vary the parameter $e=\\{1,3,6,9,12\\}$ (multiplier of the in-sample standard deviation of demand) defining product and sample sepcific uncertainty sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0bd36-7aed-491c-a705-cd2c4812a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "experiment_setup = dict(\n",
    "\n",
    "    # Set paths\n",
    "    path_data = '/home/fesc/DDDInventoryControl/Data',\n",
    "    path_weightsmodel = '/home/fesc/DDDInventoryControl/Data/WeightsModel',\n",
    "    path_results = '/home/fesc/DDDInventoryControl/Data/Results',\n",
    "    \n",
    "    # Weights models\n",
    "    global_weightsmodel = 'rfwm_global', \n",
    "    local_weightsmodel = 'rfwm_local', \n",
    "\n",
    "    # Optimization models\n",
    "    GwSAA = 'GwSAA',\n",
    "    GwSAAR = 'GwSAAR',\n",
    "    wSAA = 'wSAA',\n",
    "    wSAAR = 'wSAAR',\n",
    "    SAA = 'SAA',\n",
    "    ExPost = 'ExPost',\n",
    "    \n",
    "    # Set identifier of start period of testing horizon\n",
    "    timePeriodsTestStart = 114,\n",
    "\n",
    "    # Set product identifiers\n",
    "    products = range(1,460+1),   # Products (SKUs) k=1,...,M\n",
    "    \n",
    "    # Set problem params\n",
    "    T = 13,             # Planning horizon T\n",
    "    ts = range(1,13+1), # Periods t=1,...,T of the planning horizon\n",
    "    taus = [0,1,2,3,4], # Look-aheads tau=0,...,4\n",
    "    es = [1,3,6,9,12],  # Uncertainty set specifications e=1,...,12\n",
    "       \n",
    "    # Set cost params\n",
    "    cost_params = [\n",
    "        {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "    ],\n",
    "    \n",
    "    # Set gurobi params\n",
    "    gurobi_params = {\n",
    "    \n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 1, \n",
    "        'NonConvex': 2, \n",
    "        'PSDTol': 1e-3, # 0.1%\n",
    "        'MIPGap': 1e-3, # 0.1%\n",
    "        'NumericFocus': 0, \n",
    "        'obj_improvement': 1e-3, # 0.1%\n",
    "        'obj_timeout_sec': 3*60, # 3 min\n",
    "        'obj_timeout_max_sec': 10*60, # 10 min\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Global weights model params\n",
    "    global_weightsmodel_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'oob_score': True,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'n_estimators': [500, 1000],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [x for x in range(20, 100, 20)],  \n",
    "            'min_samples_leaf': [x for x in range(10, 100, 10)],  \n",
    "            'max_features': [x for x in range(8, 136, 8)],   \n",
    "            'max_leaf_nodes': [None],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "        },    \n",
    "\n",
    "        # Tuning params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 100,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "        # Tuning using random search\n",
    "        random_search = True\n",
    "    ),\n",
    "    \n",
    "    # Local weights model params\n",
    "    local_weightsmodel_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'oob_score': True,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 1,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'n_estimators': [25, 50],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [x for x in range(2, 20, 2)],  \n",
    "            'min_samples_leaf': [x for x in range(2, 10, 2)],  \n",
    "            'max_features': [x for x in range(4, 96, 4)],   \n",
    "            'max_leaf_nodes': [None],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "        },    \n",
    "\n",
    "        # Tuning params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 100,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 32,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "        # Tuning using random search\n",
    "        random_search = True\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)\n",
    "\n",
    "# Initialize Experiment\n",
    "experiment = Experiment(**experiment_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137c0ad-6243-4390-bd1c-6aff2a2fca74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Training and Samping\n",
    "\n",
    "The two global models (using 'Global Training and Sampling') are **Rolling Horizon Global Weighted SAA (GwSAA)**, which is our model, and **Rolling Horizon Global Robust Weighted SAA (GwSAA-R)**, which is the analogous model with robust extension. Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'global' distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$.\n",
    "\n",
    "We first load **global** experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline. Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly. Furthermore, features and demands are scaled for training (and later rescaled for sampling). \n",
    "\n",
    "The weights models - and thus the data used, weight functions, and weights per sample - are the same for the two global models **GwSAA** and **GwSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor). Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028da40-43c6-499d-a378-e9d5de6e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, ids_products, ids_timePeriods, features, features_to_scale, features_to_scale_with = experiment.load_data('global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed38d5-839e-4f71-b875-b524656c3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module of the weights model\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7822ac-eb99-4ca4-b702-593f98563c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:  \n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "    # Preprocess data for global models\n",
    "    data = pp.preprocess_weightsmodel_data(X, y, ids_products, ids_timePeriods, timePeriodsTestStart, tau, T, \n",
    "                                           features, features_to_scale, features_to_scale_with, \n",
    "                                           X_scaler=MaxQFeatureScaler(q_outlier=0.975), \n",
    "                                           y_scaler=MaxQDemandScaler(q_outlier=0.975))\n",
    "\n",
    "    # Save\n",
    "    _ = dump(data, path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bcee6-737b-4ac0-b412-837607c2edfb",
   "metadata": {},
   "source": [
    "## Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the global random forest weights model, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b34c5a-0164-4e8e-a20e-78e2d07bb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meta params for tuning global weights model available\n",
    "locals().update(experiment_setup['global_weightsmodel_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9828a1-0293-4bdb-949d-fb5440c0b446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "       \n",
    "    # Using t=1 (i.e, data available before start of testing)\n",
    "    t=1\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Extract preprocessed data\n",
    "    locals().update(data[t])\n",
    "    \n",
    "    # Initialize\n",
    "    pp = PreProcessing()\n",
    "    wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "    # Scale features and demands\n",
    "    X_train = pp.scale(X_train, X_scalers, ids_products_train)\n",
    "    y_train = pp.scale(y_train, y_scalers, ids_products_train)   \n",
    "\n",
    "    # CV time series splits\n",
    "    cv_folds = pp.split_timeseries_cv(n_splits=3, ids_timePeriods=ids_timePeriods_train)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status=True)\n",
    "    wm.save_cv_result(path=path_weightsmodel+'/'+global_weightsmodel+'_cv_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fff30-5122-4776-a507-85b29f06ecae",
   "metadata": {},
   "source": [
    "## Fit weight functions and generate weights\n",
    "\n",
    "We now fit the global random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$ and over periods $t=1,...,T$. This is done across all products at once (global training). Then, for each $\\tau=0,...,4$ and over periods $t=1,...,T$, we generate for each product (SKU) $k=1,...,M$ the weights given the test feature $x_{k,t}$. This is done *jointly* across products (by using $x_{t}=(x_{1,t},...,x_{M,t})^{\\top}$) for computational efficiency - the weights for each individual product can be extracted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac397b5-6cd1-4b43-82c6-92cc48459b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b67c0f-7c50-4b19-a292-9fcdfc8de6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "\n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "        \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "        \n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[t])\n",
    "            \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # Scale features and demands\n",
    "        X_train, X_test = pp.scale(X_train, X_scalers, ids_products_train), pp.scale(X_test, X_scalers, ids_products_test)\n",
    "        y_train, y_test = pp.scale(y_train, y_scalers, ids_products_train), pp.scale(y_test, y_scalers, ids_products_test)           \n",
    "            \n",
    "        # Load tuned weights model\n",
    "        wm.load_cv_result(path=path_weightsmodel+'/'+global_weightsmodel+'_cv_tau'+str(tau_)+'.joblib')\n",
    "        \n",
    "        # Fit weight functions  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        wm.fit(X_train, y_train)\n",
    "        weightfunctions_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "      \n",
    "        # Generate weights  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        weights[t] = wm.apply(X_train, X_test)\n",
    "        weights_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "        \n",
    "    # Status\n",
    "    print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+global_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+global_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d357c25-392d-4686-be11-9fd8a580f007",
   "metadata": {},
   "source": [
    "# Local Training and Sampling\n",
    "\n",
    "The two local models (using 'Local Training and Sampling') are **Rolling Horizon Local Weighted SAA (wSAA)**, and **Rolling Horizon Local Robust Weighted SAA (wSAA-R)**, which is the analogous model with robust extension. Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'local' distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$ for each product $k=1,...,M$ separately.\n",
    "\n",
    "We first load **local** experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline. Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly.\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two local models **wSAA** and **wSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor) and for each product (SKU) $k=1,...,M$ separately. Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) for each product (SKU) $k=1,...,M$ separately and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$ separatey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a7aac-8290-4df0-b5ed-162e92009e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, ids_products, ids_timePeriods, features = experiment.load_data('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e7a4-9bee-40f2-ad65-084f2b229d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module of the weights model\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc86c4d-52ea-4c13-ad2a-3a21be3f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:  \n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "    # Preprocess data for local models\n",
    "    data = pp.preprocess_weightsmodel_data(X, y, ids_products, ids_timePeriods, timePeriodsTestStart, tau, T, products=products)\n",
    "\n",
    "    # Save\n",
    "    _ = dump(data, path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0442f-cdaa-47ad-a691-4b2aac018b30",
   "metadata": {},
   "source": [
    "## Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the local random forest weights model for each product (SKU) $k=1,...,M$, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe32a6c-8a55-4342-9017-32c908399737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meta params for tuning global weights model available\n",
    "locals().update(experiment_setup['local_weightsmodel_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710edda0-23b8-4e8e-8c19-7163d9456b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "    # Initialize\n",
    "    cv_results = {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for product in products:\n",
    "\n",
    "        # Using t=1 (i.e, data available before start of testing)\n",
    "        t=1\n",
    "\n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[product][t])\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # CV time series splits\n",
    "        cv_folds = pp.split_timeseries_cv(n_splits=3, ids_timePeriods=ids_timePeriods_train)\n",
    "        \n",
    "        # CV search\n",
    "        cv_results[product] = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status=False)\n",
    "\n",
    "        # Status\n",
    "        print('Product '+str(product)+' of '+str(len(products))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "\n",
    "    # Save\n",
    "    _ = dump(cv_results, path_weightsmodel+'/'+local_weightsmodel+'_cv_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1c7a-4494-4191-858a-231dc1ba9fd0",
   "metadata": {},
   "source": [
    "## Fit weight functions and generate weights\n",
    "\n",
    "We now fit a local random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately (local training). Then, for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately, we generate the weights given the test feature $x_{k,t}$. This is done *separately* for each product (SKU) $k=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410060e-ec5e-4f5d-8da2-7a81da076ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c21564-d205-4df7-a645-3cb408b54692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "    \n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for product in products:\n",
    "        \n",
    "        # Initialize\n",
    "        weights[product], weightfunctions_times[product], weights_times[product] = {}, {}, {}\n",
    "    \n",
    "        # For each period t=1,...,T\n",
    "        for t in ts:\n",
    "\n",
    "            # Extract preprocessed data\n",
    "            locals().update(data[product][t])\n",
    "                    \n",
    "            # Adjust look-ahead tau to account for end of horizon\n",
    "            tau_ = min(tau,T-t)\n",
    "            \n",
    "            # Initialize\n",
    "            pp = PreProcessing()\n",
    "            wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "            # Load tuned weights model\n",
    "            wm.load_cv_result(path=path_weightsmodel+'/'+local_weightsmodel+'_cv_tau'+str(tau_)+'.joblib', product=product)\n",
    "\n",
    "            \n",
    "            # Fit weight functions  \n",
    "            st_exec, st_cpu = time.time(), time.process_time() \n",
    "            wm.fit(X_train, y_train)\n",
    "            weightfunctions_times[product][t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "            # Generate weights  \n",
    "            st_exec, st_cpu = time.time(), time.process_time() \n",
    "            weights[product][t] = wm.apply(X_train, X_test)\n",
    "            weights_times[product][t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "        # Status\n",
    "        print('Product '+str(product)+' of '+str(len(products))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True) \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+local_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+local_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9986-993c-4b9a-96e5-22444655a258",
   "metadata": {},
   "source": [
    "# Rolling Horizon Optimization\n",
    "\n",
    "The rolling horizon optimization approach is best illustrated for the application to GwSAA below. Analogously, this approach is applied for the other models, where the data used for training and sampling, fitting the weights model, and the optimization the model solved, are adjusted accordingly.\n",
    "\n",
    "**Input:** look-ahead $\\tau$, cost parameters $\\{K_{k,t},u_{k,t},h_{k,t},b_{k,t}\\}_{t=1}^{T}$ for products $k=1,....M$\n",
    "\n",
    "**For** $t=1,...,T$\n",
    "    \n",
    "* $\\tau \\gets \\min\\{T-t,\\tau\\}$     Truncate look-ahead $\\tau$ given remaining planning horizon $T-t$\n",
    " \n",
    "* **w.fit**     Train weight functions $w_{j,t,\\tau}^{\\,i}(\\,\\cdot\\,)$ on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$\n",
    "    \n",
    "* **For** $k=1,...,M$ \n",
    "    \n",
    "    * $x_{k,t} \\gets X_{k,t}$     Observe new realization $x_{k,t}$ of the feature vector $X_{k,t}$ for product $k$\n",
    "        \n",
    "    * **w.predict**     Apply $x_{k,t}$ to weight functions $w_{j,t,\\tau}^{\\,i}(\\,\\cdot\\,)$ to retrieve $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$\n",
    "    \n",
    "    * Get decision $\\hat{q}_{k,t}(I_{k,t},x_{k,t})$ by solving **GwSAA**\n",
    "        \\begin{equation*}\n",
    "            \\hat{q}_{k,t}(I_{k,t},x_{k,t}) \\in \\arg\\min_{\\substack{q_{t} \\in Q_{t} \\\\ ... \\\\ q_{t+\\tau} \\in Q_{t+\\tau}}}\\sum_{j=1}^{M}\\sum_{i=1}^{N_{j,t,\\tau}}w_{j,t,\\tau}^{\\,i}(x_{k,t})c_{k,t,\\tau}(q_{t},...,q_{t+\\tau},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i},I_{k,t})\n",
    "        \\end{equation*}\n",
    "        \n",
    "    * $I_{k,t+1} \\gets I_{k,t}+\\hat{q}_{k,t}(I_{k,t},x_{k,t})-d_{k,t}$     Apply decision $\\hat{q}_{k,t}(I_{k,t},x_{k,t})$, observe $d_{k,t} \\gets D_{k,t}$\n",
    "    \n",
    "    **EndFor**\n",
    "**EndFor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b5a1c-c253-41b3-be40-866598943f7e",
   "metadata": {},
   "source": [
    "## (a) Rolling Horizon Global Weighted SAA (GwSAA)\n",
    "\n",
    "Weighted SAA over \"global\" distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{j,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$. Data is scaled per product for training and rescaled with scaler of the current product applied to all products for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736f80d-524b-4165-8993-b59070a1a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea232ec-5727-401f-a60c-37e82bad7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f251620-54eb-412b-9cb3-2e7772622e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+GwSAA): os.mkdir(path_results+'/'+GwSAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(global_weightsmodel, GwSAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), weights=weights_[product], \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product]) \n",
    "                                             for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82613b6b-cb2a-4531-94c3-f2d2bd62afa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6739456-802b-407d-8376-12c9f472612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f990e6a-3366-45d5-8ccf-bdc88876c5fc",
   "metadata": {},
   "source": [
    "## (b) Rolling Horizon Global Robust Weighted SAA (GwSAA-R)\n",
    "\n",
    "Weighted SAA as for GwSAA and minimizing worst case cost over product-specific \"global\" uncertainty sets $\\boldsymbol{\\mathcal{U}}_{j,t,\\tau}^{\\,i}(k)=\\{\\boldsymbol{\\tilde{d}}\\in \\mathcal{D}_{k,t} \\times ... \\times \\mathcal{D}_{k,t+\\tau}: ||\\boldsymbol{\\tilde{d}}-\\boldsymbol{d}_{j}^{\\,i}|| \\leq \\epsilon_{k}\\}$, with $\\boldsymbol{\\tilde{d}}=(\\tilde{d}_{t},...,\\tilde{d}_{t+\\tau})^\\top$ and $\\boldsymbol{d}_{j}^{\\,i}=(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})^\\top$, for each pair $(j,i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22733a9b-4016-41d9-8449-087af93b3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5e177-550f-4940-b097-dcaf8819fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de89b1-5104-403d-9fe7-7cad8690c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+GwSAAR): os.mkdir(path_results+'/'+GwSAAR)\n",
    "\n",
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "        \n",
    "        # Initialize Experiment\n",
    "        experiment = Experiment(global_weightsmodel, GwSAAR, **experiment_setup)\n",
    "\n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "        # Load weights\n",
    "        weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Preprocess experiment data\n",
    "        weights_, samples_, actuals_, epsilons_ = experiment.preprocess_data(data, weights, e)\n",
    "        \n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, e=e, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         weights=weights_[product], samples=samples_[product], \n",
    "                                                                         actuals=actuals_[product], epsilons=epsilons_[product]) \n",
    "                                                 for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f851b0-18e1-4d26-bb22-245c1573f504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a3058-8de3-47d6-8c7a-6ad206a82068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aad5e6-9d9f-4a82-8d69-f7bc278dc881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc90b4-479e-4f37-8853-0b32f62cf52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562630df-2971-4cdd-8de2-c8220185aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177c4b64-a7ff-4b64-8e95-ddcd851acca5",
   "metadata": {},
   "source": [
    "## (c) Rolling Horizon Local Weighted SAA (wSAA)\n",
    "\n",
    "Weighted SAA over \"local\" distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained (for each product) on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6bda0-2542-436d-a100-79b78d590893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e508dfe-6860-48f0-8747-e7a7242092c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2309b-bd18-4044-ba60-0c619372421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+wSAA): os.mkdir(path_results+'/'+wSAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(local_weightsmodel, wSAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = load(path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), weights=weights_[product], \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product]) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d2e6d-cbc8-4156-9f6d-8db11f19889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3ae23-fecf-40c4-ab82-42d6f1d04b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35ca8e-b483-4cc4-97e1-30f3266dcfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a3ff83-6de0-4fad-8b99-a50eb9f3dd11",
   "metadata": {},
   "source": [
    "## (d) Rolling Horizon Local Robust Weighted SAA (wSAA-R)\n",
    "\n",
    "Weighted SAA as for wSAA and minimizing worst case cost over product-specific \"local\" uncertainty sets $\\boldsymbol{\\mathcal{U}}_{k,t,\\tau}^{\\,i}=\\{\\boldsymbol{\\tilde{d}}\\in \\mathcal{D}_{k,t} \\times ... \\times \\mathcal{D}_{k,t+\\tau}: ||\\boldsymbol{\\tilde{d}}-\\boldsymbol{d}_{k}^{\\,i}|| \\leq \\epsilon_{k}\\}$, with $\\boldsymbol{\\tilde{d}}=(\\tilde{d}_{t},...,\\tilde{d}_{t+\\tau})^\\top$ and $\\boldsymbol{d}_{k}^{\\,i}=(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})^\\top$, for each pair $(j,i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd9d8d-cc36-4fa8-b8b8-f0f4a6f5039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce133eb4-1175-4171-8eda-9952a2ab91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3ad4d-7fd9-4c5f-9009-913759315e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+wSAAR): os.mkdir(path_results+'/'+wSAAR)\n",
    "\n",
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "        \n",
    "        # Initialize Experiment\n",
    "        experiment = Experiment(local_weightsmodel, wSAAR, **experiment_setup)\n",
    "\n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "        # Load weights\n",
    "        weights = load(path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Preprocess experiment data\n",
    "        weights_, samples_, actuals_, epsilons_ = experiment.preprocess_data(data, weights, e)\n",
    "        \n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, e=e, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         weights=weights_[product], samples=samples_[product], \n",
    "                                                                         actuals=actuals_[product], epsilons=epsilons_[product]) \n",
    "                                                 for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356cc89-3dc4-45e1-ab7f-7bcf8c64274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d980c8-d0f7-4f19-bd17-65100814f1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650cf8f6-4fd6-4258-9498-759b49e4cbc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (e) Baseline model: Rolling Horizon Local Weighted SAA (SAA)\n",
    "\n",
    "SAA over the distribution $\\{\\frac{1}{N_{k,t,\\tau}},(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca7166-36c7-483f-a9c0-f517b4f377c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ab893-5528-47c7-ad40-386a703f94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c23da1-f4a1-4b59-81ea-49c512307d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+SAA): os.mkdir(path_results+'/'+SAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(local_weightsmodel, SAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "    # Preprocess experiment data\n",
    "    samples_, actuals_ = experiment.preprocess_data(data)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product]) \n",
    "                                             for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3ac71-3ae7-4a72-a56f-433ec92bfb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1bc2c-df43-4c08-bd5a-08c91d99b970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554967d-2acd-45c3-8e68-10cd23695bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccb147-9122-4e3f-8435-8c73275d2836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280f481-c0e0-4e07-86bc-c7e2b45fa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b4a736-d792-468a-86a1-8c95976293ed",
   "metadata": {},
   "source": [
    "## (f) Ex-post optimal model with deterministic demand\n",
    "\n",
    "Claircoyant optimal model where demand is known upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430def9f-db7e-455e-8f30-d17c836acb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e64087-863d-4172-9ce7-1b2f234f651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834f8da-ddce-4e34-a632-a87681a2079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+ExPost): os.mkdir(path_results+'/'+ExPost)\n",
    "\n",
    "\n",
    "# Initialize Experiment\n",
    "experiment = Experiment(local_weightsmodel, ExPost, **experiment_setup)\n",
    "\n",
    "# Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "data = load(path_weightsmodel+'/local_data_tau0.joblib')\n",
    "\n",
    "# Preprocess experiment data\n",
    "_, actuals = experiment.preprocess_data(data)\n",
    "\n",
    "# For ex-post clairvoyant, we can directly solve over the full horizon\n",
    "actuals_ = {}\n",
    "for product in products:\n",
    "    d = []\n",
    "    for t in ts:\n",
    "        d = d + [actuals[product][t].item()]\n",
    "    actuals_[product] = np.array(d).reshape(1,len(d))\n",
    "\n",
    "# For each product (SKU) k=1,...,M\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(product=product, wsaamodel=WeightedSAA(), actuals=actuals_[product]) \n",
    "                                         for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fde765-4351-4417-8e92-9615f138712a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a024b-e382-45e9-a686-474e7d263560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7290ae-6900-4c97-a296-0a151d134132",
   "metadata": {},
   "source": [
    "# Backup: Using old hyper parameters for global models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9daf8-66fe-46ff-bbe5-daa14d847001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Weights Model\n",
    "from WeightsModel import PreProcessing\n",
    "from WeightsModel import MaxQFeatureScaler\n",
    "from WeightsModel import MaxQDemandScaler\n",
    "from WeightsModel import RandomForestWeightsModel\n",
    "\n",
    "# Import Weighted SAA Model\n",
    "from WeightedSAA import WeightedSAA\n",
    "from WeightedSAA import RobustWeightedSAA\n",
    "from WeightedSAA import RollingHorizonOptimization\n",
    "\n",
    "# Import Experiment\n",
    "from Experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1988a3a-4814-4828-8a40-4e9fc34f8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "experiment_setup = dict(\n",
    "\n",
    "    # Set paths\n",
    "    path_data = '/home/fesc/DDDInventoryControl/Data',\n",
    "    path_weightsmodel = '/home/fesc/DDDInventoryControl/Data/WeightsModel',\n",
    "    path_results = '/home/fesc/DDDInventoryControl/Data/Results',\n",
    "    \n",
    "    # Weights models\n",
    "    global_weightsmodel = 'rfwm_global_old_hyper_params', \n",
    "\n",
    "    # Optimization models\n",
    "    GwSAA = 'GwSAA_old_hyper_params',\n",
    "    GwSAAR = 'GwSAAR_old_hyper_params',\n",
    "    \n",
    "    # Set identifier of start period of testing horizon\n",
    "    timePeriodsTestStart = 114,\n",
    "\n",
    "    # Set product identifiers\n",
    "    products = range(1,460+1),   # Products (SKUs) k=1,...,M\n",
    "    \n",
    "    # Set problem params\n",
    "    T = 13,             # Planning horizon T\n",
    "    ts = range(1,13+1), # Periods t=1,...,T of the planning horizon\n",
    "    taus = [0,1,2,3,4], # Look-aheads tau=0,...,4\n",
    "    es = [1,3,6,9,12],  # Uncertainty set specifications e=1,...,12\n",
    "       \n",
    "    # Set cost params\n",
    "    cost_params = [\n",
    "        {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "    ],\n",
    "    \n",
    "    # Set gurobi params\n",
    "    gurobi_params = {\n",
    "    \n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 1, \n",
    "        'NonConvex': 2, \n",
    "        'PSDTol': 1e-3, # 0.1%\n",
    "        'MIPGap': 1e-3, # 0.1%\n",
    "        'NumericFocus': 0, \n",
    "        'obj_improvement': 1e-3, # 0.1%\n",
    "        'obj_timeout_sec': 3*60, # 3 min\n",
    "        'obj_timeout_max_sec': 10*60, # 10 min\n",
    "        \n",
    "    },\n",
    "    \n",
    "\n",
    "    # Hyper params\n",
    "    hyper_params = {\n",
    "\n",
    "        0: {\n",
    "\n",
    "            'max_features': 144, \n",
    "            'min_samples_leaf': 10,\n",
    "            'n_estimators': 500,\n",
    "        },\n",
    "\n",
    "        1: {\n",
    "\n",
    "            'max_features': 160, \n",
    "            'min_samples_leaf': 10,\n",
    "            'n_estimators': 500,\n",
    "        },\n",
    "\n",
    "        2: {\n",
    "\n",
    "            'max_features': 96, \n",
    "            'min_samples_leaf': 10,\n",
    "            'n_estimators': 500,\n",
    "        },\n",
    "\n",
    "        3: {\n",
    "\n",
    "            'max_features': 112, \n",
    "            'min_samples_leaf': 10,\n",
    "            'n_estimators': 500,\n",
    "        },\n",
    "\n",
    "        4: {\n",
    "\n",
    "            'max_features': 48, \n",
    "            'min_samples_leaf': 10,\n",
    "            'n_estimators': 500,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)\n",
    "\n",
    "# Initialize Experiment\n",
    "experiment = Experiment(**experiment_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca642bf4-dda0-4ae3-8f1c-94e103ce78c1",
   "metadata": {},
   "source": [
    "## Global Training and Samping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192c62b-5305-48e5-958e-45c0730c0526",
   "metadata": {},
   "source": [
    "### Fit weight functions and generate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd16f85-d073-46d0-83f4-3b1ccf8bdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 64,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba27a42-a8d3-426c-b882-ab6c122ad4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "\n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "        \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "        \n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[t])\n",
    "            \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params, hyper_params[tau_])\n",
    "        \n",
    "        # Scale features and demands\n",
    "        X_train, X_test = pp.scale(X_train, X_scalers, ids_products_train), pp.scale(X_test, X_scalers, ids_products_test)\n",
    "        y_train, y_test = pp.scale(y_train, y_scalers, ids_products_train), pp.scale(y_test, y_scalers, ids_products_test)           \n",
    "                    \n",
    "        # Fit weight functions  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        wm.fit(X_train, y_train)\n",
    "        weightfunctions_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "      \n",
    "        # Generate weights  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        weights[t] = wm.apply(X_train, X_test)\n",
    "        weights_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "        \n",
    "    # Status\n",
    "    print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+global_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+global_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f9a1c-cfc9-46a9-bbcc-31424c06427f",
   "metadata": {},
   "source": [
    "## Rolling Horizon Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4c0d5-a966-4325-98ec-be9466e08cf6",
   "metadata": {},
   "source": [
    "### (a) Rolling Horizon Global Weighted SAA (GwSAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a8a38-1144-49c5-84e8-e4a11c6bebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e6b8b-fd87-4feb-9258-53f4b725cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cc4cc-0239-4189-9a4c-75c4be7dea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+GwSAA): os.mkdir(path_results+'/'+GwSAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(global_weightsmodel, GwSAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), weights=weights_[product], \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product]) \n",
    "                                             for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d662b-8bc0-4ac6-a25d-0931988f859f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c6653-6a53-4dd2-b020-0106f0030793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561cef5-40ed-425e-a6fd-0fadeb5596bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a4445-0734-4401-b62c-61e25e541346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8ce9e-2496-46e7-bb76-4f4da7a26aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbb8d9-8873-4518-8e7e-62fea49134ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0262fcf2-9a3d-48cd-8770-9207bbf353dd",
   "metadata": {},
   "source": [
    "# Trying out Robust with LDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9400b81-9e88-4800-a1a2-828988ac8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Robust Weighted SAA with Linear Decision Rules\n",
    "class RobustWeightedSAALDR:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Description ...\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    ### Init\n",
    "    def __init__(self, K=100, u=0.5, h=1, b=9, epsilon=0, LogToConsole=0, Threads=1, NonConvex=2, PSDTol=0, MIPGap=1e-3, \n",
    "                 NumericFocus=0, obj_improvement=1e-3, obj_timeout_sec=3*60, obj_timeout_max_sec=10*60, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            K (default=100): Fixed cost of ordering\n",
    "            u (default=0.5): Per unit cost of ordering\n",
    "            h (default=1): Per unit cost of inventory holding\n",
    "            b (default=9): Per unit cost of demand backlogging\n",
    "            \n",
    "            epsilon (default=0): Uncertainty set parameter\n",
    "\n",
    "            LogToConsole (default=0): ...\n",
    "            Threads (default=1): ...\n",
    "            NonConvex (default=2): ...\n",
    "            PSDTol (default=0): ...\n",
    "            MIPGap (default=1e-3): ...\n",
    "            NumericFocus (default=0): ...\n",
    "            \n",
    "            obj_improvement (default=1e-3): ...\n",
    "            obj_timeout_sec (default=3*60): ...\n",
    "            obj_timeout_max_sec (default=10*60): ...\n",
    "            \n",
    "        Further key word arguments (kwargs): ignored\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Set params\n",
    "        self.params = {\n",
    "            \n",
    "            'K': K,\n",
    "            'u': u,\n",
    "            'h': h,\n",
    "            'b': b,\n",
    "            \n",
    "            'epsilon': epsilon,\n",
    "            \n",
    "            'LogToConsole': LogToConsole,\n",
    "            'Threads': Threads,\n",
    "            'NonConvex': NonConvex,\n",
    "            'PSDTol': PSDTol,\n",
    "            'MIPGap': MIPGap,\n",
    "            'NumericFocus': NumericFocus,\n",
    "            \n",
    "            'obj_improvement': obj_improvement,\n",
    "            'obj_timeout_sec': obj_timeout_sec,\n",
    "            'obj_timeout_max_sec': obj_timeout_max_sec\n",
    "        \n",
    "        }\n",
    "        \n",
    "\n",
    "    ### Function to set params\n",
    "    def set_params(self, **kwargs):\n",
    "        \n",
    "        # Update all items that match an existing key\n",
    "        self.params.update((k, kwargs[k]) for k in set(kwargs).intersection(self.params))\n",
    "            \n",
    "        \n",
    "    ### Function to get params\n",
    "    def get_params(self):\n",
    "        \n",
    "        return self.params\n",
    "        \n",
    "\n",
    "    \n",
    "    ### Function to create and set up the model\n",
    "    def create(self, d, w, I=0, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        This function initializes and sets up a (tau+1)-periods rolling look-ahead control\n",
    "        problem in MIP formulation with Robust Weighted SAA optimization to find the next\n",
    "        decision to take (ordering quantity q of the current, first period)\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            d: demand samples i=1...n_samples\n",
    "            w: sample weights i=1...n_samples\n",
    "            I: starting inventory          \n",
    "            \n",
    "        Further key word arguments (kwargs): passed to set_params() to update (valid) paramaters, e.g., cost parameters K, u, h, b, \n",
    "        uncertainty set param epsilon\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set params\n",
    "        self.set_params(**kwargs)\n",
    "     \n",
    "        # Length of rolling look-ahead horizon\n",
    "        n_periods = d.shape[1] if d.ndim>1 else 1\n",
    "        \n",
    "        # Number of demand samples\n",
    "        n_samples = d.shape[0]\n",
    "        \n",
    "        # Number of model constraints (per demand sample i and period t)\n",
    "        n_constraints = 2\n",
    "        \n",
    "        # Cost params\n",
    "        K = self.params['K']\n",
    "        u = self.params['u']\n",
    "        h = self.params['h']\n",
    "        b = self.params['b']\n",
    "        \n",
    "        # Uncertainty set parameter epsilon\n",
    "        epsilon = self.params['epsilon']\n",
    "\n",
    "        \n",
    "        ## Constraint coefficients\n",
    "        \n",
    "        # LHS constraint coefficient matrix A1[t,s,m] with dim = tau x tau x n_constraints where A1[t,s,m]==0 for s > t\n",
    "        A1 = np.array([np.array([np.array([h,-b])\n",
    "                                if s<=t\n",
    "                                else np.array([0,0])\n",
    "                                for s in range(n_periods)])\n",
    "                      for t in range(n_periods)])\n",
    "\n",
    "        # LHS constraint coefficients A2[t,s,m] with dim = tau x tau x n_constraints where A2[t,s,m]==0 for s > t\n",
    "        A2 = np.array([np.array([np.array([-h,b])\n",
    "                                if s<=t\n",
    "                                else np.array([0,0])\n",
    "                                for s in range(n_periods)])\n",
    "                      for t in range(n_periods)])\n",
    "\n",
    "        # LHS constraint coefficients A3[t,s,m] with dim = tau x tau x n_constraints where A3[t,s,m]==0 for s != t\n",
    "        A3 = np.array([np.array([np.array([-1,-1])\n",
    "                                if s==t\n",
    "                                else np.array([0,0])\n",
    "                                for s in range(n_periods)])\n",
    "                      for t in range(n_periods)])\n",
    "\n",
    "        # RHS constraint coefficients f[t,m] with dim = tau x n_constraints \n",
    "        B = np.array([np.array([-h*I,b*I])\n",
    "                      for t in range(n_periods)])\n",
    "   \n",
    "\n",
    "\n",
    "        ## Create model\n",
    "        if self.params['LogToConsole']==0:\n",
    "            \n",
    "            env = gp.Env(empty=True)\n",
    "            env.setParam('OutputFlag',0)\n",
    "            env.start()\n",
    "            self.m = gp.Model(env=env)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.m = gp.Model()\n",
    "        \n",
    "        # Primary decision variable (ordering quantity for each t)\n",
    "        q0 = self.m.addVars(n_periods, vtype='C', name='q0')\n",
    "        Q = self.m.addVars(n_periods, n_periods, vtype='C', name='Q')\n",
    "    \n",
    "        # Auxiary decision variable (for fixed cost of ordering for each sample i and t)\n",
    "        z = self.m.addVars(n_samples, n_periods, vtype='B', name='z') \n",
    "\n",
    "        # Auxiary decision variable (for cost of inventory holding or demand backlogging for each t and sample i)\n",
    "        s0_i = self.m.addVars(n_samples, n_periods, vtype='C', name='s0_i') \n",
    "        S_i = self.m.addVars(n_samples, n_periods, n_periods, vtype='C', name='S_i') \n",
    "\n",
    "        # Auxiary decision variable (from reformulation of robust constraints)\n",
    "        l_i = self.m.addVars(n_samples, n_periods, vtype='C', lb=0, name='Lambda_i')\n",
    "        \n",
    "        # Auxiary decision variable (from reformulation of robust constraints)\n",
    "        Lambda_i = self.m.addVars(n_samples, n_periods, n_periods, n_constraints, \n",
    "                                  vtype='C', lb=0, name='Lambda_i')\n",
    "        \n",
    "        # Auxiary decision variables for norm in objective\n",
    "        norm_in_obj_inner = self.m.addVars(n_samples, n_periods, vtype='C', name='norm_in_obj_inner')\n",
    "        norm_in_obj_inner_abs = self.m.addVars(n_samples, n_periods, vtype='C', name='norm_in_obj_inner_abs')\n",
    "        norm_in_obj = self.m.addVars(n_samples, vtype='C', lb=0, name='norm_in_obj')\n",
    "        \n",
    "        # Auxiary decision variables for norms in constraints\n",
    "        norm_in_cons_inner = self.m.addVars(n_samples, n_periods, n_periods, n_constraints, vtype='C', name='norm_in_cons_inner')\n",
    "        norm_in_cons_inner_abs = self.m.addVars(n_samples, n_periods, n_periods, n_constraints, vtype='C', name='norm_in_cons_inner_abs')\n",
    "        norm_in_cons = self.m.addVars(n_samples, n_periods, n_constraints, vtype='C', lb=0, name='norm_in_cons')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        ## Constraints   \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Constraints (for each m=1...n_constraints, t=1...tau, i=1...n_samples). For the implementation using Gurobi, we do not\n",
    "        use a 'big M' formulation to model fixed cost of ordering. Thus, the mathematical formulation is slightly different than\n",
    "        provided in the paper.\n",
    "  \n",
    "        To calculate the l1 norm ||.|| we use further helper constraints.\n",
    "  \n",
    "        \n",
    "        \"\"\"        \n",
    "        \n",
    "        # Non-anticipativity of linear decision rules (set Q entries to zero where s >= t)\n",
    "        Q_cons = self.m.addConstrs((Q[t,s] == 0)\n",
    "                                   for s in range(n_periods)\n",
    "                                   for t in range(n_periods)\n",
    "                                   if (s >= t))\n",
    "        \n",
    "        # Non-anticipativity of linear decision rules (set Y_i entries to zero where s > t)\n",
    "        S_i_cons = self.m.addConstrs((S_i[i,t,s] == 0)\n",
    "                                     for s in range(n_periods) \n",
    "                                     for t in range(n_periods) \n",
    "                                     for i in range(n_samples)\n",
    "                                     if (s > t))\n",
    "        \n",
    "        # Non-negative decision space for primary decision variable\n",
    "        q_cons = self.m.addConstrs(q0[t]+gp.quicksum(Q[t,s]*d[i,s] for s in range(n_periods)) >= 0\n",
    "                                   for t in range(n_periods)\n",
    "                                   for i in range(n_samples))\n",
    "                \n",
    "        \n",
    "        # Fixed cost\n",
    "        z_cons1 = self.m.addConstrs((z[i,t] == 0) >> (q0[t]+gp.quicksum(Q[t,s]*d[i,s] for s in range(n_periods)) <= 0.5) \n",
    "                                    for t in range(n_periods) \n",
    "                                    for i in range(n_samples))\n",
    "        z_cons2 = self.m.addConstrs((z[i,t] == 1) >> (q0[t]+gp.quicksum(Q[t,s]*d[i,s] for s in range(n_periods)) >= 0.5 + 10**-9) \n",
    "                                    for t in range(n_periods) \n",
    "                                    for i in range(n_samples))\n",
    "\n",
    "        \n",
    "        # Constraint to set inner part of the norm in the constraints\n",
    "        norm_in_cons1 = self.m.addConstrs(\n",
    "\n",
    "            norm_in_cons_inner[i,t,s,m] == A1[t,s,m] * Q[s,t] + A2[t,s,m] + A3[t,s,m] * S_i[i,s,t] +  Lambda_i[i,t,s,m]\n",
    "            \n",
    "            for s in range(n_periods)\n",
    "            for m in range(n_constraints)\n",
    "            for t in range(n_periods)\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "\n",
    "         # Constraint to set inner part of the norm in the constraints as absolute values\n",
    "        norm_in_cons2 = self.m.addConstrs(\n",
    "\n",
    "            norm_in_cons_inner_abs[i,t,s,m] == gp.abs_(norm_in_cons_inner[i,t,s,m])\n",
    "            \n",
    "            for s in range(n_periods)\n",
    "            for m in range(n_constraints)\n",
    "            for t in range(n_periods)\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "\n",
    "        \n",
    "        # Constraint to get actual value of the norm in the constraints\n",
    "        norm_in_cons3 =  self.m.addConstrs(\n",
    "\n",
    "            norm_in_cons[i,t,m] == gp.quicksum(norm_in_cons_inner_abs[i,t,s,m] for s in range(n_periods))\n",
    "\n",
    "            for m in range(n_constraints)\n",
    "            for t in range(n_periods)\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Constraint to set inner part of the norm in the objective\n",
    "        norm_in_obj1 = self.m.addConstrs(\n",
    "\n",
    "            norm_in_obj_inner[i,t] == (gp.quicksum(Q[s,t]*u for s in range(n_periods)) + \n",
    "                                       gp.quicksum(S_i[i,s,t]*1 for s in range(n_periods))  + \n",
    "                                       l_i[i,t])\n",
    "            \n",
    "            for t in range(n_periods)\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "\n",
    "         # Constraint to set inner part of the norm in the objective as absolute values\n",
    "        norm_in_obj2 = self.m.addConstrs(\n",
    "\n",
    "            norm_in_obj_inner_abs[i,t] == gp.abs_(norm_in_obj_inner[i,t])\n",
    "            \n",
    "            for t in range(n_periods)\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "\n",
    "        \n",
    "        # Constraint to get actual value of the norm in the objective\n",
    "        norm_in_obj3 =  self.m.addConstrs(\n",
    "\n",
    "            norm_in_obj[i] == gp.quicksum(norm_in_obj_inner_abs[i,t] for t in range(n_periods))\n",
    "\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        # Actual constraints\n",
    "        cons = self.m.addConstrs(\n",
    "\n",
    "            \n",
    "            # A1 * (q0 + Q d)\n",
    "            gp.quicksum(A1[t,s,m]*(q0[s]+gp.quicksum(Q[s,ts]*d[i,ts] for ts in range(n_periods))) for s in range(n_periods)) +\n",
    "\n",
    "            # A2 * d\n",
    "            gp.quicksum(A2[t,s,m]*d[i,s] for s in range(n_periods)) +\n",
    "\n",
    "            # A3 * (s0_i + S_i d)\n",
    "            gp.quicksum(A3[t,s,m]*(s0_i[i,s]+gp.quicksum(S_i[i,s,ts]*d[i,ts] for ts in range(n_periods))) for s in range(n_periods)) +\n",
    "\n",
    "            # Lambda_i * d\n",
    "            gp.quicksum(Lambda_i[i,t,s,m]*d[i,s] for s in range(n_periods)) +\n",
    "\n",
    "            # epsilon*||A1*Q + A2 + A3*S_i + Lambda_i||_1\n",
    "            epsilon * norm_in_cons[i,t,m]\n",
    "\n",
    "            <= B[t,m]\n",
    "\n",
    "            for m in range(n_constraints)\n",
    "            for t in range(n_periods)\n",
    "            for i in range(n_samples) \n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        ## Objective \n",
    "        obj = self.m.setObjective(\n",
    "\n",
    "            # Weighted sum\n",
    "            gp.quicksum(\n",
    "\n",
    "                # i'th weight\n",
    "                w[i] * (                                         \n",
    "\n",
    "                    # u * (q0 + Q d)\n",
    "                    gp.quicksum(u*(q0[t]+gp.quicksum(Q[t,s]*d[i,s] for s in range(n_periods))) for t in range(n_periods)) + \n",
    "\n",
    "                    # K * z\n",
    "                    gp.quicksum(K*z[i,t] for t in range(n_periods)) +                     \n",
    "                    \n",
    "                    # s0_i + S_i d\n",
    "                    gp.quicksum(s0_i[i,t]+gp.quicksum(S_i[i,t,s]*d[i,s] for s in range(n_periods)) for t in range(n_periods)) +\n",
    "                    \n",
    "                    # l_i d\n",
    "                    gp.quicksum(l_i[i,t] * d[i,t] for t in range(n_periods)) +\n",
    "                    \n",
    "                    # epsilon * || Q^u + S_i^1 + l_i ||\n",
    "                    epsilon * norm_in_obj[i]                   \n",
    "\n",
    "                ) for i in range(n_samples)),        \n",
    "\n",
    "            # min\n",
    "            GRB.MINIMIZE\n",
    "        )\n",
    "\n",
    "        # Store n periods\n",
    "        self.n_periods = n_periods\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #### Function to dump model\n",
    "    def dump(self):\n",
    "        \n",
    "        self.m = None\n",
    "\n",
    "        \n",
    "    #### Function to optimize model\n",
    "    def optimize(self, **kwargs):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        Arguments: None\n",
    "            \n",
    "        Further key word arguments (kwargs): passed to update Gurobi meta params (other kwargs are ignored)\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            q_hat: ...\n",
    "            status: ...\n",
    "            solutions: ...\n",
    "            gap: ...\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Update gurobi meta params if provided\n",
    "        gurobi_meta_params = {\n",
    "            \n",
    "            'LogToConsole': kwargs.get('LogToConsole', self.params['LogToConsole']),\n",
    "            'Threads': kwargs.get('Threads', self.params['Threads']),\n",
    "            'NonConvex': kwargs.get('NonConvex', self.params['NonConvex']),\n",
    "            'PSDTol': kwargs.get('PSDTol', self.params['PSDTol']),\n",
    "            'MIPGap': kwargs.get('MIPGap', self.params['MIPGap']),\n",
    "            'NumericFocus': kwargs.get('NumericFocus', self.params['NumericFocus']),\n",
    "            'obj_improvement': kwargs.get('obj_improvement', self.params['obj_improvement']),\n",
    "            'obj_timeout_sec': kwargs.get('obj_timeout_sec', self.params['obj_timeout_sec']),\n",
    "            'obj_timeout_max_sec': kwargs.get('obj_timeout_max_sec', self.params['obj_timeout_max_sec'])\n",
    "        }\n",
    "        \n",
    "        self.set_params(**gurobi_meta_params)           \n",
    "            \n",
    "        # Set Gurobi meta params\n",
    "        self.m.setParam('LogToConsole', self.params['LogToConsole'])\n",
    "        self.m.setParam('Threads', self.params['Threads'])\n",
    "        self.m.setParam('NonConvex', self.params['NonConvex'])\n",
    "        self.m.setParam('PSDTol', self.params['PSDTol'])\n",
    "        self.m.setParam('MIPGap', self.params['MIPGap'])\n",
    "        self.m.setParam('NumericFocus', self.params['NumericFocus'])  \n",
    " \n",
    "\n",
    "        ## Callback on solver time and objective improvement\n",
    "        def cb(model, where):\n",
    "            \n",
    " \n",
    "            # MIP node\n",
    "            if where == GRB.Callback.MIPNODE:\n",
    "\n",
    "                # Get current incumbent objective\n",
    "                objbst = model.cbGet(GRB.Callback.MIPNODE_OBJBST)   \n",
    "                \n",
    "                # Get current soluction count\n",
    "                solcnt = model.cbGet(GRB.Callback.MIPNODE_SOLCNT)\n",
    "                \n",
    "                # If objective improved sufficiently\n",
    "                if abs(objbst - model._cur_obj) > abs(model._cur_obj * self.params['obj_improvement']):\n",
    "\n",
    "                    # Update incumbent and time\n",
    "                    model._cur_obj = objbst\n",
    "                    model._time = time.time()\n",
    "                 \n",
    "                # Terminate if objective has not improved sufficiently in 'obj_timeout_sec' seconds ...\n",
    "                if time.time() - model._time > self.params['obj_timeout_sec']:        \n",
    "                    \n",
    "                    # ... and at least one soluction has been found\n",
    "                    if solcnt > 0:\n",
    "                        model.terminate()\n",
    "                        \n",
    "                    # ... or max sec have passed\n",
    "                    elif time.time() - model._time > self.params['obj_timeout_max_sec']:\n",
    "                        model.terminate()\n",
    "            \n",
    "            \n",
    "        # Last updated objective and time\n",
    "        self.m._cur_obj = float('inf')\n",
    "        self.m._time = time.time() \n",
    "\n",
    "        # Optimize\n",
    "        self.m.optimize(callback=cb)      \n",
    "        \n",
    "        ## Solution\n",
    "        if self.m.SolCount > 0:\n",
    "        \n",
    "            # Objective value\n",
    "            v_opt = self.m.objVal\n",
    "\n",
    "            # Ordering quantities\n",
    "            #q_hat = [var.xn for var in self.m.getVars() if 'q' in var.VarName]\n",
    "            \n",
    "            \n",
    "\n",
    "            q0_hat = [var.x\n",
    "                      for var in self.m.getVars()\n",
    "                      for t in range(self.n_periods)\n",
    "                      if (var.VarName == 'q0['+str(t)+']')]\n",
    "\n",
    "            Q_hat = np.array([np.array([var.x \n",
    "                                        for var in self.m.getVars()\n",
    "                                        for s in range(self.n_periods)\n",
    "                                        if (var.VarName == 'Q['+str(t)+','+str(s)+']')])\n",
    "                              for t in range(self.n_periods)])\n",
    "\n",
    "            # If some d (actuals) are provided\n",
    "            if 'd' in kwargs:\n",
    "                q_hat = [q0_hat[t]+sum(Q_hat[t,s]*kwargs['d'][s]\n",
    "                                       for s in range(self.n_periods))\n",
    "                         for t in range(self.n_periods)]\n",
    "            else:\n",
    "                q_hat = copy.deepcopy(q0_hat)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            q_hat = [np.nan]\n",
    "            \n",
    "        \n",
    "        # Solution meta data\n",
    "        status = self.m.status\n",
    "        solutions = self.m.SolCount\n",
    "        gap = self.m.MIPGap\n",
    "        \n",
    "                    \n",
    "        # return decisions\n",
    "        return q_hat, status, solutions, gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311ceb93-4b62-43b3-b250-38d3df290c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gurobi\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce97e9-c12d-4393-a037-890c35471564",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18be10fe-0e35-4fe7-b6c6-1913c65e8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsaamodel = RobustWeightedSAALDR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6943b2fe-92f9-4cc4-bd0e-973f4cf5193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "d=np.array([[1,1,0],[0,2,3],[3,2,3],[4,2,3],[1,3,1]])\n",
    "w=np.array([1/5,1/5,1/5,1/5,1/5])\n",
    "I=0\n",
    "epsilon=10\n",
    "\n",
    "wsaamodel.create(d=d, w=w, I=0, epsilon=10)\n",
    "\n",
    "# Optimize and get decisions\n",
    "res = wsaamodel.optimize()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d83cc0fd-bb23-496d-95cb-5a09d8f6de73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([33.2, 0.5, 0.5], 2, 5, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79529b24-4944-4f11-9d19-d6a4e58bfc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e100b-6daa-408e-931a-d10aaae7e05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22faf1-935f-4444-9548-9daa272afb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde973a1-9b07-4fc1-977f-538ef66e933e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDDInventoryControl",
   "language": "python",
   "name": "dddinventorycontrol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
