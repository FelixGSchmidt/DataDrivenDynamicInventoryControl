{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "56961873-285d-4405-a578-3be0a3b88897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import pyreadr\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import itertools\n",
    "import contextlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "79bc6bc2-c52d-43a7-901c-b354eea3e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gini coefficient for weights\n",
    "def gini(array):\n",
    "    \n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array\"\"\"\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "   \n",
    "    array = np.array(array).flatten() #all values are treated equally, arrays must be 1d\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array) #values cannot be negative\n",
    "    array += 0.0000001 #values cannot be 0\n",
    "    array = np.sort(array) #values must be sorted\n",
    "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
    "    n = array.shape[0]#number of array elements\n",
    "   \n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "afd2eff7-495a-4687-9007-32b148425df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multivariate CV\n",
    "def multiVarCV(xi_hat, w_hat):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    CV = ((mu^T Sigma mu) / (mu^T mu))^(1/2)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reduce to positive weights\n",
    "    xi_hat = xi_hat[w_hat > 0,:]\n",
    "    w_hat = w_hat[w_hat > 0]\n",
    "    \n",
    "    # Create \"weighted\" xi_hat by sampling in proportion to w_hat\n",
    "    xi_hat_weighted = [[xi for i in range(0,int(w))] for (xi, w) in zip(xi_hat, w_hat*10**6)]\n",
    "    xi_hat_weighted = np.concatenate(xi_hat_weighted, axis=0)\n",
    "    \n",
    "    ## CV\n",
    "    if xi_hat.shape[1] <= 1:\n",
    "        \n",
    "        # CV\n",
    "        cv = np.std(xi_hat.flatten()) / np.mean(xi_hat.flatten())\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Get covariance matrix\n",
    "        covMatrix = np.cov(xi_hat, rowvar=False, bias=True)\n",
    "\n",
    "        # Get sample mean\n",
    "        sampleMean = np.mean(xi_hat, axis=0)\n",
    "\n",
    "        # Multi-variate CV\n",
    "        cv = np.sqrt(\n",
    "            (sum([sum(sampleMean * x) for x in covMatrix] * sampleMean)) / \n",
    "            (sum(sampleMean * sampleMean)**2)\n",
    "        )\n",
    "        \n",
    "    ## Weighted CV\n",
    "    if xi_hat_weighted.shape[1] <= 1:\n",
    "        \n",
    "        # CV\n",
    "        cv_weighted = np.std(xi_hat_weighted.flatten()) / np.mean(xi_hat_weighted.flatten())\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Get covariance matrix\n",
    "        covMatrix_weighted = np.cov(xi_hat_weighted, rowvar=False, bias=True)\n",
    "\n",
    "        # Get sample mean\n",
    "        sampleMean_weighted = np.mean(xi_hat_weighted, axis=0)\n",
    "\n",
    "        # Multi-variate CV\n",
    "        cv_weighted = np.sqrt(\n",
    "            (sum([sum(sampleMean_weighted * x) for x in covMatrix_weighted] * sampleMean_weighted)) / \n",
    "            (sum(sampleMean_weighted * sampleMean_weighted)**2)\n",
    "        )\n",
    "\n",
    "    return cv, cv_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4d97ea6a-26ce-4096-9929-625165c14fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample weights diagnostic\n",
    "def getSampleWeightsDiagnostic(model,\n",
    "                               model_params, \n",
    "                               weights,\n",
    "                               y_samples, \n",
    "                               y_samples_SKU, \n",
    "                               ID_samples, \n",
    "                               ID_samples_SKU):\n",
    "    \n",
    "    # Samples of unknown variable (i,t)\n",
    "    if model == 'global':\n",
    "\n",
    "        xi_hat = np.array(\n",
    "        y_samples.iloc[list(ID_samples.sale_yearweek < \n",
    "                       ID_samples_SKU.loc[model_params['N']].sale_yearweek),\n",
    "                                     0:model_params['T']])\n",
    "\n",
    "        xi_hat_names = np.array(\n",
    "        ID_samples.loc[list(ID_samples.sale_yearweek < \n",
    "                       ID_samples_SKU.loc[model_params['N']].sale_yearweek)].SKU_API)\n",
    "\n",
    "    else:\n",
    "\n",
    "        xi_hat = np.array(y_samples_SKU.iloc[0:model_params['N'],0:model_params['T']])  \n",
    "        xi_hat_names = np.array(ID_samples_SKU.loc[model_params['N']].SKU_API)\n",
    "\n",
    "\n",
    "    # Actuals\n",
    "    xi_act = np.array(y_samples_SKU.iloc[model_params['N'],0:model_params['T']])      \n",
    "\n",
    "    # Sample weights (i)\n",
    "    w_hat = np.array(weights[model_params['t_current']+1])\n",
    "\n",
    "    # Dataframe storing weights x SKU\n",
    "    weights_x_t = pd.DataFrame({\n",
    "        'SKU': np.repeat(model_params['SKU'], w_hat.shape[0]),\n",
    "        'SKU_API': np.repeat(ID_samples_SKU.SKU_API[0], w_hat.shape[0]),\n",
    "        'x_SKU_API': xi_hat_names,\n",
    "        'T_horizon_rolling': np.repeat(model_params['T_horizon_rolling'], w_hat.shape[0]),\n",
    "        't': np.repeat(model_params['t_current'], w_hat.shape[0]),\n",
    "        'w_hat': w_hat       \n",
    "    })\n",
    "\n",
    "\n",
    "    # Dataframe storing multi variate sample CV\n",
    "    cv, cv_weighted = multiVarCV(xi_hat, w_hat)\n",
    "    cv_x_t = pd.DataFrame({\n",
    "        'SKU': [model_params['SKU']],\n",
    "        'SKU_API': [ID_samples_SKU.SKU_API[0]],\n",
    "        'T_horizon_rolling': [model_params['T_horizon_rolling']],\n",
    "        't': model_params['t_current'],\n",
    "        'cv': [cv],\n",
    "        'cv_weighted': [cv_weighted]\n",
    "    })\n",
    "\n",
    "    return weights_x_t, cv_x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ea1b94bb-d517-4114-a5f0-abbe9f99fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function running sample weights diagnostic\n",
    "def runSampleWeightsDiagnostic(SKU, PATH_SAMPLES, PATH_DATA):\n",
    "\n",
    "    # Horizons\n",
    "    T_horizon_rolling_range=range(1,5+1)\n",
    "    T_horizon = 13\n",
    "\n",
    "    # Initialize\n",
    "    res_local_weights = pd.DataFrame()\n",
    "    res_global_weights = pd.DataFrame()\n",
    "    res_local_cv = pd.DataFrame()\n",
    "    res_global_cv = pd.DataFrame()\n",
    "\n",
    "    # For each rolling horizon\n",
    "    for T_horizon_rolling in T_horizon_rolling_range:\n",
    "\n",
    "        # Get weights\n",
    "        with open(PATH_SAMPLES+'/SKU'+str(SKU)+'/Static/Weights'+\n",
    "                  str(T_horizon_rolling)+'/weights_local_ij.p', 'rb') as f:\n",
    "            weights_local_ij = pickle.load(f)\n",
    "        del f\n",
    "        \n",
    "        with open(PATH_SAMPLES+'/SKU'+str(SKU)+'/Static/Weights'+\n",
    "                  str(T_horizon_rolling)+'/weights_global_ij.p', 'rb') as f:\n",
    "            weights_global_ij = pickle.load(f)\n",
    "        del f\n",
    "\n",
    "        # Get samples\n",
    "        robj = pyreadr.read_r(PATH_SAMPLES+'/SKU'+str(SKU)+'/Static/TmpFiles'+\n",
    "                              str(T_horizon_rolling)+'/Y_samples_mv_k.RDS')\n",
    "        y_samples_SKU = robj[None]\n",
    "        \n",
    "        robj = pyreadr.read_r(PATH_DATA+'/Y_Data_mv_NEW.RData')\n",
    "        y_samples = robj['Y_Data_mv']\n",
    "\n",
    "        robj = pyreadr.read_r(PATH_SAMPLES+'/SKU'+str(SKU)+'/Static/TmpFiles'+\n",
    "                              str(T_horizon_rolling)+'/ID_samples_k.RDS')\n",
    "        ID_samples_SKU = robj[None]\n",
    "\n",
    "        robj = pyreadr.read_r(PATH_DATA+'/ID_Data_NEW.RData')\n",
    "        ID_samples = robj['ID_Data']\n",
    "\n",
    "        # Get sampling\n",
    "        robj = pyreadr.read_r(PATH_SAMPLES+'/SKU'+str(SKU)+'/Static/TmpFiles'+\n",
    "                              str(T_horizon_rolling)+'/sampleUpTo_start.RDS')\n",
    "        sampleUpTo_start = robj[None].iloc[0,0]\n",
    "\n",
    "\n",
    "        ## Iterate over full time horizon\n",
    "        for t_current in range(T_horizon): \n",
    "\n",
    "            # Set current model params\n",
    "            model_params = dict({\n",
    "                'SKU': SKU,\n",
    "                'T_horizon_rolling': T_horizon_rolling,\n",
    "                'T': min(T_horizon_rolling,T_horizon-t_current),\n",
    "                't_current': t_current,\n",
    "                'N': int(sampleUpTo_start+t_current)\n",
    "            })    \n",
    "            \n",
    "            # Diagnostic results - local\n",
    "            weights_x_t, cv_x_t = getSampleWeightsDiagnostic(\n",
    "                'local',\n",
    "                model_params,  \n",
    "                weights_local_ij,\n",
    "                y_samples, \n",
    "                y_samples_SKU, \n",
    "                ID_samples, \n",
    "                ID_samples_SKU\n",
    "            )   \n",
    "            \n",
    "            if not res_local_weights.empty:\n",
    "                res_local_weights = res_local_weights.append(weights_x_t)   \n",
    "                res_local_cv = res_local_cv.append(cv_x_t)   \n",
    "            else:\n",
    "                res_local_weights = pd.DataFrame(weights_x_t) \n",
    "                res_local_cv = pd.DataFrame(cv_x_t) \n",
    "                                   \n",
    "            \n",
    "            # Diagnostic results - global\n",
    "            weights_x_t, cv_x_t = getSampleWeightsDiagnostic(\n",
    "                'global',\n",
    "                model_params,  \n",
    "                weights_global_ij,\n",
    "                y_samples, \n",
    "                y_samples_SKU, \n",
    "                ID_samples, \n",
    "                ID_samples_SKU\n",
    "            )\n",
    "             \n",
    "            if not res_global_weights.empty:\n",
    "                res_global_weights = res_global_weights.append(weights_x_t)   \n",
    "                res_global_cv = res_global_cv.append(cv_x_t)   \n",
    "            else:\n",
    "                res_global_weights = pd.DataFrame(weights_x_t) \n",
    "                res_global_cv = pd.DataFrame(cv_x_t) \n",
    "     \n",
    "\n",
    "                \n",
    "    ## Summarise\n",
    "    \n",
    "    # Weights\n",
    "    global_weights = res_global_weights[['SKU', 'T_horizon_rolling', 't', 'w_hat']]\n",
    "    local_weights = res_local_weights[['SKU', 'T_horizon_rolling', 't', 'w_hat']]   \n",
    "    \n",
    "    # Gini coefficient\n",
    "    global_weights_gini = res_global_weights[\n",
    "        ['SKU', 'T_horizon_rolling', 't', 'w_hat']].groupby(\n",
    "        ['SKU', 'T_horizon_rolling', 't']).agg(\n",
    "        gini_coeff = ('w_hat', gini),\n",
    "        gini_coeff_pos = ('w_hat', lambda x: gini(x.loc[x>0]))).reset_index()\n",
    "    local_weights_gini = res_local_weights[\n",
    "        ['SKU', 'T_horizon_rolling', 't', 'w_hat']].groupby(\n",
    "        ['SKU', 'T_horizon_rolling', 't']).agg(\n",
    "        gini_coeff = ('w_hat', gini),\n",
    "        gini_coeff_pos = ('w_hat', lambda x: gini(x.loc[x>0]))).reset_index()\n",
    "    \n",
    "     # Weights x SKU - global\n",
    "    global_weights_xSKU = res_global_weights.groupby(\n",
    "        ['SKU', 'SKU_API', 'x_SKU_API', 'T_horizon_rolling', 't']\n",
    "    ).agg(\n",
    "\n",
    "        w_hat = ('w_hat', sum),\n",
    "        n_weights = ('w_hat', len),\n",
    "        n_nonZeroWeights = ('w_hat', lambda w: sum(w>0))\n",
    "\n",
    "        ).groupby(\n",
    "        ['SKU', 'SKU_API', 'x_SKU_API', 'T_horizon_rolling']\n",
    "    ).agg(\n",
    "\n",
    "        w_hat = ('w_hat', np.mean),\n",
    "        n_weights = ('n_weights', np.mean),\n",
    "        n_nonZeroWeights = ('n_nonZeroWeights', np.mean)\n",
    "\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Weights x SKU - local\n",
    "    local_weights_xSKU = res_local_weights.groupby(\n",
    "        ['SKU', 'SKU_API', 'x_SKU_API', 'T_horizon_rolling', 't']\n",
    "    ).agg(\n",
    "\n",
    "        w_hat = ('w_hat', sum),\n",
    "        n_weights = ('w_hat', len),\n",
    "        n_nonZeroWeights = ('w_hat', lambda w: sum(w>0))\n",
    "\n",
    "        ).groupby(\n",
    "        ['SKU', 'SKU_API', 'x_SKU_API', 'T_horizon_rolling']\n",
    "    ).agg(\n",
    "\n",
    "        w_hat = ('w_hat', np.mean),\n",
    "        n_weights = ('n_weights', np.mean),\n",
    "        n_nonZeroWeights = ('n_nonZeroWeights', np.mean)\n",
    "\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Demand CV\n",
    "    global_cv = copy.deepcopy(res_global_cv)\n",
    "    local_cv = copy.deepcopy(res_local_cv)\n",
    "    \n",
    "    # Retun\n",
    "    return (\n",
    "        local_weights, \n",
    "        global_weights, \n",
    "        local_weights_xSKU,\n",
    "        global_weights_xSKU, \n",
    "        local_weights_gini,\n",
    "        global_weights_gini,\n",
    "        local_cv, \n",
    "        global_cv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "85042919-0505-4f2e-aa7f-25a69a234a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel process tracker\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d6ed646c-d5b1-4e28-a552-03ee2ec72b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [11:50<00:00,  1.55s/it] \n"
     ]
    }
   ],
   "source": [
    "## Run\n",
    "\n",
    "# Set folder names\n",
    "PATH_DATA = '/home/fesc/Data'\n",
    "PATH_PARAMS  = '/home/fesc/Data/Params'\n",
    "PATH_SAMPLES = '/home/fesc/Data/Samples'\n",
    "PATH_RESULTS = '/home/fesc/Data/Results'\n",
    "\n",
    "\n",
    "# SKUs\n",
    "SKU_range=range(1,460+1)\n",
    "\n",
    "# Cores\n",
    "n_jobs=32\n",
    "\n",
    "# Run\n",
    "with tqdm_joblib(tqdm(desc=\"Progress\", total=len(SKU_range))) as progress_bar:\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(runSampleWeightsDiagnostic)(SKU, \n",
    "                                                                          PATH_SAMPLES,\n",
    "                                                                          PATH_DATA) \n",
    "                                      for SKU in SKU_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8a8a878c-3ece-4d53-9976-3240afc35a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate results\n",
    "# result_local_weights = pd.DataFrame()\n",
    "# result_global_weights = pd.DataFrame()\n",
    "# result_local_weights_xSKU = pd.DataFrame()\n",
    "# result_global_weights_xSKU = pd.DataFrame()\n",
    "# result_local_weights_gini = pd.DataFrame()\n",
    "# result_global_weights_gini = pd.DataFrame()\n",
    "result_local_cv = pd.DataFrame()\n",
    "result_global_cv = pd.DataFrame()\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    # Unpack\n",
    "    (\n",
    "        local_weights, \n",
    "        global_weights, \n",
    "        local_weights_xSKU,\n",
    "        global_weights_xSKU, \n",
    "        local_weights_gini,\n",
    "        global_weights_gini,\n",
    "        local_cv, \n",
    "        global_cv\n",
    "    ) = result\n",
    "\n",
    "    ## Append\n",
    "#     local_weights['n']=1\n",
    "#     result_local_weights = result_local_weights.append(local_weights).groupby(['w_hat']).agg(\n",
    "#         n = ('n', sum)).reset_index()\n",
    "    \n",
    "#     global_weights['n']=1\n",
    "#     result_global_weights = result_global_weights.append(global_weights).groupby(['w_hat']).agg(\n",
    "#         n = ('n', sum)).reset_index()\n",
    "         \n",
    "#     result_local_weights_xSKU = result_local_weights_xSKU.append(local_weights_xSKU)\n",
    "#     result_global_weights_xSKU = result_global_weights_xSKU.append(global_weights_xSKU)\n",
    "    \n",
    "#     result_local_weights_gini = result_local_weights_gini.append(local_weights_gini)\n",
    "#     result_global_weights_gini = result_global_weights_gini.append(global_weights_gini)\n",
    "        \n",
    "    result_local_cv = result_local_cv.append(local_cv)\n",
    "    result_global_cv = result_global_cv.append(global_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0ab1a82e-b410-4157-8493-f894703004bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results\n",
    "# result_local_weights.to_csv(\n",
    "#     path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_local_allWeights.csv' , \n",
    "#     sep=','\n",
    "# )\n",
    "# result_global_weights.to_csv(\n",
    "#     path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_global_allWeights.csv' , \n",
    "#     sep=','\n",
    "# )\n",
    "# result_local_weights_xSKU.to_csv(\n",
    "#     path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_local_weightsSKU.csv' , \n",
    "#     sep=','\n",
    "# )\n",
    "# result_global_weights_xSKU.to_csv(\n",
    "#     path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_global_weightsSKU.csv' , \n",
    "#     sep=','\n",
    "# )\n",
    "\n",
    "# result_local_weights_gini.to_csv(\n",
    "#     path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_local_weightsGini.csv' , \n",
    "#     sep=','\n",
    "# )\n",
    "\n",
    "# result_global_weights_gini.to_csv(\n",
    "#     path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_global_weightsGini.csv' , \n",
    "#     sep=','\n",
    "# )\n",
    "\n",
    "result_local_cv.to_csv(\n",
    "    path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_local_demandCV.csv' , \n",
    "    sep=','\n",
    ")\n",
    "\n",
    "result_global_cv.to_csv(\n",
    "    path_or_buf=PATH_RESULTS+'/WeightsDiagnostic_global_demandCV.csv' , \n",
    "    sep=','\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f256d-23ca-44e9-af63-f608e1056d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1f072-d39d-4f4b-9faf-edfe9f0f3be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e598ffb-a126-4be3-b904-8d0f1f74fe37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0ec8f-7553-43f7-aba2-e67840d6f419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0e373-df7c-42ab-915b-82c6bcce8ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0089669-223e-4412-8ec4-812733636f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a47477-4de2-42fc-85e6-2e4842b323a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084c3cb-a7b7-4ab4-890e-3ce1389d0221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiPeriodInv",
   "language": "python",
   "name": "multiperiodinv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
