{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112af4de-adcf-410d-81ea-295b956548f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import pyreadr\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bbed8-ef18-44a5-bc3c-beffacee976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import WeightsKernel\n",
    "from WeightsKernel import RandomForestWeightsKernel\n",
    "from WeightsKernel import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a5489-8cb0-4a4e-8436-e5d96eca4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reshaping based on rolling horizon (iid)\n",
    "def reshape_data(ID_Data_train, X_Data_train, Y_Data_train, test_start, tau, iid = True):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Function that creates training data based on start of test horizon (indicated by the first sale_yearweek\n",
    "    of the test horizon) and given the rolling horizon reshapes the data as consecutive demand vectors\n",
    "    whose length equals the length of the rolling horizon. If iid == True then reshapes such that the\n",
    "    rows of the data are iid in the sense that consecutive demand vectors do not overlap.\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "        ID_Data_train: data frame storing identifiers\n",
    "        X_Data_train: feature data frame\n",
    "        Y_Data_train: demand data frame\n",
    "        test_start: start period of test horizon\n",
    "        tau: look-ahead\n",
    "        iid: True if data should be reshaped to iid\n",
    "\n",
    "    Outputs: \n",
    "\n",
    "        id_train: reshaped data frame storing identifiers\n",
    "        X_train: reshaped feature data as np.array \n",
    "        y_train: reshaped demand data as np.array \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Combine IDs with Y data, sort by SKU and sale yearweek, and select training data\n",
    "    data = pd.concat([ID_Data_train, Y_Data_train], axis=1)\n",
    "\n",
    "    # Create look-aheads given rolling horizon\n",
    "    Y = {}\n",
    "    for tau in range(0,tau+1):\n",
    "        Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "\n",
    "    Y_Data_train = pd.DataFrame(Y)\n",
    "\n",
    "    # Remove look-ahead that would run over training horizon\n",
    "    ID_Data_train = ID_Data_train.loc[~np.isnan(np.array(Y_Data_train.iloc[:,0:(tau+1)])).any(axis=1)]\n",
    "    X_Data_train = X_Data_train.loc[~np.isnan(np.array(Y_Data_train.iloc[:,0:(tau+1)])).any(axis=1)]\n",
    "    Y_Data_train = Y_Data_train.loc[~np.isnan(np.array(Y_Data_train.iloc[:,0:(tau+1)])).any(axis=1)]\n",
    "\n",
    "       \n",
    "    ## Reshape to iid\n",
    "    if iid:\n",
    "\n",
    "        ## Get slices of sale yearweeks to ensure iid data\n",
    "        max_sale_yearweek = min(max(ID_Data_train.sale_yearweek),test_start-(tau+1))\n",
    "\n",
    "        slices = []\n",
    "        factor=0\n",
    "        step=0\n",
    "\n",
    "        # Get every T_horizon_rolling'th sale_yearweek starting from the last sale_yearweek\n",
    "        while max_sale_yearweek - step > 0:\n",
    "            factor = factor + 1\n",
    "            slices = slices + [max_sale_yearweek - step]\n",
    "            step = (tau+1) * factor        \n",
    "\n",
    "        # Apply slices\n",
    "        Y_Data_train = Y_Data_train.loc[ID_Data_train.sale_yearweek.isin(slices)]\n",
    "        X_Data_train = X_Data_train.loc[ID_Data_train.sale_yearweek.isin(slices)]\n",
    "        ID_Data_train = ID_Data_train.loc[ID_Data_train.sale_yearweek.isin(slices)]\n",
    "\n",
    "    \n",
    "    # Tansfrom data to arrays\n",
    "    id_train = ID_Data_train\n",
    "    X_train = np.array(X_Data_train)\n",
    "    y_train = np.array(Y_Data_train)\n",
    "    y_train = y_train.flatten() if y_train.shape[1] == 1 else y_train\n",
    "\n",
    "    # Return \n",
    "    return id_train, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86611211-35a9-4d74-b0a5-de5908d82ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reshaping based on rolling horizon (iid)\n",
    "def reshape_data(data, timePeriods, maxTimePeriod, tau, iid = True):\n",
    "\n",
    "\n",
    " \n",
    "    ## Reshape to iid\n",
    "    if iid:\n",
    "\n",
    "        ## Get slices of timePeriods\n",
    "        maxTimePeriod = min(max(timePeriods), maxTimePeriod-tau)\n",
    "\n",
    "        slices = []\n",
    "        factor = 0\n",
    "        step = 0\n",
    "        \n",
    "        while maxTimePeriod - step > 0:\n",
    "            factor = factor + 1\n",
    "            slices = slices + [maxTimePeriod - step]\n",
    "            step = (tau+1) * factor        \n",
    "\n",
    "        # Apply slices\n",
    "        sliced_data = data.loc[timePeriods.isin(slices)]\n",
    "\n",
    "    # Return \n",
    "    return sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b12e3-0f5d-4809-acc6-35951509908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_variables(vars_to_scale, vars_to_scale_with, scaler):\n",
    "    \n",
    "    scaler_fitted = scaler.fit(vars_to_scale_with)\n",
    "    \n",
    "    vars_scaled = scaler_fitted.transform(vars_to_scale)\n",
    "\n",
    "    return vars_scaled, scaler_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff0b9e-1200-46f7-a663-3a4ba8bef0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder names as global variables\n",
    "os.chdir('/home/fesc/MM/')\n",
    "global PATH_DATA, PATH_PARAMS, PATH_SAMPLES, PATH_RESULTS\n",
    "\n",
    "PATH_DATA = '/home/fesc/MM/Data'\n",
    "PATH_PARAMS  = '/home/fesc/MM/Data/Params'\n",
    "PATH_KERNELS = '/home/fesc/MM/Data/Kernels'\n",
    "PATH_SAMPLES = '/home/fesc/MM/Data/Samples'\n",
    "PATH_RESULTS = '/home/fesc/MM/Data/Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dce9bd-11aa-4a16-b5c3-347b52b757c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period and SKU ranges\n",
    "T = 13\n",
    "Tau = 5\n",
    "t_range = range(0,T)\n",
    "tau_range = range(0,Tau)\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Train/test split\n",
    "test_start = 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966a209-bf5c-48a7-a26c-7d8f16a73c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb38ac6-bb28-43b4-893a-08bda2729d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "X_Data_train = X_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "Y_Data_train = Y_Data.loc[ID_Data.sale_yearweek < test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d981e-028c-4181-b844-c80355a90e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features\n",
    "\n",
    "# Prep scaling\n",
    "vars_to_scale_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'Feature'].values\n",
    "vars_to_scale_with_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'ScaleWith'].values\n",
    "\n",
    "vars_to_scale = np.array(X_Data[vars_to_scale_names])\n",
    "vars_to_scale_with = np.array(X_Data_train[vars_to_scale_with_names])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale\n",
    "vars_scaled, scaler_fitted = scale_variables(vars_to_scale, vars_to_scale_with, scaler)\n",
    "\n",
    "# Add back column names\n",
    "vars_scaled = pd.DataFrame(vars_scaled, columns=vars_to_scale_names)\n",
    "\n",
    "# Scaled features\n",
    "X_Data_scaled = copy.deepcopy(X_Data)\n",
    "for col in vars_scaled.columns:\n",
    "    X_Data_scaled[col] = vars_scaled[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2cf2c-2f65-4ba0-a554-a3111dea1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demand\n",
    "\n",
    "# Prep scaling\n",
    "vars_to_scale = np.array(Y_Data)\n",
    "vars_to_scale_with = np.array(Y_Data_train)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale\n",
    "vars_scaled, scaler_fitted = scale_variables(vars_to_scale, vars_to_scale_with, scaler)\n",
    "Y_Data_scaled = pd.DataFrame(vars_scaled, columns=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5082f-e69b-411c-b6de-20fd1cd7d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reshaping for multi-period modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2507a6-268c-4294-b61c-e5d3c4e1add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of training data for scaled data\n",
    "X_Data_scaled_train = X_Data_scaled.loc[ID_Data.sale_yearweek < test_start]\n",
    "Y_Data_scaled_train = Y_Data_scaled.loc[ID_Data.sale_yearweek < test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb7a32-f678-4424-8bec-3349e5288362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape demand to multi-period\n",
    "data = pd.concat([ID_Data_train, Y_Data_scaled_train], axis=1)\n",
    "Y = {}\n",
    "for tau in tau_range:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data_scaled_train = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafaac6e-fc00-43bb-9c87-3c733ac437cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d784bb-44f0-4832-9fb3-2e892cfc3db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db3bfb-b1f9-4fb4-9186-558327aed316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d1b5c-9526-4263-bd2e-3597e7981c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e4ede-076e-44a5-9be4-f6ee89c77744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3187ab-f755-4e74-8077-c86aa46f819d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51957b83-c79a-41e9-b0fb-d6096bf84ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set paths and names\n",
    "path_cv = '/home/fesc/MM/Data/Kernels'\n",
    "name_cv = 'cv_rfwk_global_scaled'\n",
    "path_kernel = '/home/fesc/MM/Data/Kernels'\n",
    "name_kernel = 'rfwk_global_scaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008879c-a118-478f-90e8-cb574603f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters to tune random forest weights kernels\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(20, 1000, 20)],  \n",
    "    'min_samples_leaf': [x for x in range(10, 1000, 10)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'random_search': True,\n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 8,\n",
    "    'verbose': 2\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec335c0-4b60-4542-bae9-c732e0fa91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tune random forest weights kernels\n",
    "\n",
    "# For tau=0,...,4\n",
    "for tau in tau_range:\n",
    "\n",
    "    # Tansfrom data to arrays\n",
    "    X_train = np.array(X_Data_scaled_train)\n",
    "    y_train = np.array(Y_Data_scaled_train.iloc[:,0:(tau+1)])\n",
    "\n",
    "    # Remove look-ahead that would run over training horizon\n",
    "    id_train = ID_Data_train.loc[~np.isnan(y_train).any(axis=1),:]\n",
    "    X_train = X_train[~np.isnan(y_train).any(axis=1),:]\n",
    "    y_train = y_train[~np.isnan(y_train).any(axis=1),:]\n",
    "    y_train = y_train.flatten() if y_train.shape[1] == 1 else y_train\n",
    "    \n",
    "    # Initialize\n",
    "    prep = PreProcessor()\n",
    "    kernel = RandomForestWeightsKernel()\n",
    "\n",
    "    # CV folds\n",
    "    cv_folds = prep.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = kernel.tune(X=X_train, y=y_train, cv_folds=cv_folds, model_params=model_params, \n",
    "                             tuning_params=tuning_params, hyper_params_grid=hyper_params_grid)\n",
    "    \n",
    "    # Save\n",
    "    kernel.save_cv_result(path=path_cv+'/'+name_cv+'_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f3580-80fe-4199-8a40-d292b1489ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit random forest weights kernels and generate weights\n",
    "\n",
    "# For period t=1,...,T\n",
    "for t in t_range:\n",
    "    \n",
    "    # Selection of training data\n",
    "    ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start + t]\n",
    "    X_Data_scaled_train = X_Data_scaled.loc[ID_Data.sale_yearweek < test_start + t]\n",
    "    Y_Data_scaled_train = Y_Data_scaled.loc[ID_Data.sale_yearweek < test_start + t]\n",
    "\n",
    "    # Selection of test data\n",
    "    ID_Data_test = ID_Data.loc[ID_Data.sale_yearweek == test_start + t]\n",
    "    X_Data_scaled_test = X_Data_scaled.loc[ID_Data.sale_yearweek == test_start + t]\n",
    "    Y_Data_scaled_test = Y_Data_scaled.loc[ID_Data.sale_yearweek == test_start + t]\n",
    "\n",
    "    # Adjust tau's to account of end of planning horizon\n",
    "    tau_range = range(0,min(max(tau_range),T-t-1)+1)\n",
    "    \n",
    "    # For tau=0,...,4\n",
    "    for tau in tau_range:\n",
    "        \n",
    "        # Print status\n",
    "        start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        print('## Period = '+str(t+1)+', rolling horizon = '+str(tau+1)+': Fitting random forest weights kernel...')\n",
    "        \n",
    "        # Tansfrom data to arrays\n",
    "        X_train, X_test = np.array(X_Data_scaled_train), np.array(X_Data_scaled_test)\n",
    "        y_train, y_test = np.array(Y_Data_scaled_train.iloc[:,0:(tau+1)]), np.array(Y_Data_scaled_test.iloc[:,0:(tau+1)])\n",
    "        \n",
    "        # Remove look-ahead that would run over training horizon\n",
    "        id_train = ID_Data_train.loc[~np.isnan(y_train).any(axis=1),:]\n",
    "        X_train = X_train[~np.isnan(y_train).any(axis=1),:]\n",
    "        y_train = y_train[~np.isnan(y_train).any(axis=1),:]\n",
    "        \n",
    "        # Reshape y\n",
    "        y_train = y_train.flatten() if y_train.shape[1] == 1 else y_train\n",
    "        y_test = y_test.flatten() if y_test.shape[1] == 1 else y_test\n",
    "                \n",
    "        # Initialize weights kernel\n",
    "        kernel = RandomForestWeightsKernel()\n",
    "\n",
    "        # Load cv results\n",
    "        kernel.load_cv_result(path=path_cv+'/'+name_cv+'_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "        # Fit random forest weights kernel\n",
    "        kernel.fit(X=X_train, y=y_train, model_params={'n_jobs': 32, 'verbose': 1})\n",
    "        \n",
    "        # Save fit\n",
    "        kernel.save_fit(path=path_kernel+'/'+name_kernel+'_t'+str(t+1)+'_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "        # Print status\n",
    "        print('## ... fit took', dt.datetime.now().replace(microsecond=0) - start_time)         \n",
    "        \n",
    "        # Get weights\n",
    "\n",
    "        # Get samples \n",
    "\n",
    "        # Get weights diagnostics (RMSSE, SPL, etc. ...?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0ff291-7d3e-4b07-8d69-35f33c2d5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Period = 1, rolling horizon = 1:\n",
      "#### Period = 1, rolling horizon = 2:\n",
      "#### Period = 1, rolling horizon = 3:\n",
      "#### Period = 1, rolling horizon = 4:\n",
      "#### Period = 1, rolling horizon = 5:\n",
      "#### Period = 2, rolling horizon = 1:\n",
      "#### Period = 2, rolling horizon = 2:\n",
      "#### Period = 2, rolling horizon = 3:\n",
      "#### Period = 2, rolling horizon = 4:\n",
      "#### Period = 2, rolling horizon = 5:\n",
      "#### Period = 3, rolling horizon = 1:\n",
      "#### Period = 3, rolling horizon = 2:\n",
      "#### Period = 3, rolling horizon = 3:\n",
      "#### Period = 3, rolling horizon = 4:\n",
      "#### Period = 3, rolling horizon = 5:\n",
      "#### Period = 4, rolling horizon = 1:\n",
      "#### Period = 4, rolling horizon = 2:\n",
      "#### Period = 4, rolling horizon = 3:\n",
      "#### Period = 4, rolling horizon = 4:\n",
      "#### Period = 4, rolling horizon = 5:\n",
      "#### Period = 5, rolling horizon = 1:\n",
      "#### Period = 5, rolling horizon = 2:\n",
      "#### Period = 5, rolling horizon = 3:\n",
      "#### Period = 5, rolling horizon = 4:\n",
      "#### Period = 5, rolling horizon = 5:\n",
      "#### Period = 6, rolling horizon = 1:\n",
      "#### Period = 6, rolling horizon = 2:\n",
      "#### Period = 6, rolling horizon = 3:\n",
      "#### Period = 6, rolling horizon = 4:\n",
      "#### Period = 6, rolling horizon = 5:\n",
      "#### Period = 7, rolling horizon = 1:\n",
      "#### Period = 7, rolling horizon = 2:\n",
      "#### Period = 7, rolling horizon = 3:\n",
      "#### Period = 7, rolling horizon = 4:\n",
      "#### Period = 7, rolling horizon = 5:\n",
      "#### Period = 8, rolling horizon = 1:\n",
      "#### Period = 8, rolling horizon = 2:\n",
      "#### Period = 8, rolling horizon = 3:\n",
      "#### Period = 8, rolling horizon = 4:\n",
      "#### Period = 8, rolling horizon = 5:\n",
      "#### Period = 9, rolling horizon = 1:\n",
      "#### Period = 9, rolling horizon = 2:\n",
      "#### Period = 9, rolling horizon = 3:\n",
      "#### Period = 9, rolling horizon = 4:\n",
      "#### Period = 9, rolling horizon = 5:\n",
      "#### Period = 10, rolling horizon = 1:\n",
      "#### Period = 10, rolling horizon = 2:\n",
      "#### Period = 10, rolling horizon = 3:\n",
      "#### Period = 10, rolling horizon = 4:\n",
      "#### Period = 11, rolling horizon = 1:\n",
      "#### Period = 11, rolling horizon = 2:\n",
      "#### Period = 11, rolling horizon = 3:\n",
      "#### Period = 12, rolling horizon = 1:\n",
      "#### Period = 12, rolling horizon = 2:\n",
      "#### Period = 13, rolling horizon = 1:\n"
     ]
    }
   ],
   "source": [
    "T=13\n",
    "Tau=4\n",
    "\n",
    "\n",
    "# Fit weights model and generate weights for period t=1,...,T\n",
    "for t in range(0,T):\n",
    "    \n",
    "    # Adjust tau's to account of end of planning horizon\n",
    "    tau_range = range(0,min(max(range(0,Tau+1)),T-t-1)+1)\n",
    "    \n",
    "    # For tau=0,...,4\n",
    "    for tau in tau_range:\n",
    "        \n",
    "        # Status\n",
    "        print('#### Period = '+str(t+1)+', rolling horizon = '+str(tau+1)+':')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd73505-cf98-4212-b0dd-47d4527eabf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d73161-c9d2-45d6-a581-7d01d6274d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb127bec-47a7-4472-a0f9-69bacf4d6009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921798c-cab6-4c71-a18a-f9168d1e31dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbf079-2d40-4aae-8e61-b0a9519e2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ToDo: add CPU timing, add options for iid reshaping yes/no, scaling yes/no\n",
    "## tbc: extract/prep samples, add weights diagnostic (e.g., RMSSE, SPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90540b6c-c9f0-4c76-8993-b2169313c89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d6a8a-4ac2-4f53-8c82-98424f53a516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95df8d-6198-45c9-b9c8-50096bd87c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048594c4-5fa8-4f42-bebf-791f9359359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "weights = kernel.apply(X_test)    \n",
    "\n",
    "# Get weights diagnostics (RMSSE, SPL, etc. ...?) ...\n",
    "\n",
    "# Save weights, samples, diagnostics ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a62f3-f858-4f31-bdc1-c04009488a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7acbba-6f21-4109-add7-69a28f106525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721f368-2c5b-440a-a4ec-d507e3404d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### IID #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4b74f-da2d-46cb-ac4c-4f905a6afef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set paths and names\n",
    "path_cv = '/home/fesc/MM/Data/Kernels'\n",
    "name_cv = 'cv_rfwk_global_scaled_iid'\n",
    "path_kernel = '/home/fesc/MM/Data/Kernels'\n",
    "name_kernel = 'rfwk_global_scaled_iid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af8be-22ab-443e-b40e-37c42e183a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters to tune random forest weights kernels\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(20, 1000, 20)],  \n",
    "    'min_samples_leaf': [x for x in range(10, 1000, 10)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'random_search': True,\n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 8,\n",
    "    'verbose': 2\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559efe01-5c0c-40af-b6ed-f0a4b299b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tune random forest weights kernels\n",
    "\n",
    "# Time period and SKU ranges\n",
    "T = 13\n",
    "Tau = 5\n",
    "t_range = range(0,T)\n",
    "tau_range = range(0,Tau)\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Train/test split\n",
    "test_start = 114\n",
    "\n",
    "# For tau=0,...,4\n",
    "for tau in tau_range:\n",
    "    \n",
    "    # Reshape\n",
    "    id_train = reshape_data(data=ID_Data_train, timePeriods=ID_Data_train.sale_yearweek, maxTimePeriod=test_start-1, tau=tau, iid=True)\n",
    "    X_train = reshape_data(data=X_Data_scaled_train, timePeriods=ID_Data_train.sale_yearweek, maxTimePeriod=test_start-1, tau=tau, iid=True)\n",
    "    y_train = reshape_data(data=Y_Data_scaled_train, timePeriods=ID_Data_train.sale_yearweek, maxTimePeriod=test_start-1, tau=tau, iid=True)\n",
    "    \n",
    "    # Tansfrom data to arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train.iloc[:,0:(tau+1)])\n",
    "    y_train = y_train.flatten() if y_train.shape[1] == 1 else y_train    \n",
    "    \n",
    "    # Initialize\n",
    "    prep = PreProcessor()\n",
    "    kernel = RandomForestWeightsKernel()\n",
    "\n",
    "    # CV folds\n",
    "    cv_folds = prep.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = kernel.tune(X=X_train, y=y_train, cv_folds=cv_folds, model_params=model_params, \n",
    "                             tuning_params=tuning_params, hyper_params_grid=hyper_params_grid)\n",
    "    \n",
    "    # Save\n",
    "    kernel.save_cv_result(path=path_cv+'/'+name_cv+'_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25994cc-b1df-4cab-acc2-4888d983ce40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Fit random forest weights kernels and generate weights\n",
    "\n",
    "# Time period and SKU ranges\n",
    "T = 13\n",
    "Tau = 5\n",
    "t_range = range(0,T)\n",
    "tau_range = range(0,Tau)\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Train/test split\n",
    "test_start = 114\n",
    "\n",
    "# Initialize\n",
    "weights = {}\n",
    "exec_time_sec = {}\n",
    "cpu_time_sec = {}\n",
    "\n",
    "\n",
    "# For period t=1,...,T\n",
    "for t in t_range:\n",
    "    \n",
    "    # Selection of training data\n",
    "    ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start + t]\n",
    "    X_Data_scaled_train = X_Data_scaled.loc[ID_Data.sale_yearweek < test_start + t]\n",
    "    Y_Data_scaled_train = Y_Data_scaled.loc[ID_Data.sale_yearweek < test_start + t]\n",
    "\n",
    "    # Selection of test data\n",
    "    ID_Data_test = ID_Data.loc[ID_Data.sale_yearweek == test_start + t]\n",
    "    X_Data_scaled_test = X_Data_scaled.loc[ID_Data.sale_yearweek == test_start + t]\n",
    "    Y_Data_scaled_test = Y_Data_scaled.loc[ID_Data.sale_yearweek == test_start + t]\n",
    "\n",
    "    # Adjust tau's to account of end of planning horizon\n",
    "    tau_range = range(0,min(max(tau_range),T-t-1)+1)\n",
    "    \n",
    "    # Initialize\n",
    "    weights[t] = {}\n",
    "    exec_time_sec[t] = {}\n",
    "    cpu_time_sec[t] = {}\n",
    "    \n",
    "    # For tau=0,...,4\n",
    "    for tau in tau_range:\n",
    "        \n",
    "        # Status\n",
    "        print('#### Period = '+str(t+1)+', rolling horizon = '+str(tau+1)+':')\n",
    "        print('# Fitting random forest weights kernel...')\n",
    "        \n",
    "        # Timer\n",
    "        exec_time_sec[t][tau] = {}\n",
    "        cpu_time_sec[t][tau] = {}\n",
    "        start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        st_exec = time.time()\n",
    "        st_cpu = time.process_time()  \n",
    "        \n",
    "        # Reshape\n",
    "        id_train = reshape_data(data=ID_Data_train, timePeriods=ID_Data_train.sale_yearweek, maxTimePeriod=test_start-1, tau=tau, iid=True)\n",
    "        X_train = reshape_data(data=X_Data_scaled_train, timePeriods=ID_Data_train.sale_yearweek, maxTimePeriod=test_start-1, tau=tau, iid=True)\n",
    "        y_train = reshape_data(data=Y_Data_scaled_train, timePeriods=ID_Data_train.sale_yearweek, maxTimePeriod=test_start-1, tau=tau, iid=True)\n",
    "\n",
    "        # Tansfrom data to arrays\n",
    "        X_train, X_test = np.array(X_train), np.array(X_Data_scaled_test)\n",
    "        y_train, y_test = np.array(y_train.iloc[:,0:(tau+1)]), np.array(Y_Data_scaled_test.iloc[:,0:(tau+1)])\n",
    "        y_train = y_train.flatten() if y_train.shape[1] == 1 else y_train    \n",
    "        y_test = y_test.flatten() if y_test.shape[1] == 1 else y_test\n",
    "                \n",
    "        # Initialize weights kernel\n",
    "        kernel = RandomForestWeightsKernel()\n",
    "\n",
    "        # Load cv results\n",
    "        kernel.load_cv_result(path=path_cv+'/'+name_cv+'_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "        # Fit random forest weights kernel\n",
    "        kernel.fit(X=X_train, y=y_train, model_params={'n_jobs': 32, 'verbose': 1})\n",
    "        \n",
    "        # Save fit\n",
    "        kernel.save_fit(path=path_kernel+'/'+name_kernel+'_t'+str(t+1)+'_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "        # Timer\n",
    "        end_time = dt.datetime.now().replace(microsecond=0) - start_time\n",
    "        exec_time_sec[t][tau]['fit'] = time.time()-st_exec\n",
    "        cpu_time_sec[t][tau]['fit'] = time.process_time()-st_cpu\n",
    "\n",
    "        # Status\n",
    "        print('# ...done in', end_time)         \n",
    "        print('# Generating weights...')\n",
    "        \n",
    "        # Timer\n",
    "        start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        st_exec = time.time()\n",
    "        st_cpu = time.process_time()  \n",
    "        \n",
    "        # Get weights\n",
    "        weights[t][tau] = kernel.apply(X_test, model_params={'n_jobs': 32, 'verbose': 0})    \n",
    "\n",
    "        # Timer\n",
    "        end_time = dt.datetime.now().replace(microsecond=0) - start_time\n",
    "        exec_time_sec[t][tau]['weights'] = time.time()-st_exec\n",
    "        cpu_time_sec[t][tau]['weights'] = time.process_time()-st_cpu\n",
    "        \n",
    "        # Status\n",
    "        print('# ...done in', end_time)     \n",
    "        \n",
    "# Save results\n",
    "joblib.dump(weights, path_kernel+'/'+name_kernel+'_weights.joblib')    \n",
    "joblib.dump(exec_time_sec, path_kernel+'/'+name_kernel+'_weights_exec_time_sec.joblib')  \n",
    "joblib.dump(cpu_time_sec, path_kernel+'/'+name_kernel+'_weights_cpu_time_sec.joblib')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc882fb-7322-4d71-958e-2f8af0163039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9d683-94c8-41ed-9424-1ccbbc993e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a32f3-c72d-414e-aadb-aae0207eabae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640e29c-092a-4a0d-a9c9-63bc83f4ee06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302af2e0-5dd4-41ff-91c7-36fa5d859e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c953ef0-c93f-4d0a-9a07-4a60abee2b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055300a9-da4f-4201-b68a-508e28a2cd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08133e-89b9-48bd-80a1-e964c1a1b81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67419f02-f6d3-45d5-959d-e4d7f73a5a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiPeriodInv",
   "language": "python",
   "name": "multiperiodinv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
