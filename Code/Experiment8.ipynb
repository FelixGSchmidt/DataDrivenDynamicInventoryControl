{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa32c966-eb6a-49ab-8484-fdd0b7e22e0a",
   "metadata": {},
   "source": [
    "# Setup the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac97b00-9903-4bc6-a040-a8f5b7e4b68c",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f822ec-cf00-4419-bbd6-354376582363",
   "metadata": {},
   "source": [
    "## Experiment data\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ of product $j$ at time $n$\n",
    "- **Y_Data** (pd.DataFrame) is the demand time series data, i.e., each row is a demand observations $d_{j,n}$ of product $j$ at time $n$\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets\n",
    "\n",
    "Data is loaded before each experiment using **load_experiment_data()**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d67cd-e230-495a-a031-1e5a7ebc4dd7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b518e25-3378-4cde-9fca-4ea5cccd902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "import itertools\n",
    "import contextlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Weights Model\n",
    "from WeightsModel4 import PreProcessing\n",
    "from WeightsModel4 import MaxQFeatureScaler\n",
    "from WeightsModel4 import MaxQDemandScaler\n",
    "from WeightsModel4 import RandomForestWeightsModel\n",
    "\n",
    "# Import (Rolling Horizon) Weighted SAA models\n",
    "from WeightedSAA6 import WeightedSAA\n",
    "from WeightedSAA6 import RobustWeightedSAA\n",
    "from WeightedSAA6 import RollingHorizonOptimization\n",
    "\n",
    "# Import experiment ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d60c56-dbb7-42e2-a8b7-f478cb4dc818",
   "metadata": {},
   "source": [
    "## Paths and reference names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331e5ab6-1320-4122-be2b-84b7bb7f89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder names as global variables\n",
    "os.chdir('/home/fesc/DataDrivenDynamicInventoryControl/')\n",
    "global PATH_DATA, PATH_WEIGHTSMODEL, PATH_RESULTS\n",
    "\n",
    "PATH_DATA = '/home/fesc/DataDrivenDynamicInventoryControl/Data' \n",
    "PATH_WEIGHTSMODEL = '/home/fesc/DataDrivenDynamicInventoryControl/Data/WeightsModel'\n",
    "PATH_RESULTS = '/home/fesc/DataDrivenDynamicInventoryControl/Data/Results'\n",
    "\n",
    "# Weights models\n",
    "global_weightsmodel = 'rfwm_global_r_z_h' \n",
    "local_weightsmodel = 'rfwm_local_r_z_h' \n",
    "\n",
    "# Optimization models\n",
    "GwSAA = 'GwSAA'\n",
    "GwSAAR = 'GwSAAR'\n",
    "wSAA = 'wSAA'\n",
    "wSAAR = 'wSAAR'\n",
    "SAA = 'SAA'\n",
    "ExPost = 'ExPost'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4317c6a-c127-4153-92fc-3c9c3f01c96a",
   "metadata": {},
   "source": [
    "## Experiment paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448d7b6-5ed5-4c5a-98de-61f6012b6ec8",
   "metadata": {},
   "source": [
    "We run an experiment for all given products (SKUs) $k=1,...,M$ over a test planning horizon $t=1,...,T$ with $T=13$ for three different cost parameter settings $\\{K, u, h, b\\}$ that vary the critical ratio ($CR=\\frac{b}{b+h}$) of holding and backlogging yielding\n",
    "- $CR=0.50$: $\\{K=100, u=0.5, h=1, b=1\\}$\n",
    "- $CR=0.75$: $\\{K=100, u=0.5, h=1, b=3\\}$\n",
    "- $CR=0.90$: $\\{K=100, u=0.5, h=1, b=9\\}$\n",
    "\n",
    "Experiments are run for different choices of the look-ahead $\\tau=0,...,4$. For robust models, we vary the parameter $e=\\{1,3,6,9,12\\}$ (multiplier of the in-sample standard deviation of demand) defining product and sample sepcific uncertainty sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6948b9d8-5514-4c0c-b4fa-8f5e4bd9e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period and SKU ranges\n",
    "T = 13                  # Planning horizon T\n",
    "ts = range(1,13+1)      # Periods t=1,...,T of the planning horizon\n",
    "taus = range(0,4+1)     # Look-aheads tau=0,...,4 to use\n",
    "es = [1,3,6,9,12]       # Uncertainty set specifications e=1,...,12\n",
    "SKUs = range(1,460+1)   # Products (SKUs) k=1,...,M\n",
    "products = range(1,460+1)\n",
    "\n",
    "# Train/test split (first timePeriods of testing horizon)\n",
    "timePeriodsTestStart = 114\n",
    "\n",
    "# Cost param settings\n",
    "cost_params = [\n",
    "\n",
    "    {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "    {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "    {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b66664-13e7-48e6-a841-4a2c29234807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(which_data=None, path=None):\n",
    "    \n",
    "    if path is None:\n",
    "        path = ''\n",
    "        \n",
    "    # Read data\n",
    "    ID_Data = pd.read_csv(path+'/ID_Data.csv')\n",
    "    X_Data = pd.read_csv(path+'/X_Data.csv')\n",
    "    X_Data_Columns = pd.read_csv(path+'/X_Data_Columns4.csv')\n",
    "    Y_Data = pd.read_csv(path+'/Y_Data.csv')\n",
    "    \n",
    "    # Feature names\n",
    "    features = np.array(X_Data_Columns.Feature)\n",
    "    features_global = np.array(X_Data_Columns.loc[X_Data_Columns.Global == 'YES'].Feature)\n",
    "    features_global_to_scale = np.array(X_Data_Columns.loc[(X_Data_Columns.Global == 'YES') & (X_Data_Columns.Scale == 'YES')].Feature)\n",
    "    features_global_to_scale_with = np.array(X_Data_Columns.loc[(X_Data_Columns.Global == 'YES') & (X_Data_Columns.Scale == 'YES')].ScaleWith)\n",
    "    features_local = np.array(X_Data_Columns.loc[X_Data_Columns.Local == 'YES'].Feature)\n",
    "\n",
    "    # Ensure data is sorted by SKU and sale_yearweek for further preprocessing\n",
    "    data = pd.concat([ID_Data, X_Data, Y_Data], axis=1).sort_values(by=['SKU', 'sale_yearweek']).reset_index(drop=True)\n",
    "    X = np.array(data[X_Data.columns])\n",
    "    y = np.array(data[Y_Data.columns]).flatten()\n",
    "    products = np.array(data['SKU']).astype(int)\n",
    "    timePeriods = np.array(data['sale_yearweek']).astype(int)\n",
    "    \n",
    "    # Decide which data to return\n",
    "    if which_data is None:\n",
    "        \n",
    "        data = X, y, products, timePeriods, features, features_global, features_global_to_scale, features_global_to_scale_with, features_local\n",
    "    \n",
    "    elif which_data == 'global':\n",
    "\n",
    "        # Select global features\n",
    "        X = X[:,[feat in features_global for feat in features]]\n",
    "\n",
    "        data = X, y, products, timePeriods, features_global, features_global_to_scale, features_global_to_scale_with\n",
    "        \n",
    "    elif which_data == 'local':\n",
    "        \n",
    "        # Select local features\n",
    "        X = X[:,[feat in features_local for feat in features]]\n",
    "        \n",
    "        data = X, y, products, timePeriods, features_local\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb08648-4933-455e-be9e-488d8b2431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to run an experiment over a list of given cost parameter settings and the specified model\n",
    "def run_experiment(wsaamodel, cost_params, actuals, samples=None, weights=None, epsilons=None, print_progress=False,\n",
    "                   path_to_save=None, name_to_save=None, return_results=True, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Raise error if cost_params is not a list of dict(s)\n",
    "    if not type(cost_params)==list:\n",
    "        raise ValueError('Argument cost_params has to be a list of at least one dict with keys {K, u, h, b}')  \n",
    "    \n",
    "    # Timer\n",
    "    st_exec, st_cpu = time.time(), time.process_time()\n",
    "\n",
    "    # Status\n",
    "    if print_progress and 'SKU' in kwargs: print('SKU:', kwargs['SKU'])\n",
    "    \n",
    "    # Initialize\n",
    "    ropt, results = RollingHorizonOptimization(), pd.DataFrame()\n",
    "\n",
    "    # For each cost param setting\n",
    "    for cost_params_ in cost_params:\n",
    "\n",
    "        # Print progress\n",
    "        if print_progress: print('...cost param setting:', cost_params_)\n",
    "        \n",
    "        # Check if samples provided\n",
    "        if not samples is None:\n",
    "            \n",
    "            # Apply (Weighted) SAA  model\n",
    "            wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "            result = ropt.run(wsaamodel, samples, actuals, weights, epsilons)\n",
    "             \n",
    "            # Get T\n",
    "            T = len(samples)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Apply ex-post clairvoyant model\n",
    "            wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "            result = ropt.run_expost(wsaamodel, actuals)\n",
    "            \n",
    "            # Get T\n",
    "            T = actuals.shape[1]\n",
    "\n",
    "        # Store results\n",
    "        meta = pd.DataFrame({'CR': cost_params_['CR'], **kwargs}, index=list(range(T)))\n",
    "        results = pd.concat([results, pd.concat([meta, result], axis=1)], axis=0)\n",
    "\n",
    "    # Save result as csv file\n",
    "    if not path_to_save is None and not name_to_save is None:\n",
    "        results.to_csv(path_or_buf=(path_to_save+'/'+name_to_save+'_SKU'+str(kwargs.get('SKU', None))+\n",
    "                                    '_tau'+str(kwargs.get('tau', None))+'.csv'), sep=',', index=False)\n",
    "\n",
    "    # Timer\n",
    "    exec_time_sec, cpu_time_sec = time.time() - st_exec, time.process_time() - st_cpu\n",
    "    \n",
    "    # Status\n",
    "    if print_progress: print('>>>> Done:', str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    # Return  \n",
    "    return results if return_results else {'SKU': kwargs.get('SKU', None), 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b98b8e-b576-4519-85a0-f6efcbe8966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Context manager (Credits: 'https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution')\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137c0ad-6243-4390-bd1c-6aff2a2fca74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Training and Samping\n",
    "\n",
    "The two global models (using 'Global Training and Sampling') are **Rolling Horizon Global Weighted SAA (GwSAA)**, which is our model, and **Rolling Horizon Global Robust Weighted SAA (GwSAA-R)**, which is the analogous model with robust extension.\n",
    "\n",
    "Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'global' distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a0600-86eb-4c40-9525-5478c3c79644",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We first load global experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline.\n",
    "\n",
    "Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly. Furthermore, features and demands are scaled for training (and later rescaled for sampling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c63e6-9e8a-4230-8f84-1cb0be138a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028da40-43c6-499d-a378-e9d5de6e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, products, timePeriods, features, features_to_scale, features_to_scale_with = load_experiment_data('global', PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d8b4a-bd25-4d13-b945-76749eba4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize\n",
    "    data = {}\n",
    "    \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "    \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "\n",
    "        # Preprocess data for global models\n",
    "        data_ = pp.preprocess_global_data(X, y, products, timePeriods, timePeriodsTestStart+t-1, tau_, features, features_to_scale, \n",
    "                                          features_to_scale_with, X_scaler=MaxQFeatureScaler(q_outlier=0.975), \n",
    "                                          y_Scaler=MaxQDemandScaler(q_outlier=0.975))\n",
    "\n",
    "        # Extract data\n",
    "        X_train, X_test, y_train, y_test, products_train, products_test, timePeriods_train, timePeriods_test, X_scalers, y_scalers = data_\n",
    "   \n",
    "        # Store data\n",
    "        data[t] = {\n",
    "            \n",
    "            'timePeriods_train': timePeriods_train, 'timePeriods_test': timePeriods_test,\n",
    "            'products_train': products_train, 'products_test': products_test,\n",
    "            'X_scalers': X_scalers, 'y_scalers': y_scalers,\n",
    "            'X_train': X_train, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        # Save\n",
    "        _ = joblib.dump(data, PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')      \n",
    "\n",
    "        # TODO epsilons = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd893890-6849-48fe-8568-5887a176f1c8",
   "metadata": {},
   "source": [
    "## Weights model\n",
    "\n",
    "The weights models - and thus the data used, weight functions, and weights per sample - are the same for the two global models **GwSAA** and **GwSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor). Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bcee6-737b-4ac0-b412-837607c2edfb",
   "metadata": {},
   "source": [
    "### Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the global random forest weights model, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b187aa7-4edd-4288-88ea-fe5a6ca96a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(20, 100, 20)],  \n",
    "    'min_samples_leaf': [x for x in range(10, 100, 10)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "tuning_params = {     \n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 8,\n",
    "    'verbose': 2\n",
    "}    \n",
    "\n",
    "random_search = True\n",
    "print_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9828a1-0293-4bdb-949d-fb5440c0b446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "       \n",
    "    # Using t=1 (i.e, data available before start of testing)\n",
    "    t=1\n",
    "\n",
    "    # Load and extract preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "    timePeriods_train, timePeriods_test = data[t]['timePeriods_train'], data[t]['timePeriods_test']\n",
    "    products_train, products_test = data[t]['products_train'], data[t]['products_test']\n",
    "    X_scalers, y_scalers = data[t]['X_scalers'], data[t]['y_scalers']\n",
    "    X_train, X_test = data[t]['X_train'], data[t]['X_test']\n",
    "    y_train, y_test = data[t]['y_train'], data[t]['y_test']\n",
    "    \n",
    "    # Initialize\n",
    "    pp = PreProcessing()\n",
    "    wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "    # Scale features and demands\n",
    "    X_train = pp.scale(X_train, X_scalers, products_train)\n",
    "    y_train = pp.scale(y_train, y_scalers, products_train)   \n",
    "\n",
    "    # CV time series splits\n",
    "    cv_folds = pp.split_timeseries_cv(n_splits=3, timePeriods=timePeriods_train)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status)\n",
    "    wm.save_cv_result(path=PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_cv_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e1431-8e18-455e-926f-965abf0d2e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfeb12-97ce-411e-80f0-1d7d0a5019be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a35ae-a90a-4d9b-8f68-f3c917ab401a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135fff30-5122-4776-a507-85b29f06ecae",
   "metadata": {},
   "source": [
    "### Fit weight functions and generate weights\n",
    "\n",
    "We now fit the global random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$ and over periods $t=1,...,T$. This is done across all products at once (global training). Then, for each $\\tau=0,...,4$ and over periods $t=1,...,T$, we generate for each product (SKU) $k=1,...,M$ the weights given the test feature $x_{k,t}$. This is done *jointly* across products (by using $x_{t}=(x_{1,t},...,x_{M,t})^{\\top}$) for computational efficiency - the weights for each individual product can be extracted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac397b5-6cd1-4b43-82c6-92cc48459b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_params = {\n",
    "    #'n_jobs': 32,\n",
    "    'n_jobs': 64,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b67c0f-7c50-4b19-a292-9fcdfc8de6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "\n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "        \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "\n",
    "        \n",
    "        # Load and extract preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "        timePeriods_train, timePeriods_test = data[t]['timePeriods_train'], data[t]['timePeriods_test']\n",
    "        products_train, products_test = data[t]['products_train'], data[t]['products_test']\n",
    "        X_scalers, y_scalers = data[t]['X_scalers'], data[t]['y_scalers']\n",
    "        X_train, X_test = data[t]['X_train'], data[t]['X_test']\n",
    "        y_train, y_test = data[t]['y_train'], data[t]['y_test']\n",
    "        \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # Scale features and demands\n",
    "        X_train, X_test = pp.scale(X_train, X_scalers, products_train), pp.scale(X_test, X_scalers, products_test)\n",
    "        y_train, y_test = pp.scale(y_train, y_scalers, products_train), pp.scale(y_test, y_scalers, products_test)           \n",
    "            \n",
    "        # Load tuned weights model\n",
    "        wm.load_cv_result(path=PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_cv_tau'+str(tau_)+'.joblib')\n",
    "        \n",
    "        # Fit weight functions  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        wm.fit(X_train, y_train)\n",
    "        weightfunctions_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "        # Generate weights  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        weights[t] = wm.apply(X_train, X_test)\n",
    "        weights_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "        \n",
    "    # Status\n",
    "    print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "        \n",
    "    # Save\n",
    "    _ = joblib.dump(weights, PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = joblib.dump(weightfunctions_times, PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = joblib.dump(weights_times, PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670dd4f-30dd-47a0-ae2e-d9a2fb1bc7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9c398-7660-4471-93e8-3b14eb9c5f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb286db-6601-471c-8902-5311ccd78a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a5ac4-f30b-4345-8c0f-70a976174124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d128b8d-f93b-4075-b217-bda9c6c99575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06748fe-fb0b-46fc-9379-4d94c8c62844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48516cbd-5a31-43ad-bee6-f90086149943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae60bff-a54a-431b-88da-f6c777c4adc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4a44c-2119-46fd-a789-c63cbbfbd8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f6f1b-0440-4c6e-ad66-174d8451b017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e739223-1511-4e45-90ac-8200e53decc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed8131-7c14-4331-ab7e-239d3cf2b8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c109b-a958-4789-ac3b-328c38d3d378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d357c25-392d-4686-be11-9fd8a580f007",
   "metadata": {},
   "source": [
    "# Local Training and Sampling\n",
    "\n",
    "The two local models (using 'Local Training and Sampling') are **Rolling Horizon Local Weighted SAA (wSAA)**, and **Rolling Horizon Local Robust Weighted SAA (wSAA-R)**, which is the analogous model with robust extension.\n",
    "\n",
    "Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'local' distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$ for each product $k=1,...,M$ separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1793967-c0a7-49f9-afbe-3c2edc82a733",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We first load and pre-process the data. This includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ where n is the number of training observations (rows) in the data\n",
    "- **Y_Data** (pd.DataFrame) is the demand data $d_{j,n}$ (a times series per product)\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cde388-4e04-432e-a25f-c7c86ad4a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local_r'\n",
    "weightsmodel_name = 'rfwm_local_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c111f-7ccb-4ddb-aa74-41e9c671bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns4.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a27ac-fb86-4586-97e1-04fb33da5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_Data_Columns = X_Data_Columns.loc[X_Data_Columns.Local == 'YES']\n",
    "X_Data = X_Data[X_Data_Columns.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badda3cf-e799-4d46-9fcf-af3ec6dac26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by SKU and sale_yearweek for preprocessing\n",
    "data = pd.concat([ID_Data, X_Data, Y_Data], axis=1).sort_values(by=['SKU', 'sale_yearweek']).reset_index(drop=True)\n",
    "\n",
    "ID_Data = data[ID_Data.columns]\n",
    "X_Data = data[X_Data.columns]\n",
    "Y_Data = data[Y_Data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2044675-b66d-42d9-ab1d-24cd56e81dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-period demand vectors\n",
    "data = pd.concat([ID_Data, Y_Data], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "Y_Data = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be213da-406b-4e42-8699-2f5973a9c767",
   "metadata": {},
   "source": [
    "## Weights model\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two local models **wSAA** and **wSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor) and for each product (SKU) $k=1,...,M$ separately. Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) for each product (SKU) $k=1,...,M$ separately and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$ separatey)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0442f-cdaa-47ad-a691-4b2aac018b30",
   "metadata": {},
   "source": [
    "### Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the local random forest weights model for each product (SKU) $k=1,...,M$, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653420-33b5-4569-8397-2da1590e53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to tune random forest weights kernels\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(2, 20, 1)],  \n",
    "    'min_samples_leaf': [x for x in range(2, 10, 1)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}    \n",
    "\n",
    "random_search = True\n",
    "print_status = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381d48c-ccee-44df-9335-7595ae2c48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    cv_results = {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for SKU in SKUs:\n",
    "\n",
    "        # Initialize preprocessing module\n",
    "        pp = PreProcessing()\n",
    "\n",
    "        # Select and reshape training and test data\n",
    "        args = {'train': (ID_Data.SKU == SKU) & (ID_Data.sale_yearweek < test_start - tau)}\n",
    "\n",
    "        # id_train = pp.train_test_split2(ID_Data, **args)\n",
    "        # X_train = pp.train_test_split2(X_Data, **args, to_array=True)\n",
    "        # y_train = pp.train_test_split2(Y_Data, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)\n",
    "        \n",
    "        \n",
    "        id_train = pp.train_test_split(ID_Data, **args)\n",
    "        X_train = pp.train_test_split(X_Data, **args, to_array=True)\n",
    "        y_train = pp.train_test_split(Y_Data, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)\n",
    "\n",
    "        # Initialize\n",
    "        weightsmodel = RandomForestWeightsModel(model_params)\n",
    "\n",
    "        # CV search\n",
    "        cv_folds = pp.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "        cv_results[SKU] = weightsmodel.tune(X_train, y_train, cv_folds, hyper_params_grid, \n",
    "                                            tuning_params, random_search, print_status)\n",
    "        \n",
    "        # Status\n",
    "        print('SKU '+str(SKU)+' of '+str(len(SKUs))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "\n",
    "    # Save\n",
    "    _ = joblib.dump(cv_results, PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1c7a-4494-4191-858a-231dc1ba9fd0",
   "metadata": {},
   "source": [
    "### Fit weight functions and generate weights\n",
    "\n",
    "We now fit a local random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately (local training). Then, for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately, we generate the weights given the test feature $x_{k,t}$. This is done *separately* for each product (SKU) $k=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410060e-ec5e-4f5d-8da2-7a81da076ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806721b-465b-4edf-b312-a88245d9b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    samples, weightfunctions, weightfunctions_times, weights, weights_times = {}, {}, {}, {}, {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for SKU in SKUs:\n",
    "        \n",
    "        # Initialize\n",
    "        samples[SKU], weightfunctions[SKU], weightfunctions_times[SKU], weights[SKU], weights_times[SKU] = {}, {}, {}, {}, {}\n",
    "        \n",
    "        # For each period t=1,...,T\n",
    "        for t in ts:\n",
    "        \n",
    "            # Adjust look-ahead tau to account for end of horizon\n",
    "            tau_ = min(tau,T-t)\n",
    "\n",
    "            # Generate samples, fit weight functions, and generate weights (based on tuned weights model)\n",
    "            weightsmodel = RandomForestWeightsModel()\n",
    "            weightsmodel.load_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau_)+'.joblib', SKU=SKU)\n",
    "            res = weightsmodel.training_and_sampling(ID_Data.loc[ID_Data.SKU==SKU], X_Data.loc[ID_Data.SKU==SKU], Y_Data.loc[ID_Data.SKU==SKU], \n",
    "                                                     tau=tau_, timePeriods=ID_Data.loc[ID_Data.SKU==SKU].sale_yearweek, \n",
    "                                                     timePeriodsTestStart=test_start+t-1, model_params=model_params)\n",
    "            samples[SKU][t], weightfunctions[SKU][t], weightfunctions_times[SKU][t], weights[SKU][t], weights_times[SKU][t] = res\n",
    "\n",
    "        # Status\n",
    "        print('SKU '+str(SKU)+' of '+str(len(SKUs))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "        \n",
    "    # Save\n",
    "    _ = joblib.dump(samples, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')  \n",
    "    _ = joblib.dump(weightfunctions, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weightfunctions_times, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_times_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weights, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weights_times, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_times_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17b3f0-3b89-4b50-bf05-4ff9e354f56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c5382-2068-4a23-ab8f-dd0a8099cab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18a62e-a6ee-4574-ab58-6b508bb5aa60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc0162-892b-457d-8324-808a9ec7796c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4df65-7215-4e79-b44e-8e4f2d9925bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba764f-24b1-43ef-8fde-39ef29e55cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746bbca-95cd-4012-a2ae-643beb38d019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af6eba-d18e-4571-bf48-774160b7ee3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edca9986-993c-4b9a-96e5-22444655a258",
   "metadata": {},
   "source": [
    "# Rolling Horizon Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b5a1c-c253-41b3-be40-866598943f7e",
   "metadata": {},
   "source": [
    "## (a) Rolling Horizon Global Weighted SAA (GwSAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea232ec-5727-401f-a60c-37e82bad7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/'+GwSAA+'_FINAL',\n",
    "    'name_to_save': GwSAA+'_FINAL',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}\n",
    "\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5248f73a-8f0f-4b52-9d43-8f15a0aa0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-ahead tau=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [03:53<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-ahead tau=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [42:07<00:00,  5.50s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-ahead tau=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [45:21<00:00,  5.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-ahead tau=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [54:49<00:00,  7.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-ahead tau=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [1:12:17<00:00,  9.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    pp = PreProcessing()\n",
    "\n",
    "    # Load and extract preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load and extract weights\n",
    "    weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Samples\n",
    "    samples_ = {}\n",
    "\n",
    "    # For products k=1,...,M\n",
    "    for product in products:\n",
    "\n",
    "        # Initialize\n",
    "        samples_[product] = {}\n",
    "\n",
    "        # For periods t=1,...,T\n",
    "        for t in ts:\n",
    "\n",
    "            # Get demand data, product identifiers, and fitted scalers\n",
    "            y_train = data[t]['y_train']\n",
    "            products_train = data[t]['products_train']\n",
    "            y_scalers = data[t]['y_scalers']\n",
    "\n",
    "            # Scale demand with fitted demand scaler per product\n",
    "            y_train_z = pp.scale(y_train, y_scalers, products_train)  \n",
    "\n",
    "            # Rescale demand with fitted demand scaler of the current product\n",
    "            y_train_zz = pp.rescale(y_train_z, y_scalers[product]) \n",
    "\n",
    "            # Store demand samples\n",
    "            samples_[product][t] = copy.deepcopy(y_train_zz)\n",
    "\n",
    "    # Actuals\n",
    "    actuals_ = {}\n",
    "\n",
    "    # For products k=1,...,M\n",
    "    for product in products:\n",
    "\n",
    "        # Initialize\n",
    "        actuals_[product] = {}\n",
    "\n",
    "        # For periods t=1,...,T\n",
    "        for t in ts:\n",
    "\n",
    "            # Get demand actuals and product identifiers\n",
    "            y_test = data[t]['y_test']\n",
    "            products_test = data[t]['products_test']\n",
    "\n",
    "            # Store demand actuals\n",
    "            actuals_[product][t] = y_test[products_test==product].flatten()\n",
    "            \n",
    "    # Weights   \n",
    "    weights_ = {}\n",
    "\n",
    "    # For products k=1,...,M\n",
    "    for product in products:\n",
    "\n",
    "        # Initialize\n",
    "        weights_[product] = {}\n",
    "\n",
    "        # For periods t=1,...,T\n",
    "        for t in ts:\n",
    "\n",
    "            # Get product identifiers\n",
    "            products_test = data[t]['products_test']\n",
    "\n",
    "            # Store weights\n",
    "            weights_[product][t] = weights[t][products_test==product].flatten()\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiment)(tau=tau, SKU=product, wsaamodel=WeightedSAA(), samples=samples_[product], \n",
    "                                                                     weights=weights_[product], actuals=actuals_[product], \n",
    "                                                                     **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025441f-5534-4ee2-a07c-666e7749dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WITH EXPERIMENT AS OWN CLASS ####\n",
    "\n",
    "#### CODE NOT YET WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4236f61-52e6-4f37-8fe3-39c9d496c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize\n",
    "    experiment = Experiment()\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_experiment_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, SKU=product, wsaamodel=WeightedSAA(), weights=weights_[product], \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product], \n",
    "                                                                     **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0257f-eb49-43f3-9096-a6de3b1fcc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c313c1-cf05-4fbe-8e6d-741fd24e61f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd76b22-7b3a-426d-b3c6-897f0b0ea232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f990e6a-3366-45d5-8ccf-bdc88876c5fc",
   "metadata": {},
   "source": [
    "## (b) Rolling Horizon Global Robust Weighted SAA (GwSAA-R)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20cc3496-a393-41bc-bedd-069d126aad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/'+GwSAAR+'_FINAL',\n",
    "    'name_to_save_prefix': GwSAAR+'_FINAL',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}\n",
    "\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff7d42-139d-4b72-85bf-0fbed2a1448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty set parameter e=1...\n",
      "...look-ahead tau=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [08:35<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [46:07<00:00,  6.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  25%|██▌       | 115/460 [16:21<17:54,  3.11s/it]  /home/fesc/.conda/envs/multiPeriodInv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Progress: 100%|██████████| 460/460 [56:41<00:00,  7.39s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [1:19:22<00:00, 10.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [2:09:05<00:00, 16.84s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty set parameter e=3...\n",
      "...look-ahead tau=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [08:47<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [45:05<00:00,  5.88s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [57:17<00:00,  7.47s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [1:20:10<00:00, 10.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [2:03:46<00:00, 16.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty set parameter e=6...\n",
      "...look-ahead tau=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [08:52<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [44:22<00:00,  5.79s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [56:24<00:00,  7.36s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [1:19:46<00:00, 10.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [2:05:26<00:00, 16.36s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty set parameter e=9...\n",
      "...look-ahead tau=0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [08:56<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [44:54<00:00,  5.86s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 460/460 [57:05<00:00,  7.45s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...look-ahead tau=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|▋         | 29/460 [15:01<7:37:23, 63.67s/it] "
     ]
    }
   ],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "    \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        pp = PreProcessing()\n",
    "\n",
    "        # Load and extract preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        # Load and extract weights\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Samples\n",
    "        samples_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            samples_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand data, product identifiers, and fitted scalers\n",
    "                y_train = data[t]['y_train']\n",
    "                products_train = data[t]['products_train']\n",
    "                y_scalers = data[t]['y_scalers']\n",
    "\n",
    "                # Scale demand with fitted demand scaler per product\n",
    "                y_train_z = pp.scale(y_train, y_scalers, products_train)  \n",
    "\n",
    "                # Rescale demand with fitted demand scaler of the current product\n",
    "                y_train_zz = pp.rescale(y_train_z, y_scalers[product]) \n",
    "\n",
    "                # Store demand samples\n",
    "                samples_[product][t] = copy.deepcopy(y_train_zz)\n",
    "\n",
    "        # Actuals\n",
    "        actuals_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            actuals_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand actuals and product identifiers\n",
    "                y_test = data[t]['y_test']\n",
    "                products_test = data[t]['products_test']\n",
    "\n",
    "                # Store demand actuals\n",
    "                actuals_[product][t] = y_test[products_test==product].flatten()\n",
    "\n",
    "        # Weights   \n",
    "        weights_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            weights_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get product identifiers\n",
    "                products_test = data[t]['products_test']\n",
    "                \n",
    "                # Store weights\n",
    "                weights_[product][t] = weights[t][products_test==product].flatten()\n",
    "\n",
    "        # Epsilons  \n",
    "        epsilons_ = {}\n",
    "        \n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            epsilons_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand data, product identifiers, and fitted scalers\n",
    "                y_train = data[t]['y_train']\n",
    "                products_train = data[t]['products_train']\n",
    "    \n",
    "                # Calculate epsilon as e * in-sample standard deviation of current product's demand\n",
    "                epsilons_[product][t] = e * np.std(y_train[products_train==product].flatten())\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiment)(tau=tau, SKU=product, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         samples=samples_[product], weights=weights_[product], epsilons=epsilons_[product],\n",
    "                                                                         actuals=actuals_[product], e=e, **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7c739-3536-4f0c-be6b-a2b48663ac45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c64be-484e-458c-b062-8128a0cce7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef5c6d-d226-46b8-a0eb-8bcbcf626885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEW CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27a85bc-3db4-46fb-9020-ff7833462550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "    \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Initialize\n",
    "        experiment = Experiment()\n",
    "\n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        # Load weights\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Preprocess experiment data\n",
    "        weights_, samples_, actuals_, epsilons_ = experiment.preprocess_experiment_data(data, weights, e=e)\n",
    "\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, SKU=product, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         weights=weights_[product], samples=samples_[product], \n",
    "                                                                         actuals=actuals_[product], epsilons=epsilons_[product],\n",
    "                                                                         e=e, **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a87fe4-9f7b-45ec-9bd0-d89989d39f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fdcfd-9c32-407f-a9ae-b35d98734a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372a4b7-9b75-4eee-a56d-0fb68c7bb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154df6e7-8e2f-4e90-9f67-06ad2948df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c876ec2-021d-4faf-a276-3f5dfb61a59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aad5e6-9d9f-4a82-8d69-f7bc278dc881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc90b4-479e-4f37-8853-0b32f62cf52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562630df-2971-4cdd-8de2-c8220185aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177c4b64-a7ff-4b64-8e95-ddcd851acca5",
   "metadata": {},
   "source": [
    "## (c) Rolling Horizon Local Weighted SAA (wSAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86228ea-cc1f-447a-a3f1-ff95887b38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/'+wSAA+'_FINAL',\n",
    "    'name_to_save': wSAA+'_FINAL',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}\n",
    "\n",
    "n_jobs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e54514-7c62-4791-a34a-5f404b7dc986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "    weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "    samples, actuals, weights = prep_samples_and_weights(samples, weights, SKUs=SKUs, ts=ts)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples[SKU], weights=weights[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d2e6d-cbc8-4156-9f6d-8db11f19889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3ae23-fecf-40c4-ab82-42d6f1d04b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35ca8e-b483-4cc4-97e1-30f3266dcfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a3ff83-6de0-4fad-8b99-a50eb9f3dd11",
   "metadata": {},
   "source": [
    "## (d) Rolling Horizon Local Robust Weighted SAA (wSAA-R)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189defe-7b4b-433f-8bab-150e4c5250d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local_not_reshaped'\n",
    "weightsmodel_name = 'rfwm_local_not_reshaped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83e339-7ec6-4a30-8315-3bdfd9616e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/wSAAR',\n",
    "    'name_to_save_prefix': 'wSAAR',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b6457-abd5-4d2d-ac5d-4d74f6da08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in [1,3,6,9,12]:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "        \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        samples, actuals, weights, epsilons = prep_samples_and_weights(samples, weights, e=e, SKUs=SKUs, ts=ts)\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                     samples=samples[SKU], weights=weights[SKU], epsilons=epsilons[SKU],\n",
    "                                                                     actuals=actuals[SKU], e=e, **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f42885-78fc-45fa-93d5-d6ea85789961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602f846-a0d0-4b1f-b046-13dd0feaeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356cc89-3dc4-45e1-ab7f-7bcf8c64274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d980c8-d0f7-4f19-bd17-65100814f1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650cf8f6-4fd6-4258-9498-759b49e4cbc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (e) Baseline model: Rolling Horizon Local Weighted SAA (SAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36241d0-6ac1-4fdf-9ead-18bc7d921e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/SAA',\n",
    "    'name_to_save': 'SAA',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1c7f7-e1ae-4a9b-b474-19d1d5b10420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_local_samples_not_reshaped_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    samples, actuals = prep_samples_and_weights(samples, SKUs=SKUs, ts=ts)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1bc2c-df43-4c08-bd5a-08c91d99b970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554967d-2acd-45c3-8e68-10cd23695bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccb147-9122-4e3f-8435-8c73275d2836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280f481-c0e0-4e07-86bc-c7e2b45fa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b4a736-d792-468a-86a1-8c95976293ed",
   "metadata": {},
   "source": [
    "## (f) Ex-post optimal model with deterministic demand\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696c509-5415-49f7-a94e-b69279244785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/ExPost',\n",
    "    'name_to_save': 'ExPost',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc93a75-85e3-428a-ac69-a824756d68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_local_samples_not_reshaped_tau'+str(0)+'.joblib')\n",
    "actuals = {}\n",
    "for SKU in SKUs:\n",
    "    d = []\n",
    "    for t in ts:\n",
    "        d = d + [samples[SKU][t]['y_test'].item()]\n",
    "    actuals[SKU] = np.array(d).reshape(1,len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67975180-c482-416c-9a03-18a97c42677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each product (SKU) k=1,...,M\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(SKU=SKU, wsaamodel=WeightedSAA(), actuals=actuals[SKU], **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b089dda-c51e-4ad8-9246-acbf1729136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d10eb6-98c8-49b9-8335-eac24a9e37c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ec46e-1bd4-4b0e-8a18-0c0caf340dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49b4a4-51a2-4f0f-93ce-9c0e10d84277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08ce33-6328-49c9-a5ec-0a00cf7c0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9daf8-66fe-46ff-bbe5-daa14d847001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a8a38-1144-49c5-84e8-e4a11c6bebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiPeriodInv",
   "language": "python",
   "name": "multiperiodinv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
