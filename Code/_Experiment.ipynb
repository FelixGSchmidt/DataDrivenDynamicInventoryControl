{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3d67cd-e230-495a-a031-1e5a7ebc4dd7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b518e25-3378-4cde-9fca-4ea5cccd902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Weights Model\n",
    "from WeightsModel import PreProcessing\n",
    "from WeightsModel import MaxQFeatureScaler\n",
    "from WeightsModel import MaxQDemandScaler\n",
    "from WeightsModel import RandomForestWeightsModel\n",
    "\n",
    "# Import Weighted SAA Model\n",
    "from WeightedSAA import WeightedSAA\n",
    "from WeightedSAA import RobustWeightedSAA\n",
    "from WeightedSAA import RollingHorizonOptimization\n",
    "\n",
    "# Import Experiment\n",
    "from Experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8cb12-853b-457f-91ff-8e41687c3862",
   "metadata": {},
   "source": [
    "# Setup the experiment\n",
    "\n",
    "**Experiment data** consists of four data sets.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ of product $j$ at time $n$\n",
    "- **Y_Data** (pd.DataFrame) is the demand time series data, i.e., each row is a demand observations $d_{j,n}$ of product $j$ at time $n$\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets\n",
    "\n",
    "Data is loaded before each experiment using **load_data()**.\n",
    "\n",
    "We run an experiment for all given products (SKUs) $k=1,...,M$ over a test planning horizon $t=1,...,T$ with $T=13$ for three different cost parameter settings $\\{K, u, h, b\\}$ that vary the critical ratio ($CR=\\frac{b}{b+h}$) of holding and backlogging yielding\n",
    "- $CR=0.50$: $\\{K=100, u=0.5, h=1, b=1\\}$\n",
    "- $CR=0.75$: $\\{K=100, u=0.5, h=1, b=3\\}$\n",
    "- $CR=0.90$: $\\{K=100, u=0.5, h=1, b=9\\}$\n",
    "\n",
    "Experiments are run for different choices of the look-ahead $\\tau=0,...,4$. For robust models, we vary the parameter $e=\\{1,3,6,9,12\\}$ (multiplier of the in-sample standard deviation of demand) defining product and sample sepcific uncertainty sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b0bd36-7aed-491c-a705-cd2c4812a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "experiment_setup = dict(\n",
    "\n",
    "    # Set paths\n",
    "    path_data = '/home/fesc/DDDInventoryControl/Data',\n",
    "    path_weightsmodel = '/home/fesc/DDDInventoryControl/Data/WeightsModel',\n",
    "    path_results = '/home/fesc/DDDInventoryControl/Data/Results',\n",
    "    \n",
    "    # Weights models\n",
    "    global_weightsmodel = 'rfwm_global', \n",
    "    local_weightsmodel = 'rfwm_local', \n",
    "\n",
    "    # Optimization models\n",
    "    GwSAA = 'GwSAA',\n",
    "    GwSAAR = 'GwSAAR',\n",
    "    wSAA = 'wSAA',\n",
    "    wSAAR = 'wSAAR',\n",
    "    SAA = 'SAA',\n",
    "    ExPost = 'ExPost',\n",
    "    \n",
    "    # Set identifier of start period of testing horizon\n",
    "    timePeriodsTestStart = 114,\n",
    "\n",
    "    # Set product identifiers\n",
    "    products = range(1,460+1),   # Products (SKUs) k=1,...,M\n",
    "    \n",
    "    # Set problem params\n",
    "    T = 13,             # Planning horizon T\n",
    "    ts = range(1,13+1), # Periods t=1,...,T of the planning horizon\n",
    "    taus = [0,1,2,3,4], # Look-aheads tau=0,...,4\n",
    "    es = [1,3,6,9,12],  # Uncertainty set specifications e=1,...,12\n",
    "       \n",
    "    # Set cost params\n",
    "    cost_params = [\n",
    "        {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "    ],\n",
    "    \n",
    "    # Set gurobi params\n",
    "    gurobi_params = {\n",
    "    \n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 1, \n",
    "        'NonConvex': 2, \n",
    "        'PSDTol': 1e-3, # 0.1%\n",
    "        'MIPGap': 1e-3, # 0.1%\n",
    "        'NumericFocus': 0, \n",
    "        'obj_improvement': 1e-3, # 0.1%\n",
    "        'obj_timeout_sec': 3*60, # 3 min\n",
    "        'obj_timeout_max_sec': 10*60, # 10 min\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Global weights model params\n",
    "    global_weightsmodel_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'oob_score': True,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'n_estimators': [500, 1000],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [x for x in range(20, 100, 20)],  \n",
    "            'min_samples_leaf': [x for x in range(10, 100, 10)],  \n",
    "            'max_features': [x for x in range(8, 136, 8)],   \n",
    "            'max_leaf_nodes': [None],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "        },    \n",
    "\n",
    "        # Tuning params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 100,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "        # Tuning using random search\n",
    "        random_search = True\n",
    "    ),\n",
    "    \n",
    "    # Local weights model params\n",
    "    local_weightsmodel_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'oob_score': True,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 1,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'n_estimators': [25, 50],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [x for x in range(2, 20, 2)],  \n",
    "            'min_samples_leaf': [x for x in range(2, 10, 2)],  \n",
    "            'max_features': [x for x in range(4, 96, 4)],   \n",
    "            'max_leaf_nodes': [None],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "        },    \n",
    "\n",
    "        # Tuning params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 100,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 32,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "        # Tuning using random search\n",
    "        random_search = True\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)\n",
    "\n",
    "# Initialize Experiment\n",
    "experiment = Experiment(**experiment_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137c0ad-6243-4390-bd1c-6aff2a2fca74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Training and Samping\n",
    "\n",
    "The two global models (using 'Global Training and Sampling') are **Rolling Horizon Global Weighted SAA (GwSAA)**, which is our model, and **Rolling Horizon Global Robust Weighted SAA (GwSAA-R)**, which is the analogous model with robust extension. Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'global' distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$.\n",
    "\n",
    "We first load **global** experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline. Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly. Furthermore, features and demands are scaled for training (and later rescaled for sampling). \n",
    "\n",
    "The weights models - and thus the data used, weight functions, and weights per sample - are the same for the two global models **GwSAA** and **GwSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor). Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028da40-43c6-499d-a378-e9d5de6e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, ids_products, ids_timePeriods, features, features_to_scale, features_to_scale_with = experiment.load_data('global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed38d5-839e-4f71-b875-b524656c3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module of the weights model\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7822ac-eb99-4ca4-b702-593f98563c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:  \n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "    # Preprocess data for global models\n",
    "    data = pp.preprocess_weightsmodel_data(X, y, ids_products, ids_timePeriods, timePeriodsTestStart, tau, T, \n",
    "                                           features, features_to_scale, features_to_scale_with, \n",
    "                                           X_scaler=MaxQFeatureScaler(q_outlier=0.975), \n",
    "                                           y_scaler=MaxQDemandScaler(q_outlier=0.975))\n",
    "\n",
    "    # Save\n",
    "    _ = dump(data, path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bcee6-737b-4ac0-b412-837607c2edfb",
   "metadata": {},
   "source": [
    "## Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the global random forest weights model, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b34c5a-0164-4e8e-a20e-78e2d07bb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meta params for tuning global weights model available\n",
    "locals().update(experiment_setup['global_weightsmodel_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9828a1-0293-4bdb-949d-fb5440c0b446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "       \n",
    "    # Using t=1 (i.e, data available before start of testing)\n",
    "    t=1\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Extract preprocessed data\n",
    "    locals().update(data[t])\n",
    "    \n",
    "    # Initialize\n",
    "    pp = PreProcessing()\n",
    "    wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "    # Scale features and demands\n",
    "    X_train = pp.scale(X_train, X_scalers, ids_products_train)\n",
    "    y_train = pp.scale(y_train, y_scalers, ids_products_train)   \n",
    "\n",
    "    # CV time series splits\n",
    "    cv_folds = pp.split_timeseries_cv(n_splits=3, ids_timePeriods=ids_timePeriods_train)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status=True)\n",
    "    wm.save_cv_result(path=path_weightsmodel+'/'+global_weightsmodel+'_cv_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fff30-5122-4776-a507-85b29f06ecae",
   "metadata": {},
   "source": [
    "## Fit weight functions and generate weights\n",
    "\n",
    "We now fit the global random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$ and over periods $t=1,...,T$. This is done across all products at once (global training). Then, for each $\\tau=0,...,4$ and over periods $t=1,...,T$, we generate for each product (SKU) $k=1,...,M$ the weights given the test feature $x_{k,t}$. This is done *jointly* across products (by using $x_{t}=(x_{1,t},...,x_{M,t})^{\\top}$) for computational efficiency - the weights for each individual product can be extracted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac397b5-6cd1-4b43-82c6-92cc48459b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b67c0f-7c50-4b19-a292-9fcdfc8de6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "\n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "        \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "        \n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[t])\n",
    "            \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # Scale features and demands\n",
    "        X_train, X_test = pp.scale(X_train, X_scalers, ids_products_train), pp.scale(X_test, X_scalers, ids_products_test)\n",
    "        y_train, y_test = pp.scale(y_train, y_scalers, ids_products_train), pp.scale(y_test, y_scalers, ids_products_test)           \n",
    "            \n",
    "        # Load tuned weights model\n",
    "        wm.load_cv_result(path=path_weightsmodel+'/'+global_weightsmodel+'_cv_tau'+str(tau_)+'.joblib')\n",
    "        \n",
    "        # Fit weight functions  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        wm.fit(X_train, y_train)\n",
    "        weightfunctions_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "      \n",
    "        # Generate weights  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        weights[t] = wm.apply(X_train, X_test)\n",
    "        weights_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "        \n",
    "    # Status\n",
    "    print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+global_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+global_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d357c25-392d-4686-be11-9fd8a580f007",
   "metadata": {},
   "source": [
    "# Local Training and Sampling\n",
    "\n",
    "The two local models (using 'Local Training and Sampling') are **Rolling Horizon Local Weighted SAA (wSAA)**, and **Rolling Horizon Local Robust Weighted SAA (wSAA-R)**, which is the analogous model with robust extension. Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'local' distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$ for each product $k=1,...,M$ separately.\n",
    "\n",
    "We first load **local** experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline. Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly.\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two local models **wSAA** and **wSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor) and for each product (SKU) $k=1,...,M$ separately. Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) for each product (SKU) $k=1,...,M$ separately and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$ separatey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a7aac-8290-4df0-b5ed-162e92009e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, ids_products, ids_timePeriods, features = experiment.load_data('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e7a4-9bee-40f2-ad65-084f2b229d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module of the weights model\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc86c4d-52ea-4c13-ad2a-3a21be3f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:  \n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "    # Preprocess data for local models\n",
    "    data = pp.preprocess_weightsmodel_data(X, y, ids_products, ids_timePeriods, timePeriodsTestStart, tau, T, products=products)\n",
    "\n",
    "    # Save\n",
    "    _ = dump(data, path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0442f-cdaa-47ad-a691-4b2aac018b30",
   "metadata": {},
   "source": [
    "## Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the local random forest weights model for each product (SKU) $k=1,...,M$, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe32a6c-8a55-4342-9017-32c908399737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meta params for tuning global weights model available\n",
    "locals().update(experiment_setup['local_weightsmodel_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710edda0-23b8-4e8e-8c19-7163d9456b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "    # Initialize\n",
    "    cv_results = {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for product in products:\n",
    "\n",
    "        # Using t=1 (i.e, data available before start of testing)\n",
    "        t=1\n",
    "\n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[product][t])\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # CV time series splits\n",
    "        cv_folds = pp.split_timeseries_cv(n_splits=3, ids_timePeriods=ids_timePeriods_train)\n",
    "        \n",
    "        # CV search\n",
    "        cv_results[product] = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status=False)\n",
    "\n",
    "        # Status\n",
    "        print('Product '+str(product)+' of '+str(len(products))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "\n",
    "    # Save\n",
    "    _ = dump(cv_results, path_weightsmodel+'/'+local_weightsmodel+'_cv_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1c7a-4494-4191-858a-231dc1ba9fd0",
   "metadata": {},
   "source": [
    "## Fit weight functions and generate weights\n",
    "\n",
    "We now fit a local random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately (local training). Then, for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately, we generate the weights given the test feature $x_{k,t}$. This is done *separately* for each product (SKU) $k=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410060e-ec5e-4f5d-8da2-7a81da076ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c21564-d205-4df7-a645-3cb408b54692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "    \n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for product in products:\n",
    "        \n",
    "        # Initialize\n",
    "        weights[product], weightfunctions_times[product], weights_times[product] = {}, {}, {}\n",
    "    \n",
    "        # For each period t=1,...,T\n",
    "        for t in ts:\n",
    "\n",
    "            # Extract preprocessed data\n",
    "            locals().update(data[product][t])\n",
    "                    \n",
    "            # Adjust look-ahead tau to account for end of horizon\n",
    "            tau_ = min(tau,T-t)\n",
    "            \n",
    "            # Initialize\n",
    "            pp = PreProcessing()\n",
    "            wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "            # Load tuned weights model\n",
    "            wm.load_cv_result(path=path_weightsmodel+'/'+local_weightsmodel+'_cv_tau'+str(tau_)+'.joblib', product=product)\n",
    "\n",
    "            \n",
    "            # Fit weight functions  \n",
    "            st_exec, st_cpu = time.time(), time.process_time() \n",
    "            wm.fit(X_train, y_train)\n",
    "            weightfunctions_times[product][t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "            # Generate weights  \n",
    "            st_exec, st_cpu = time.time(), time.process_time() \n",
    "            weights[product][t] = wm.apply(X_train, X_test)\n",
    "            weights_times[product][t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "        # Status\n",
    "        print('Product '+str(product)+' of '+str(len(products))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True) \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+local_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+local_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9986-993c-4b9a-96e5-22444655a258",
   "metadata": {},
   "source": [
    "# Rolling Horizon Optimization\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b5a1c-c253-41b3-be40-866598943f7e",
   "metadata": {},
   "source": [
    "## (a) Rolling Horizon Global Weighted SAA (GwSAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3736f80d-524b-4165-8993-b59070a1a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea232ec-5727-401f-a60c-37e82bad7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f251620-54eb-412b-9cb3-2e7772622e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+GwSAA): os.mkdir(path_results+'/'+GwSAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(global_weightsmodel, GwSAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), weights=weights_[product], \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product]) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64fd32-0127-42f5-8648-6bfc067e34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(wsaamodel=WeightedSAA(), actuals, samples=None, weights=None, epsilons=None, print_progress=False, return_results=False, **kwargs):\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf9e47-03ba-4a88-8615-37b2b6f5b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e1120-023d-44bb-a954-2e0313ad1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.experiment_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff233ff-a659-4ac4-833b-2ca0385d308b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae680a-902b-4225-b453-f78da8a69172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set path\n",
    "# if not os.path.exists(path_results+'/'+GwSAA): os.mkdir(path_results+'/'+GwSAA)\n",
    "\n",
    "# tau=2\n",
    "    \n",
    "# # Print:\n",
    "# print('Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "# # Initialize Experiment\n",
    "# experiment = Experiment(global_weightsmodel, GwSAA, **experiment_setup)\n",
    "\n",
    "# # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "# data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "# # Load weights\n",
    "# weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "# # Preprocess experiment data\n",
    "# weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34f309-1dce-481a-8619-c0269a734873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719a01b-103b-4cf1-935e-9153f7e71e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For each product (SKU) k=1,...,M\n",
    "# with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "#     resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), weights=weights_[product], \n",
    "#                                                                  samples=samples_[product], actuals=actuals_[product]) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15cdba-1e07-45dc-b0de-72f27fecc234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82613b6b-cb2a-4531-94c3-f2d2bd62afa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6739456-802b-407d-8376-12c9f472612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248f73a-8f0f-4b52-9d43-8f15a0aa0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set path\n",
    "# if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# # For each look-ahead tau=0,...,4\n",
    "# for tau in taus:\n",
    "    \n",
    "#     # Print:\n",
    "#     print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "#     # Prepare data\n",
    "#     pp = PreProcessing()\n",
    "\n",
    "#     # Load and extract preprocessed data (alternatively, data can be preprocessed here)\n",
    "#     data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "#     # Load and extract weights\n",
    "#     weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "#     # Samples\n",
    "#     samples_ = {}\n",
    "\n",
    "#     # For products k=1,...,M\n",
    "#     for product in products:\n",
    "\n",
    "#         # Initialize\n",
    "#         samples_[product] = {}\n",
    "\n",
    "#         # For periods t=1,...,T\n",
    "#         for t in ts:\n",
    "\n",
    "#             # Get demand data, product identifiers, and fitted scalers\n",
    "#             y_train = data[t]['y_train']\n",
    "#             products_train = data[t]['products_train']\n",
    "#             y_scalers = data[t]['y_scalers']\n",
    "\n",
    "#             # Scale demand with fitted demand scaler per product\n",
    "#             y_train_z = pp.scale(y_train, y_scalers, products_train)  \n",
    "\n",
    "#             # Rescale demand with fitted demand scaler of the current product\n",
    "#             y_train_zz = pp.rescale(y_train_z, y_scalers[product]) \n",
    "\n",
    "#             # Store demand samples\n",
    "#             samples_[product][t] = copy.deepcopy(y_train_zz)\n",
    "\n",
    "#     # Actuals\n",
    "#     actuals_ = {}\n",
    "\n",
    "#     # For products k=1,...,M\n",
    "#     for product in products:\n",
    "\n",
    "#         # Initialize\n",
    "#         actuals_[product] = {}\n",
    "\n",
    "#         # For periods t=1,...,T\n",
    "#         for t in ts:\n",
    "\n",
    "#             # Get demand actuals and product identifiers\n",
    "#             y_test = data[t]['y_test']\n",
    "#             products_test = data[t]['products_test']\n",
    "\n",
    "#             # Store demand actuals\n",
    "#             actuals_[product][t] = y_test[products_test==product].flatten()\n",
    "            \n",
    "#     # Weights   \n",
    "#     weights_ = {}\n",
    "\n",
    "#     # For products k=1,...,M\n",
    "#     for product in products:\n",
    "\n",
    "#         # Initialize\n",
    "#         weights_[product] = {}\n",
    "\n",
    "#         # For periods t=1,...,T\n",
    "#         for t in ts:\n",
    "\n",
    "#             # Get product identifiers\n",
    "#             products_test = data[t]['products_test']\n",
    "\n",
    "#             # Store weights\n",
    "#             weights_[product][t] = weights[t][products_test==product].flatten()\n",
    "    \n",
    "#     # For each product (SKU) k=1,...,M\n",
    "#     with tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "#         resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiment)(tau=tau, SKU=product, wsaamodel=WeightedSAA(), samples=samples_[product], \n",
    "#                                                                      weights=weights_[product], actuals=actuals_[product], \n",
    "#                                                                      **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025441f-5534-4ee2-a07c-666e7749dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WITH EXPERIMENT AS OWN CLASS ####\n",
    "\n",
    "#### CODE NOT YET WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4236f61-52e6-4f37-8fe3-39c9d496c806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0257f-eb49-43f3-9096-a6de3b1fcc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c313c1-cf05-4fbe-8e6d-741fd24e61f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd76b22-7b3a-426d-b3c6-897f0b0ea232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f990e6a-3366-45d5-8ccf-bdc88876c5fc",
   "metadata": {},
   "source": [
    "## (b) Rolling Horizon Global Robust Weighted SAA (GwSAA-R)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc3496-a393-41bc-bedd-069d126aad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/'+GwSAAR+'_FINAL',\n",
    "    'name_to_save_prefix': GwSAAR+'_FINAL',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}\n",
    "\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff7d42-139d-4b72-85bf-0fbed2a1448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "    \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        pp = PreProcessing()\n",
    "\n",
    "        # Load and extract preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        # Load and extract weights\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Samples\n",
    "        samples_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            samples_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand data, product identifiers, and fitted scalers\n",
    "                y_train = data[t]['y_train']\n",
    "                products_train = data[t]['products_train']\n",
    "                y_scalers = data[t]['y_scalers']\n",
    "\n",
    "                # Scale demand with fitted demand scaler per product\n",
    "                y_train_z = pp.scale(y_train, y_scalers, products_train)  \n",
    "\n",
    "                # Rescale demand with fitted demand scaler of the current product\n",
    "                y_train_zz = pp.rescale(y_train_z, y_scalers[product]) \n",
    "\n",
    "                # Store demand samples\n",
    "                samples_[product][t] = copy.deepcopy(y_train_zz)\n",
    "\n",
    "        # Actuals\n",
    "        actuals_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            actuals_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand actuals and product identifiers\n",
    "                y_test = data[t]['y_test']\n",
    "                products_test = data[t]['products_test']\n",
    "\n",
    "                # Store demand actuals\n",
    "                actuals_[product][t] = y_test[products_test==product].flatten()\n",
    "\n",
    "        # Weights   \n",
    "        weights_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            weights_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get product identifiers\n",
    "                products_test = data[t]['products_test']\n",
    "                \n",
    "                # Store weights\n",
    "                weights_[product][t] = weights[t][products_test==product].flatten()\n",
    "\n",
    "        # Epsilons  \n",
    "        epsilons_ = {}\n",
    "        \n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            epsilons_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand data, product identifiers, and fitted scalers\n",
    "                y_train = data[t]['y_train']\n",
    "                products_train = data[t]['products_train']\n",
    "    \n",
    "                # Calculate epsilon as e * in-sample standard deviation of current product's demand\n",
    "                epsilons_[product][t] = e * np.std(y_train[products_train==product].flatten())\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiment)(tau=tau, SKU=product, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         samples=samples_[product], weights=weights_[product], epsilons=epsilons_[product],\n",
    "                                                                         actuals=actuals_[product], e=e, **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e541a6-3468-4d93-b227-bca917c1ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in [12]:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "    \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in [2,3,4]:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        pp = PreProcessing()\n",
    "\n",
    "        # Load and extract preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        # Load and extract weights\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Samples\n",
    "        samples_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            samples_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand data, product identifiers, and fitted scalers\n",
    "                y_train = data[t]['y_train']\n",
    "                products_train = data[t]['products_train']\n",
    "                y_scalers = data[t]['y_scalers']\n",
    "\n",
    "                # Scale demand with fitted demand scaler per product\n",
    "                y_train_z = pp.scale(y_train, y_scalers, products_train)  \n",
    "\n",
    "                # Rescale demand with fitted demand scaler of the current product\n",
    "                y_train_zz = pp.rescale(y_train_z, y_scalers[product]) \n",
    "\n",
    "                # Store demand samples\n",
    "                samples_[product][t] = copy.deepcopy(y_train_zz)\n",
    "\n",
    "        # Actuals\n",
    "        actuals_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            actuals_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand actuals and product identifiers\n",
    "                y_test = data[t]['y_test']\n",
    "                products_test = data[t]['products_test']\n",
    "\n",
    "                # Store demand actuals\n",
    "                actuals_[product][t] = y_test[products_test==product].flatten()\n",
    "\n",
    "        # Weights   \n",
    "        weights_ = {}\n",
    "\n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            weights_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get product identifiers\n",
    "                products_test = data[t]['products_test']\n",
    "                \n",
    "                # Store weights\n",
    "                weights_[product][t] = weights[t][products_test==product].flatten()\n",
    "\n",
    "        # Epsilons  \n",
    "        epsilons_ = {}\n",
    "        \n",
    "        # For products k=1,...,M\n",
    "        for product in products:\n",
    "\n",
    "            # Initialize\n",
    "            epsilons_[product] = {}\n",
    "\n",
    "            # For periods t=1,...,T\n",
    "            for t in ts:\n",
    "\n",
    "                # Get demand data, product identifiers, and fitted scalers\n",
    "                y_train = data[t]['y_train']\n",
    "                products_train = data[t]['products_train']\n",
    "    \n",
    "                # Calculate epsilon as e * in-sample standard deviation of current product's demand\n",
    "                epsilons_[product][t] = e * np.std(y_train[products_train==product].flatten())\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiment)(tau=tau, SKU=product, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         samples=samples_[product], weights=weights_[product], epsilons=epsilons_[product],\n",
    "                                                                         actuals=actuals_[product], e=e, **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7c739-3536-4f0c-be6b-a2b48663ac45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c64be-484e-458c-b062-8128a0cce7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef5c6d-d226-46b8-a0eb-8bcbcf626885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEW CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27a85bc-3db4-46fb-9020-ff7833462550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "    \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Initialize\n",
    "        experiment = Experiment()\n",
    "\n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        # Load weights\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Preprocess experiment data\n",
    "        weights_, samples_, actuals_, epsilons_ = experiment.preprocess_experiment_data(data, weights, e=e)\n",
    "\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, SKU=product, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         weights=weights_[product], samples=samples_[product], \n",
    "                                                                         actuals=actuals_[product], epsilons=epsilons_[product],\n",
    "                                                                         e=e, **experiment_params) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a87fe4-9f7b-45ec-9bd0-d89989d39f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fdcfd-9c32-407f-a9ae-b35d98734a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372a4b7-9b75-4eee-a56d-0fb68c7bb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154df6e7-8e2f-4e90-9f67-06ad2948df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c876ec2-021d-4faf-a276-3f5dfb61a59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aad5e6-9d9f-4a82-8d69-f7bc278dc881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc90b4-479e-4f37-8853-0b32f62cf52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562630df-2971-4cdd-8de2-c8220185aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177c4b64-a7ff-4b64-8e95-ddcd851acca5",
   "metadata": {},
   "source": [
    "## (c) Rolling Horizon Local Weighted SAA (wSAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86228ea-cc1f-447a-a3f1-ff95887b38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/'+wSAA+'_FINAL',\n",
    "    'name_to_save': wSAA+'_FINAL',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}\n",
    "\n",
    "n_jobs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e54514-7c62-4791-a34a-5f404b7dc986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "    weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "    samples, actuals, weights = prep_samples_and_weights(samples, weights, SKUs=SKUs, ts=ts)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples[SKU], weights=weights[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d2e6d-cbc8-4156-9f6d-8db11f19889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3ae23-fecf-40c4-ab82-42d6f1d04b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35ca8e-b483-4cc4-97e1-30f3266dcfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a3ff83-6de0-4fad-8b99-a50eb9f3dd11",
   "metadata": {},
   "source": [
    "## (d) Rolling Horizon Local Robust Weighted SAA (wSAA-R)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189defe-7b4b-433f-8bab-150e4c5250d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local_not_reshaped'\n",
    "weightsmodel_name = 'rfwm_local_not_reshaped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83e339-7ec6-4a30-8315-3bdfd9616e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/wSAAR',\n",
    "    'name_to_save_prefix': 'wSAAR',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b6457-abd5-4d2d-ac5d-4d74f6da08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in [1,3,6,9,12]:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "        \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        samples, actuals, weights, epsilons = prep_samples_and_weights(samples, weights, e=e, SKUs=SKUs, ts=ts)\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                     samples=samples[SKU], weights=weights[SKU], epsilons=epsilons[SKU],\n",
    "                                                                     actuals=actuals[SKU], e=e, **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f42885-78fc-45fa-93d5-d6ea85789961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602f846-a0d0-4b1f-b046-13dd0feaeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356cc89-3dc4-45e1-ab7f-7bcf8c64274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d980c8-d0f7-4f19-bd17-65100814f1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650cf8f6-4fd6-4258-9498-759b49e4cbc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (e) Baseline model: Rolling Horizon Local Weighted SAA (SAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36241d0-6ac1-4fdf-9ead-18bc7d921e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/SAA',\n",
    "    'name_to_save': 'SAA',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1c7f7-e1ae-4a9b-b474-19d1d5b10420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_local_samples_not_reshaped_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    samples, actuals = prep_samples_and_weights(samples, SKUs=SKUs, ts=ts)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1bc2c-df43-4c08-bd5a-08c91d99b970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554967d-2acd-45c3-8e68-10cd23695bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccb147-9122-4e3f-8435-8c73275d2836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280f481-c0e0-4e07-86bc-c7e2b45fa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b4a736-d792-468a-86a1-8c95976293ed",
   "metadata": {},
   "source": [
    "## (f) Ex-post optimal model with deterministic demand\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696c509-5415-49f7-a94e-b69279244785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/ExPost',\n",
    "    'name_to_save': 'ExPost',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc93a75-85e3-428a-ac69-a824756d68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_local_samples_not_reshaped_tau'+str(0)+'.joblib')\n",
    "actuals = {}\n",
    "for SKU in SKUs:\n",
    "    d = []\n",
    "    for t in ts:\n",
    "        d = d + [samples[SKU][t]['y_test'].item()]\n",
    "    actuals[SKU] = np.array(d).reshape(1,len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67975180-c482-416c-9a03-18a97c42677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each product (SKU) k=1,...,M\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(SKU=SKU, wsaamodel=WeightedSAA(), actuals=actuals[SKU], **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b089dda-c51e-4ad8-9246-acbf1729136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d10eb6-98c8-49b9-8335-eac24a9e37c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ec46e-1bd4-4b0e-8a18-0c0caf340dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49b4a4-51a2-4f0f-93ce-9c0e10d84277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08ce33-6328-49c9-a5ec-0a00cf7c0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9daf8-66fe-46ff-bbe5-daa14d847001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a8a38-1144-49c5-84e8-e4a11c6bebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDDInventoryControl",
   "language": "python",
   "name": "dddinventorycontrol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
