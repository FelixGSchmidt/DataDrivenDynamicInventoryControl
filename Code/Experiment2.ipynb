{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa32c966-eb6a-49ab-8484-fdd0b7e22e0a",
   "metadata": {},
   "source": [
    "# Imports, paths, and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b518e25-3378-4cde-9fca-4ea5cccd902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "import itertools\n",
    "import contextlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import Weights Model\n",
    "import WeightsModel2\n",
    "from WeightsModel2 import RandomForestWeightsModel\n",
    "from WeightsModel2 import PreProcessing\n",
    "\n",
    "# Import (Rolling Horizon) Weighted SAA models\n",
    "from WeightedSAA2 import WeightedSAA\n",
    "from WeightedSAA2 import RobustWeightedSAA\n",
    "from WeightedSAA2 import RollingHorizonOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e5ab6-1320-4122-be2b-84b7bb7f89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder names as global variables\n",
    "os.chdir('/home/fesc/MM/')\n",
    "global PATH_DATA, PATH_PARAMS, PATH_KERNELS, PATH_SAMPLES, PATH_RESULTS\n",
    "\n",
    "PATH_DATA = '/home/fesc/MM/Data'\n",
    "PATH_PARAMS  = '/home/fesc/MM/Data/Params'\n",
    "PATH_WEIGHTSMODEL = '/home/fesc/MM/Data/WeightsModel'\n",
    "PATH_SAMPLES = '/home/fesc/MM/Data/Samples'\n",
    "PATH_RESULTS = '/home/fesc/MM/Data/Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e1dae-58c3-463d-bc4a-8cfbb4e60388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period and SKU ranges\n",
    "T = 13                  # Planning horizon T\n",
    "ts = range(1,13+1)      # Periods t=1,...,T of the planning horizon\n",
    "taus = range(0,4+1)     # Look-aheads tau=0,...,4 to use\n",
    "SKUs = range(1,460+1)   # Products (SKUs) k=1,...,M\n",
    "\n",
    "# Train/test split (first timePeriods of testing horizon)\n",
    "test_start = 114"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137c0ad-6243-4390-bd1c-6aff2a2fca74",
   "metadata": {},
   "source": [
    "# Global Models\n",
    "\n",
    "The two global models (using 'Global Training and Sampling') are **Rolling Horizon Global Weighted SAA (GwSAA)**, which is our model, and **Rolling Horizon Global Robust Weighted SAA (GwSAA-R)**, which is the analogous model with robust extension.\n",
    "\n",
    "Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'global' distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{j,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6c9b9-f722-44f1-8f73-dd8c964a2bbf",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We first load and pre-process the data. This includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences. In particular for the global model, we furthermore apply scaling to relevant variables using sklearn's 'MinMaxScaler' fitted on the training horizon.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ where n is the number of training observations (rows) in the data\n",
    "- **Y_Data** (pd.DataFrame) is the demand data $d_{j,n}$ (a times series per product)\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7ccc8-3afa-44aa-a296-c538eef73b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9a08d-e0c8-4d6d-b26f-44eef6d7633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22262b-07c2-4fc0-b32f-518b0163ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_Data_Columns = X_Data_Columns.loc[X_Data_Columns.Global == 'YES']\n",
    "X_Data = X_Data[X_Data_Columns.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e9b65-8a57-4d7f-a994-a5249ae5563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by SKU and sale_yearweek for preprocessing\n",
    "data = pd.concat([ID_Data, X_Data, Y_Data], axis=1).sort_values(by=['SKU', 'sale_yearweek']).reset_index(drop=True)\n",
    "\n",
    "ID_Data = data[ID_Data.columns]\n",
    "X_Data = data[X_Data.columns]\n",
    "Y_Data = data[Y_Data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73609a-fc6e-4b7a-99b5-1cb2567700f4",
   "metadata": {},
   "source": [
    "### Scaling - Features\n",
    "\n",
    "Selected relevant features (as defined by the meta data table 'X_Data_Columns') data are scaled (by product) for training using sklean's min-max scaling method (scaler is fitted once by product on the initial training horizon before model application over periods $t=1,...,T$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49841f7-a0e7-45e0-96c6-dbbb994fbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "X_Data_train = X_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "\n",
    "# Prepare\n",
    "vars_to_scale_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'Feature'].values\n",
    "vars_to_scale_with_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'ScaleWith'].values\n",
    "\n",
    "vars_to_scale = np.array(X_Data[vars_to_scale_names])\n",
    "vars_to_scale_with = np.array(X_Data_train[vars_to_scale_with_names])\n",
    "\n",
    "vars_to_scale_groups = np.array(ID_Data.SKU)\n",
    "vars_to_scale_with_groups = np.array(ID_Data_train.SKU)\n",
    "\n",
    "# Fit and transform\n",
    "scaler = MinMaxScaler()\n",
    "vars_scaled, scaler_fitted = pp.scale_variables(vars_to_scale, vars_to_scale_with, vars_to_scale_groups, vars_to_scale_with_groups, scaler)\n",
    "\n",
    "# Reshape to original data\n",
    "vars_scaled = pd.concat([pd.DataFrame(vars_scaled[i], columns=vars_to_scale_names) for i in vars_scaled]).reset_index(drop=True)\n",
    "X_Data_z = copy.deepcopy(X_Data)\n",
    "for col in vars_scaled.columns:\n",
    "    X_Data_z[col] = vars_scaled[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafd292-2c37-46de-a6a6-48b85a890457",
   "metadata": {},
   "source": [
    "### Scaling - Demands\n",
    "\n",
    "Demands for training are scaled (by produc) using sklean's min-max scaling method (scaler is fitted once by product on the initial training horizon before model application over periods $t=1,...,T$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cc37e-2a25-43ca-bf80-dfe4951435f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "Y_Data_train = Y_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "\n",
    "# Prepare\n",
    "vars_to_scale = np.array(Y_Data)\n",
    "vars_to_scale_with = np.array(Y_Data_train)\n",
    "\n",
    "vars_to_scale_groups = np.array(ID_Data.SKU)\n",
    "vars_to_scale_with_groups = np.array(ID_Data_train.SKU)\n",
    "\n",
    "# Fit and transform\n",
    "scaler = MinMaxScaler()\n",
    "vars_scaled, scaler_fitted = pp.scale_variables(vars_to_scale, vars_to_scale_with, vars_to_scale_groups, vars_to_scale_with_groups, scaler)\n",
    "\n",
    "# Reshape to original data\n",
    "Y_Data_z = pd.concat([pd.DataFrame(vars_scaled[i], columns=['Y']) for i in vars_scaled]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e5c6e-3a9c-4bcf-a248-801e0a603d21",
   "metadata": {},
   "source": [
    "### Reshape to multi-period demand\n",
    "\n",
    "We now reshape the time series of demands per product to consecutive $(\\tau+1)$-periods demand vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4f43b-c5b0-43f8-b3b5-a48626c61ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-period demand vectors\n",
    "data = pd.concat([ID_Data, Y_Data_z], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data_z = pd.DataFrame(Y)\n",
    "\n",
    "data = pd.concat([ID_Data, Y_Data], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd893890-6849-48fe-8568-5887a176f1c8",
   "metadata": {},
   "source": [
    "## Weights model\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two global models **GwSAA** and **GwSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor). Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bcee6-737b-4ac0-b412-837607c2edfb",
   "metadata": {},
   "source": [
    "### Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the global random forest weights model, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91f4f8-5666-4b96-8bc0-9337babe7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names\n",
    "weightsmodel_cv_name = 'cv_rfwm_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b187aa7-4edd-4288-88ea-fe5a6ca96a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to tune random forest weights kernels\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(20, 1000, 20)],  \n",
    "    'min_samples_leaf': [x for x in range(10, 1000, 10)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'random_search': True,\n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 8,\n",
    "    'verbose': 2\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9828a1-0293-4bdb-949d-fb5440c0b446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune random forest weights models for tau=0,...,4\n",
    "for tau in taus:\n",
    "        \n",
    "    # Select training data\n",
    "    train = (ID_Data.sale_yearweek < test_start)\n",
    "    rolling_horizon = [l for l in range(0,tau+1)]\n",
    "        \n",
    "    # Select training data\n",
    "    ID_Data_train = ID_Data.loc[train]\n",
    "    X_Data_z_train = X_Data_z.loc[train]\n",
    "    Y_Data_z_train = Y_Data_z.loc[train].iloc[:,rolling_horizon]\n",
    "\n",
    "    # Reshape to match (tau+1)-periods rolling horizon\n",
    "    timePeriods = ID_Data_train.sale_yearweek\n",
    "    maxTimePeriod = test_start-1\n",
    "    \n",
    "    id_train = pp.reshape_data(ID_Data_train, timePeriods, maxTimePeriod, tau)\n",
    "    X_train_z = pp.reshape_data(X_Data_z_train, timePeriods, maxTimePeriod, tau)\n",
    "    y_train_z = pp.reshape_data(Y_Data_z_train, timePeriods, maxTimePeriod, tau)\n",
    "    \n",
    "    # Tansfrom data to arrays\n",
    "    X_train_z = np.array(X_train_z)\n",
    "    y_train_z = np.array(y_train_z).flatten() if np.array(y_train_z).shape[1] == 1 else np.array(y_train_z)    \n",
    "\n",
    "    # Initialize\n",
    "    weightsmodel = RandomForestWeightsModel()\n",
    "\n",
    "    # CV folds\n",
    "    cv_folds = pp.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = weightsmodel.tune(X=X_train_z, y=y_train_z, cv_folds=cv_folds, model_params=model_params, \n",
    "                                   tuning_params=tuning_params, hyper_params_grid=hyper_params_grid)\n",
    "    \n",
    "    # Save\n",
    "    weightsmodel.save_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fff30-5122-4776-a507-85b29f06ecae",
   "metadata": {},
   "source": [
    "### Fit weights model and generate weights\n",
    "\n",
    "We now fit the global random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$ and period $t=1,...,T$. This is done across all products at once (global training). Then, for each $\\tau=0,...,4$ and period $t=1,...,T$, we generate for each product (SKU) $k=1,...,M$ the weights given the test feature $x_{k,t}$. This is done *jointly* across products for computational efficiency - the weights for each product are extracted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae5a0d-a93b-4f93-836e-77dc00c4bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names\n",
    "weightsmodel_cv_name = 'cv_rfwm_global'\n",
    "weightsmodel_name = 'rfwm_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26099e4-5a4b-4b06-87c8-b56b726da927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "samples = {}\n",
    "weightfunctions = {}\n",
    "weights = {}\n",
    "exec_time_sec = {}\n",
    "cpu_time_sec = {}\n",
    "        \n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Initialize\n",
    "    samples[tau] = {}\n",
    "    weightfunctions[tau] = {}\n",
    "    weights[tau] = {}\n",
    "    exec_time_sec[tau] = {}\n",
    "    cpu_time_sec[tau] = {}\n",
    "\n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "        \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_t = min(tau,T-t)\n",
    "\n",
    "        # Status\n",
    "        print('#### Look-ahead tau='+str(tau)+' (adjusted to tau\\'='+str(tau_t)+'), period t='+str(t)+' ...')\n",
    "        start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        exec_time_sec[tau][t] = {}\n",
    "        cpu_time_sec[tau][t] = {}\n",
    "        \n",
    "        # Select training and test data\n",
    "        train = (ID_Data.sale_yearweek < test_start+t-1)\n",
    "        test = (ID_Data.sale_yearweek == test_start+t-1)\n",
    "        rolling_horizon = [l for l in range(0,tau_t+1)]\n",
    "        \n",
    "        ID_Data_train, ID_Data_test = ID_Data.loc[train], ID_Data.loc[test]\n",
    "        X_Data_z_train, X_Data_z_test = X_Data_z.loc[train], X_Data_z.loc[test]\n",
    "        Y_Data_z_train, Y_Data_z_test = Y_Data_z.loc[train].iloc[:,rolling_horizon], Y_Data_z.loc[test].iloc[:,rolling_horizon]\n",
    "        Y_Data_train, Y_Data_test = Y_Data.loc[train].iloc[:,rolling_horizon], Y_Data.loc[test].iloc[:,rolling_horizon]\n",
    "\n",
    "        # Reshape to match (tau+1)-periods rolling horizon\n",
    "        timePeriods = ID_Data_train.sale_yearweek\n",
    "        maxTimePeriod = test_start-1+t-1\n",
    "    \n",
    "        id_train = pp.reshape_data(ID_Data_train, timePeriods, maxTimePeriod, tau_t)\n",
    "        X_train_z = pp.reshape_data(X_Data_z_train, timePeriods, maxTimePeriod, tau_t)\n",
    "        y_train_z = pp.reshape_data(Y_Data_z_train, timePeriods, maxTimePeriod, tau_t)\n",
    "        y_train = pp.reshape_data(Y_Data_train, timePeriods, maxTimePeriod, tau_t)\n",
    "        \n",
    "        # Tansfrom data to arrays\n",
    "        X_train_z, X_test_z = np.array(X_train_z), np.array(X_Data_z_test)\n",
    "        y_train_z, y_test_z = np.array(y_train_z), np.array(Y_Data_z_test)\n",
    "        y_train, y_test = np.array(y_train), np.array(Y_Data_test) \n",
    "        \n",
    "        if tau_t == 0:\n",
    "            \n",
    "            y_train_z, y_test_z = y_train_z.flatten(), y_test_z.flatten()\n",
    "            y_train, y_test = y_train.flatten(), y_test.flatten() \n",
    "\n",
    "        # Check if fit already exists, due to the adjusted look-ahead tau_t:\n",
    "        if t in weightfunctions[tau_t].keys() if tau_t in weightfunctions.keys() else False:\n",
    "            \n",
    "            # Set weightsmodel to fitted weightsmodel already existing\n",
    "            weightsmodel = weightfunctions[tau_t][t]\n",
    "            weightfunctions[tau][t] = weightfunctions[tau_t][t]\n",
    "            exec_time_sec[tau][t]['fit'] = exec_time_sec[tau_t][t]['fit']\n",
    "            cpu_time_sec[tau][t]['fit'] = cpu_time_sec[tau_t][t]['fit']\n",
    "            \n",
    "        else: \n",
    "                    \n",
    "            # Timer start\n",
    "            st_exec = time.time()\n",
    "            st_cpu = time.process_time() \n",
    "        \n",
    "            # Initialize weights model\n",
    "            weightsmodel = RandomForestWeightsModel()\n",
    "\n",
    "            # Load cv results\n",
    "            weightsmodel.load_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau_t)+'.joblib')\n",
    "\n",
    "            # Fit random forest weights model\n",
    "            weightfunctions[tau][t] = weightsmodel.fit(X=X_train_z, y=y_train_z, model_params={'n_jobs': 32, 'verbose': 0})\n",
    "\n",
    "            # Timer end\n",
    "            exec_time_sec[tau][t]['fit'] = time.time()-st_exec\n",
    "            cpu_time_sec[tau][t]['fit'] = time.process_time()-st_cpu\n",
    "\n",
    "        # Check if weights already exists due to the adjusted look-ahead tau_t:\n",
    "        if t in weights[tau_t].keys() if tau_t in weights.keys() else False:\n",
    "            \n",
    "            # Set weights to weights already existing\n",
    "            weights[tau][t] = weights[tau_t][t]\n",
    "            exec_time_sec[tau][t]['weights'] = exec_time_sec[tau_t][t]['weights']\n",
    "            cpu_time_sec[tau][t]['weights'] = cpu_time_sec[tau_t][t]['weights']\n",
    "            \n",
    "        else: \n",
    "\n",
    "            # Timer start\n",
    "            st_exec = time.time()\n",
    "            st_cpu = time.process_time()  \n",
    "            \n",
    "                    \n",
    "            \"\"\"\n",
    "            Note: Each row in X_test belongs to a different product, i.e., we create each n weights for each of the m products\n",
    "            in X_test at once for the current period and look-ahead.\n",
    "\n",
    "            \"\"\"\n",
    "        \n",
    "            # Initialize\n",
    "            weights[tau][t] = {}\n",
    "            \n",
    "            # Get weights\n",
    "            w = weightsmodel.apply(X=X_train_z, x=X_test_z, model_params={'n_jobs': 32, 'verbose': 0})    \n",
    "\n",
    "            # Store weights for each X_test_z[m,] separately (each corresponding to a test product (SKU))\n",
    "            for SKU in SKUs:\n",
    "                \n",
    "                weights[tau][t][SKU] = w[ID_Data_test.SKU==SKU,].flatten()\n",
    "                \n",
    "            # Timer end\n",
    "            exec_time_sec[tau][t]['weights'] = time.time()-st_exec\n",
    "            cpu_time_sec[tau][t]['weights'] = time.process_time()-st_cpu\n",
    "            \n",
    "            \n",
    "        # Check if samples already exists due to the adjusted look-ahead tau_t:\n",
    "        if t in samples[tau_t].keys() if tau_t in samples.keys() else False:\n",
    "            \n",
    "            # Set samples to samples already existing\n",
    "            samples[tau][t] = samples[tau_t][t]\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            # Initialize\n",
    "            samples[tau][t] = {}\n",
    "            \n",
    "            # Store unscaled samples for each test product separately\n",
    "            for SKU in SKUs:\n",
    "                \n",
    "                samples[tau][t][SKU] = {'y_train': y_train,\n",
    "                                        'y_test': y_test[ID_Data_test.SKU==SKU], \n",
    "                                        'id_train': id_train,\n",
    "                                        'id_test': ID_Data_test.loc[ID_Data_test.SKU==SKU]}\n",
    "\n",
    "        # Status\n",
    "        print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "\n",
    "# Save results\n",
    "joblib.dump(samples, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples.joblib')    \n",
    "joblib.dump(weightfunctions, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions.joblib')    \n",
    "joblib.dump(weights, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights.joblib')    \n",
    "joblib.dump(exec_time_sec, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightsmodel_exec_time_sec.joblib')  \n",
    "joblib.dump(cpu_time_sec, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightsmodel_cpu_time_sec.joblib')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383db927-7544-4179-a25a-4a16269ed58e",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbcc6d-7bec-4f29-a741-7bca6b4a6b54",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "We first define a function that prepares the data needed for an experiment (depending on the model/approach). \n",
    "\n",
    "If no sampling strategy is provided via the optional argument 'sampling', no weights are retrieved, else 'global' or 'local' weights are retrieved and historical demands are prepared for 'global' or 'local' sampling, respectively. \n",
    "    \n",
    "If the optional argument 'e' is provided, the function additionally outputs 'epsilon' which is the uncertainty set threshold for robust optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b642319-45d6-427c-b0ce-6d51380bccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data to a run an experiment over the full planning horizon\n",
    "def prep_data(SKU, tau, T, sale_yearweek, path_data, path_samples, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    This function prepares the data needed for (weighted, robust) optimization. If no sampling strategy is\n",
    "    provided via the optional argument 'sampling', no weights are retrieved, else 'global' or 'local' weights\n",
    "    are retrieved and historical demands are prepared for 'global' or 'local' sampling, respectively. If the\n",
    "    optional argument 'e' is provided, the function additionally outputs 'epsilon' which is the uncertainty\n",
    "    set threshold for robust optimization.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        SKU: product (SKU) identifier\n",
    "        tau: length of rolling look-ahead horizon\n",
    "        T: Length T of the test horizon\n",
    "        sale_yearweek: Last sale_yearweek of training data\n",
    "        path_data: path of data\n",
    "        path_samples: path of samples\n",
    "\n",
    "    Optional arguments: \n",
    "\n",
    "        sampling: Sampling strategy (either 'global', 'local'), with\n",
    "            - 'global': uses weights generated with global training\n",
    "            - 'local': uses weights generated with local training\n",
    "        e: Robust uncertainty set threshold multiplier, with\n",
    "            - int: uses e as multiplier for product's in sample standard deviation as the uncertainty set threshold \n",
    "\n",
    "    Output:\n",
    "\n",
    "        y: demand data - np.array of shape (n_samples, n_periods)\n",
    "        ids_train: list of selector series (True/False of length n_samples) - list with lengths of the test horizon\n",
    "        ids_test: list of selector series (True/False of length n_samples) - list with lengths of the test horizon\n",
    "\n",
    "        weights (optional): list of weights (flat np.array of length ids_train of t'th test period) - list \n",
    "        with length of test horizon\n",
    "        epsilons (optional): list of epsilons - list with length of the test horizon\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Demand samples\n",
    "    robj = pyreadr.read_r(path_data+'/Y_Data_mv_NEW.RData')\n",
    "    y_samples = np.array(robj['Y_Data_mv'])\n",
    "\n",
    "    # IDs of local demand samples\n",
    "    robj = pyreadr.read_r(path_data+'/ID_Data_NEW.RData')\n",
    "    ID_samples = robj['ID_Data']\n",
    "\n",
    "    # IDs of local demand samples\n",
    "    robj = pyreadr.read_r(path_samples+'/SKU'+str(SKU)+'/Static/TmpFiles'+\n",
    "                          str(tau)+'/ID_samples_k.RDS')\n",
    "    ID_samples_SKU = robj[None]\n",
    "\n",
    "    # If sampling strategy is provided\n",
    "    if 'sampling' in kwargs:\n",
    "\n",
    "        # Weights\n",
    "        with open(path_samples+'/SKU'+str(SKU)+'/Static/Weights'+\n",
    "                  str(tau)+'/weights_'+kwargs['sampling']+'_ij.p', 'rb') as f:\n",
    "            weighty_ij = pickle.load(f)\n",
    "        del f\n",
    "\n",
    "        # Demand samples for global sampling\n",
    "        if kwargs['sampling'] == 'global':\n",
    "            y = y_samples\n",
    "\n",
    "        # Demand samples for local sampling\n",
    "        if kwargs['sampling'] == 'local':\n",
    "            y = y_samples[ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]]\n",
    "\n",
    "    # Default: local demand samples\n",
    "    else:\n",
    "        y = y_samples[ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]]\n",
    "\n",
    "\n",
    "    # Reshape data for each t=1...T (i.e., each period of the test horizon)\n",
    "    ids_train = []\n",
    "    ids_test = []\n",
    "\n",
    "    weights = [] if 'sampling' in kwargs else None\n",
    "    epsilons = [] if 'e' in kwargs else None\n",
    "\n",
    "    # Iterate over t\n",
    "    for t in range(T):\n",
    "\n",
    "        # If sampling strategy is provided\n",
    "        if 'sampling' in kwargs:\n",
    "\n",
    "            # IDs of demand samples for global sampling\n",
    "            if kwargs['sampling'] == 'global':\n",
    "                ids_train = ids_train + [ID_samples.sale_yearweek < sale_yearweek+t]\n",
    "\n",
    "            # IDs of demand samples for local sampling\n",
    "            if kwargs['sampling'] == 'local':\n",
    "                ids_train = ids_train + [(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                         (ID_samples.sale_yearweek < sale_yearweek+t)]                   \n",
    "\n",
    "            # Weights for global/local\n",
    "            weights = weights + [weighty_ij[t+1]]\n",
    "\n",
    "        # Default: IDs of demand samples for local sampling\n",
    "        else:\n",
    "            ids_train = ids_train + [(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                         (ID_samples.sale_yearweek < sale_yearweek+t)]\n",
    "\n",
    "\n",
    "\n",
    "        # IDs of demand samples for testing \n",
    "        ids_test = ids_test + [(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                         (ID_samples.sale_yearweek == sale_yearweek+t)]\n",
    "\n",
    "\n",
    "        # If e is provided, calculate robust optimization parameter epsilon\n",
    "        if 'e' in kwargs:\n",
    "            epsilons = epsilons + [kwargs['e']*np.std(y_samples[(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                                                (ID_samples.sale_yearweek < sale_yearweek+t),0])]\n",
    "\n",
    "\n",
    "    # Return\n",
    "    return y, ids_train, ids_test, weights, epsilons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5cbb4-fff4-4337-8b49-c6de6af255fc",
   "metadata": {},
   "source": [
    "### Experiment wrapper\n",
    "\n",
    "We now define a 'wrapper' function that iterates the experiment for a given SKU over differen cost parameter settins and lengths of the rolling look-ahead horizon tau.\n",
    "\n",
    "If the parameter sampling is provided (either 'global' or 'local), the function uses the specified sampling strategy. Else, SAA is performed. If the multiplier 'e' for the uncertainty set threshold epsilon is provided, the function performs the robust extension.\n",
    " \n",
    "Where: epsilon[t] = e *  in-sample standard deviation of the current product (SKU).\n",
    "\n",
    "The function prepares and calls the experiment over t=1...T for each cost paramater setting and look-ahead horizon tau and then summarises the results including performance and performance meta inormation. It also saves the results in CSV format to the specified path and the function can also be used in parallel processing environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc346f-5a76-43e8-bb52-60985447651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(SKU, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Description ...\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        SKU: product (SKU) identifier\n",
    "        sale_yearweek: Last sale_yearweek of training data\n",
    "        T: Length T of the test horizon\n",
    "        tau: List of lengths of rolling look-ahead horizons\n",
    "        cost_params: dictionary/dictionary of dictionaries of cost parameters {'CR', 'K', 'u', 'h', 'b'}\n",
    "        gurobi_params: dictionary of gurobi meta params {'LogToConsole', 'Threads', 'NonConvex' \n",
    "                                                         'PSDTol', 'MIPGap', 'NumericFocus',\n",
    "                                                         'obj_improvement', obj_timeout_sec'}\n",
    "        path: directory where results should be saved\n",
    "        model_name: model name for the file to save results\n",
    "        \n",
    "    Optional arguments:\n",
    "    \n",
    "        sampling: sampling strategy (either 'global' or 'local'); performs SAA if not provided\n",
    "        e: robust uncertainty set threshold multiplier; performs no robust extension if not provided\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    st_exec = time.time()\n",
    "    st_cpu = time.process_time()\n",
    "    \n",
    "    # Print progress\n",
    "    if kwargs['print_progress']: \n",
    "        print('SKU:', SKU)\n",
    "    \n",
    "    # Initialize\n",
    "    rhopt = RollingHorizonOptimization()\n",
    "    results = pd.DataFrame()\n",
    "   \n",
    "    # For each cost param setting\n",
    "    for cost_params in kwargs['cost_params'].values():\n",
    "        \n",
    "        # Print progress\n",
    "        if kwargs['print_progress']: \n",
    "            print('... cost param setting:', cost_params)\n",
    "    \n",
    "        # For each rolling look-ahead horizon\n",
    "        for tau in kwargs['tau']:\n",
    "            \n",
    "            # Print progress\n",
    "            if kwargs['print_progress']: \n",
    "                print('...... look-ahead horizon:', tau)\n",
    "    \n",
    "            ## Weighted SAA\n",
    "            if 'sampling' in kwargs:\n",
    "    \n",
    "                ## Weighted Robust SAA  \n",
    "                if 'e' in kwargs:\n",
    "\n",
    "                    # Prepare data\n",
    "                    data = prep_data(SKU, tau, kwargs['T'], kwargs['sale_yearweek'], PATH_DATA, PATH_SAMPLES, sampling=kwargs['sampling'], e=kwargs['e'])\n",
    "                    y, ids_train, ids_test, weights, epsilons = data\n",
    "                    \n",
    "                    # Create empty model\n",
    "                    wsaamodel = RobustWeightedSAA(**kwargs['gurobi_params'])\n",
    "\n",
    "                    # Run rolling horizon model over t=1...T\n",
    "                    result = rhopt.run(y, ids_train, ids_test, tau, wsaamodel, weights=weights, epsilons=epsilons, **cost_params)\n",
    "\n",
    "                ## Weighted SAA\n",
    "                else: \n",
    "                    \n",
    "                    # Prepare data\n",
    "                    data = prep_data(SKU, tau, kwargs['T'], kwargs['sale_yearweek'], PATH_DATA, PATH_SAMPLES, sampling=kwargs['sampling'])\n",
    "                    y, ids_train, ids_test, weights, _ = data\n",
    "                    \n",
    "                    # Create empty model\n",
    "                    wsaamodel = WeightedSAA(**kwargs['gurobi_params'])\n",
    "\n",
    "                    # Run rolling horizon model over t=1...T\n",
    "                    result = rhopt.run(y, ids_train, ids_test, tau, wsaamodel, weights=weights, **cost_params)\n",
    "\n",
    "\n",
    "            ## SAA\n",
    "            else:\n",
    "                \n",
    "                # Prepare data\n",
    "                data = prep_data(SKU, tau, kwargs['T'], kwargs['sale_yearweek'], PATH_DATA, PATH_SAMPLES)\n",
    "                y, ids_train, ids_test, _, _ = data\n",
    "\n",
    "                # Create empty model\n",
    "                wsaamodel = WeightedSAA(**kwargs['gurobi_params'])\n",
    "\n",
    "                # Run rolling horizon model over t=1...T\n",
    "                result = rhopt.run(y, ids_train, ids_test, tau, wsaamodel, **cost_params)\n",
    "\n",
    "            \n",
    "            ## ToDo: ExPost\n",
    "            \n",
    "            # Store result\n",
    "            meta = pd.DataFrame({\n",
    "\n",
    "                'SKU': np.repeat(SKU,kwargs['T']),\n",
    "                'n_periods': np.repeat(kwargs['T'],kwargs['T']),\n",
    "                'tau': np.repeat(tau,kwargs['T']),\n",
    "                'CR': np.repeat(cost_params['CR'],kwargs['T']),\n",
    "                'LogToConsole': np.repeat(kwargs['gurobi_params']['LogToConsole'],kwargs['T']),\n",
    "                'Threads': np.repeat(kwargs['gurobi_params']['Threads'],kwargs['T']),\n",
    "                'NonConvex': np.repeat(kwargs['gurobi_params']['NonConvex'],kwargs['T']),\n",
    "                'PSDTol': np.repeat(kwargs['gurobi_params']['PSDTol'],kwargs['T']),\n",
    "                'MIPGap': np.repeat(kwargs['gurobi_params']['MIPGap'],kwargs['T']),\n",
    "                'NumericFocus': np.repeat(kwargs['gurobi_params']['NumericFocus'],kwargs['T']),\n",
    "                'obj_improvement': np.repeat(kwargs['gurobi_params']['obj_improvement'],kwargs['T']),\n",
    "                'obj_timeout_sec': np.repeat(kwargs['gurobi_params']['obj_timeout_sec'],kwargs['T']),\n",
    "                'e': np.repeat(kwargs['e'],kwargs['T']) if 'e' in kwargs else np.repeat(0,kwargs['T']),\n",
    "                'epsilon': [epsilon for epsilon in epsilons] if 'e' in kwargs else np.repeat(0,kwargs['T'])\n",
    "            })\n",
    "\n",
    "            result = pd.concat([meta, result], axis=1)\n",
    "\n",
    "            # Store\n",
    "            if not results.empty:\n",
    "                results = results.append(result)   \n",
    "            else:\n",
    "                results = pd.DataFrame(result) \n",
    "\n",
    "    # Save result\n",
    "    save_log = results.to_csv(\n",
    "        path_or_buf=kwargs['path']+'/'+kwargs['model_name']+'_SKU'+str(SKU)+(('_e'+str(kwargs['e'])) if 'e' in kwargs else '')+'.csv', \n",
    "        sep=',', index=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Time\n",
    "    exec_time_sec = time.time() - st_exec\n",
    "    cpu_time_sec = time.process_time() - st_cpu\n",
    "    \n",
    "    # Print progress\n",
    "    if kwargs['print_progress']: \n",
    "        print('>>>> Done:',str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    \n",
    "    # Returns results \n",
    "    if (kwargs['return_results'] if 'return_results' in kwargs else False):\n",
    "        return results\n",
    "    \n",
    "    # Returns a log\n",
    "    else:\n",
    "        return  {'SKU': SKU, 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5ad57-a7b9-4993-8ab8-259e7672d77b",
   "metadata": {},
   "source": [
    "### Context Manager\n",
    "\n",
    "This is a context manager for parellel execution with the purpose of reporting progress. \n",
    "\n",
    "Credits: https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14c5a8-8076-49c0-ad02-7293ce48ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fff32-4632-47ff-9cf7-96be464bcbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54623ac-bde3-493b-9622-f007f064de8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5a05f-860e-4072-94ce-dcb19a1390d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561b5a1c-c253-41b3-be40-866598943f7e",
   "metadata": {},
   "source": [
    "### Rolling Horizon Global Weighted SAA (GwSAA)\n",
    "\n",
    "Description ...\n",
    "\n",
    "The code below runs an experiment for all given products (SKUs) $k=1,...,M$ over a test planning horizon $t=1,...,T$ with $T=13$ for three different cost parameter settings $\\{K, u, h, b\\}$ that vary the critical ratio ($CR=\\frac{b}{b+h}$) of holding and backlogging yielding\n",
    "- $CR=0.50$: $\\{K=100, u=0.5, h=1, b=1\\}$\n",
    "- $CR=0.75$: $\\{K=100, u=0.5, h=1, b=3\\}$\n",
    "- $CR=0.90$: $\\{K=100, u=0.5, h=1, b=9\\}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59a764-18cd-4471-a18d-dab5dc4761bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(wsaamodel, samples, weights=None, epsilons=None, cost_params=None, print_progress=False, \n",
    "                    path_to_save=None, name_to_save=None, return_results=True, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Timer\n",
    "    st_exec = time.time()\n",
    "    st_cpu = time.process_time()\n",
    "    \n",
    "    # Status\n",
    "    if print_progress and 'SKU' in kwargs: \n",
    "        print('SKU:', kwargs['SKU'])\n",
    "    \n",
    "    # Initialize\n",
    "    ropt = RollingHorizonOptimization()\n",
    "    results = pd.DataFrame()\n",
    "   \n",
    "    # If cost params are provided\n",
    "    if not cost_params is None:\n",
    "        \n",
    "        # If provided, has to be a list of dict(s)\n",
    "        if type(cost_params)==list:\n",
    "\n",
    "            # For each cost param setting (if provied) \n",
    "            for cost_params_ in cost_params:\n",
    "\n",
    "                # Print progress\n",
    "                if print_progress: print('... cost param setting:', cost_params_)\n",
    "\n",
    "                # Apply (Weighted) SAA  model\n",
    "                wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "                result = ropt.run2(wsaamodel, samples, weights, epsilons)\n",
    "\n",
    "                ## Store results\n",
    "                meta = pd.DataFrame({'CR': cost_params_['CR'], **kwargs}, index=list(range(len(samples))))\n",
    "                results = pd.concat([results, pd.concat([meta, result], axis=1)], axis=0)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # Raise error if cost params is not None but also no list\n",
    "            raise ValueError('If provided, cost_params has to be a list of at least one dict with keys {K, u, h, b}')\n",
    "    \n",
    "    # If no cost params provided, run with model's current params (default or as initialized)\n",
    "    else:\n",
    "\n",
    "        # Apply (Weighted) SAA  model\n",
    "        wsaamodel.set_params(**kwargs)\n",
    "        result = ropt.run2(wsaamodel, samples, weights, epsilons)\n",
    "\n",
    "        ## Store results\n",
    "        meta = pd.DataFrame(kwargs, index=list(range(len(samples))))\n",
    "        results = pd.concat([results, pd.concat([meta, result], axis=1)], axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save result as csv file\n",
    "    if not path_to_save is None and not name_to_save is None:\n",
    "        results.to_csv(path_or_buf=path_to_save+'/'+name_to_save+'_SKU'+str(SKU)+'.csv', sep=',', index=False)\n",
    "\n",
    "    \n",
    "    # Timer\n",
    "    exec_time_sec = time.time() - st_exec\n",
    "    cpu_time_sec = time.process_time() - st_cpu\n",
    "    \n",
    "    # Status\n",
    "    if print_progress: \n",
    "        print('>>>> Done:',str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    # Return  \n",
    "    output = results if return_results else {'SKU': SKU, 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235330d3-7c86-4e08-a06b-88876e4db2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': [\n",
    "\n",
    "        {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "\n",
    "    ],\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    #'path_to_save': PATH_RESULTS+'/GwSAA_NEW',\n",
    "    #'name_to_save': 'GwSAA_NEW',\n",
    "    'print_progress': True,\n",
    "    #'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e8c4c-38d5-4900-8bd7-a6f049428326",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 4\n",
    "SKU = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f525b-99eb-4703-9a8a-76b30391e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing\n",
    "weightsmodel_name = 'rfwm_local'\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples.joblib')    \n",
    "weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights.joblib')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91757c2-944b-4e35-bc1a-6ecfbb4b7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ = {}\n",
    "samples_ = {}\n",
    "\n",
    "for t in ts:\n",
    "    weights_[t] = weights[tau][t][SKU]\n",
    "    samples_[t] = samples[tau][t][SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbd3fb-e960-4cd4-bc4a-0b68bf42165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = run_experiments(wsaamodel=WeightedSAA(), samples=samples_, weights=weights_, **experiment_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b73e9-7e34-4055-ab23-9d800a4b5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da7574-2ba2-4d06-b7df-911ab3381fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f392e58-bbeb-4887-8d2a-cfebdbd02d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798971a9-57eb-4507-894d-38e16356a139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91d52d-ba81-4100-aaf0-4b95ae4196a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcedd012-6106-4165-952a-91e3911f7d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506e97d-2683-4359-b3e2-bb7e060d59ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5eeb4d-24e3-4571-966e-82ede17d670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurobi_params = {\n",
    "\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "}\n",
    "\n",
    "cost_params=[\n",
    "\n",
    "    {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "    {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "    {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5b36a-4bb7-45f8-b65c-ca01d5b2b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 4\n",
    "SKU = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56849c1-31d8-400b-bf93-f4e3e21b2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing\n",
    "weightsmodel_name = 'rfwm_global'\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples.joblib')    \n",
    "weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights.joblib')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa102a3-3ac7-4870-963e-492f01c56774",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ = {}\n",
    "samples_ = {}\n",
    "\n",
    "for t in ts:\n",
    "    weights_[t] = weights[tau][t][SKU]\n",
    "    samples_[t] = samples[tau][t][SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8aa89-4c9d-4c9b-85ed-a405e5ab0f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_global = run_experiments(wsaamodel=WeightedSAA(), samples=samples_, weights=weights_, cost_params=cost_params, print_progress=True, **gurobi_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82457f52-2745-4292-afd2-1f23b9dd033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_global.to_csv('test_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e698c-3e37-425d-9849-889751eb0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[tau][t][SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fc553-1473-44b9-911e-78248b163535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b95c7-3a43-4b85-9d7e-3e93f0db2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing\n",
    "weightsmodel_name = 'rfwm_local'\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples.joblib')    \n",
    "weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights.joblib')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d1bfd-c5e3-4838-8a47-866f5efac4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ = {}\n",
    "samples_ = {}\n",
    "\n",
    "for t in ts:\n",
    "    weights_[t] = weights[tau][t][SKU]\n",
    "    samples_[t] = samples[tau][t][SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6023d7-b028-4cb3-a3d4-73d2edc119d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_local = run_experiments(wsaamodel=WeightedSAA(), samples=samples_, weights=weights_, cost_params=cost_params, print_progress=True, **gurobi_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee23cd-6fe8-49e2-9581-73dbe988ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_local.to_csv('test_local.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd8979-5b53-4d79-94fd-8f23d3121e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_saa = run_experiments(wsaamodel=WeightedSAA(), samples=samples_, cost_params=cost_params, print_progress=True, **gurobi_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2cf1b-3a2b-44b0-a0b7-040017822e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_saa.to_csv('test_saa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9785d-fead-4137-849c-721903025aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa87898-8bbf-4464-bb14-9d6d9ff2d334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd852a9a-b0d0-4d21-a7bb-8b805730d0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80955b38-a2c5-4b4f-bce4-8de0ae43dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical demand samples available in period t\n",
    "d = samples[t]['y_train']\n",
    "\n",
    "# Weights for Weighted SAA\n",
    "if not weights is None:\n",
    "\n",
    "    w = weights[t] \n",
    "\n",
    "# Weights for SAA (all weights are 1/n_samples)\n",
    "else:\n",
    "\n",
    "    w = np.array([np.repeat(1/d.shape[0],d.shape[0])]).flatten()\n",
    "\n",
    "\n",
    "## Summarize weights and samples\n",
    "d = d[list(w > 0)]\n",
    "w = w[list(w > 0)]\n",
    "\n",
    "df=pd.DataFrame(data=np.hstack(\n",
    "    (w.reshape(w.shape[0],1), d.reshape(d.shape[0],1) if d.ndim == 1 else d)), \n",
    "        columns=[-1] + list(range(0,d.shape[1] if d.ndim > 1 else 1))).groupby(\n",
    "    list(range(0,d.shape[1] if d.ndim > 1 else 1))).agg(\n",
    "        w = (-1, np.sum)).reset_index()\n",
    "\n",
    "d = np.array(df[list(range(0,d.shape[1] if d.ndim > 1 else 1))])\n",
    "w = np.array([df.w]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64796e-112e-4b00-b64d-d859397f1ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372ccba-0c5c-4e4e-aa34-e37ce080e58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b5cc4-0601-4661-a82e-614163d858c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8691f7-b339-489e-a887-6264ee711267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea070a-103e-49e0-a607-570d231b4d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fd121-a00e-43ea-b920-a7bc7c31f3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55bfc9a-0125-49a6-b77e-36ac99f432d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7255cbf-361c-4b29-a1fc-b8577670bc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22204df8-830d-41a3-bbe5-0cdaa26142b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd4ab7-279c-4b26-96be-540e18a1d8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75456055-48ef-487c-925a-808404d35535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a287505f-69c3-446b-a7cc-621666c96273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f990e6a-3366-45d5-8ccf-bdc88876c5fc",
   "metadata": {},
   "source": [
    "### Rolling Horizon Global Robust Weighted SAA (GwSAA-R)\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109908f-979e-4e89-8c96-3bd91ad43f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b6628-604f-402e-8992-edf3368ed8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d357c25-392d-4686-be11-9fd8a580f007",
   "metadata": {},
   "source": [
    "# Local Models\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1793967-c0a7-49f9-afbe-3c2edc82a733",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We first load and pre-process the data. This includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences. For the local models, no demand- and feature scaling is needed.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ where n is the number of training observations (rows) in the data\n",
    "- **Y_Data** (pd.DataFrame) is the demand data $d_{j,n}$ (a times series per product)\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148d7df-58e8-42ed-a2c2-b7c3aa0256a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c111f-7ccb-4ddb-aa74-41e9c671bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a27ac-fb86-4586-97e1-04fb33da5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_Data_Columns = X_Data_Columns.loc[X_Data_Columns.Local == 'YES']\n",
    "X_Data = X_Data[X_Data_Columns.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badda3cf-e799-4d46-9fcf-af3ec6dac26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by SKU and sale_yearweek for preprocessing\n",
    "data = pd.concat([ID_Data, X_Data, Y_Data], axis=1).sort_values(by=['SKU', 'sale_yearweek']).reset_index(drop=True)\n",
    "\n",
    "ID_Data = data[ID_Data.columns]\n",
    "X_Data = data[X_Data.columns]\n",
    "Y_Data = data[Y_Data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e5ef2-ebdf-4e04-ac35-2d822a8ba7df",
   "metadata": {},
   "source": [
    "### Reshape to multi-period demand\n",
    "\n",
    "We now reshape the time series of demands per product to consecutive $(\\tau+1)$-periods demand vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4eed71-f73d-4868-b8dc-cceca3348f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-period vectors\n",
    "data = pd.concat([ID_Data, Y_Data], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be213da-406b-4e42-8699-2f5973a9c767",
   "metadata": {},
   "source": [
    "## Weights model\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two local models **wSAA** and **wSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor) and for each product (SKU) $k=1,...,M$ separately. Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) for each product (SKU) $k=1,...,M$ separately and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$ separatey)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0442f-cdaa-47ad-a691-4b2aac018b30",
   "metadata": {},
   "source": [
    "### Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the local random forest weights model for each product (SKU) $k=1,...,M$, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd47d2-3b07-4727-82fb-a995a973358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653420-33b5-4569-8397-2da1590e53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to tune random forest weights kernels\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(2, 20, 1)],  \n",
    "    'min_samples_leaf': [x for x in range(2, 10, 1)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'random_search': True,\n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 8,\n",
    "    'verbose': 2\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6600b3-2b06-4811-ae64-1df59b47c48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune random forest weights models for tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Initialize\n",
    "    cv_results = {}\n",
    "    \n",
    "    # For each product (SKU)\n",
    "    for SKU in SKUs:\n",
    "        \n",
    "        # Status\n",
    "        print('........................... SKU', str(SKU), '...........................')\n",
    "        \n",
    "        # Select training data\n",
    "        train = (ID_Data.SKU == SKU) & (ID_Data.sale_yearweek < test_start)\n",
    "        rolling_horizon = [l for l in range(0,tau+1)]\n",
    "\n",
    "        # Select training data\n",
    "        ID_Data_train = ID_Data.loc[train]\n",
    "        X_Data_train = X_Data.loc[train]\n",
    "        Y_Data_train = Y_Data.loc[train].iloc[:,rolling_horizon]\n",
    "\n",
    "        # Reshape to match (tau+1)-periods rolling horizon\n",
    "        timePeriods = ID_Data_train.sale_yearweek\n",
    "        maxTimePeriod = test_start-1\n",
    "\n",
    "        id_train = pp.reshape_data(ID_Data_train, timePeriods, maxTimePeriod, tau)\n",
    "        X_train = pp.reshape_data(X_Data_train, timePeriods, maxTimePeriod, tau)\n",
    "        y_train = pp.reshape_data(Y_Data_train, timePeriods, maxTimePeriod, tau)\n",
    "\n",
    "        # Tansfrom data to arrays\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).flatten() if np.array(y_train).shape[1] == 1 else np.array(y_train)    \n",
    "\n",
    "        # Initialize\n",
    "        weightsmodel = RandomForestWeightsModel()\n",
    "\n",
    "        # CV folds\n",
    "        cv_folds = pp.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "\n",
    "        # CV search\n",
    "        cv_results[SKU] = weightsmodel.tune(X=X_train, y=y_train, cv_folds=cv_folds, model_params=model_params, \n",
    "                                            tuning_params=tuning_params, hyper_params_grid=hyper_params_grid)\n",
    "\n",
    "    # Save\n",
    "    out = joblib.dump(cv_results, PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1c7a-4494-4191-858a-231dc1ba9fd0",
   "metadata": {},
   "source": [
    "### Fit weights model and generate weights\n",
    "\n",
    "We now fit a local random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately (local training). Then, for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately, we generate the weights given the test feature $x_{k,t}$. This is done *separately* for each product (SKU) $k=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410060e-ec5e-4f5d-8da2-7a81da076ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local'\n",
    "weightsmodel_name = 'rfwm_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4fd08f-4ed3-4076-9b3f-1c925c6ac3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "samples = {}\n",
    "weightfunctions = {}\n",
    "weights = {}\n",
    "exec_time_sec = {}\n",
    "cpu_time_sec = {}\n",
    "        \n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Initialize\n",
    "    samples[tau] = {}\n",
    "    weightfunctions[tau] = {}\n",
    "    weights[tau] = {}\n",
    "    exec_time_sec[tau] = {}\n",
    "    cpu_time_sec[tau] = {}\n",
    "\n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "        \n",
    "        # Initialize\n",
    "        samples[tau][t] = {}\n",
    "        weightfunctions[tau][t] = {}\n",
    "        weights[tau][t] = {}\n",
    "        exec_time_sec[tau][t] = {}\n",
    "        cpu_time_sec[tau][t] = {}\n",
    "            \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_t = min(tau,T-t)\n",
    "        \n",
    "        # Status\n",
    "        print('#### Look-ahead tau='+str(tau)+' (adjusted to tau\\'='+str(tau_t)+'), period t='+str(t)+'...')\n",
    "        start_time = dt.datetime.now().replace(microsecond=0)\n",
    "            \n",
    "        # For each product (SKU)\n",
    "        for SKU in SKUs:\n",
    "\n",
    "            # Timer\n",
    "            exec_time_sec[tau][t][SKU] = {}\n",
    "            cpu_time_sec[tau][t][SKU] = {}\n",
    "\n",
    "            # Select training and test data\n",
    "            train = (ID_Data.SKU == SKU) & (ID_Data.sale_yearweek < test_start+t-1)\n",
    "            test = (ID_Data.SKU == SKU) & (ID_Data.sale_yearweek == test_start+t-1)\n",
    "            rolling_horizon = [l for l in range(0,tau_t+1)]\n",
    "\n",
    "            ID_Data_train, ID_Data_test = ID_Data.loc[train], ID_Data.loc[test]\n",
    "            X_Data_train, X_Data_test = X_Data.loc[train], X_Data.loc[test]\n",
    "            Y_Data_train, Y_Data_test = Y_Data.loc[train].iloc[:,rolling_horizon], Y_Data.loc[test].iloc[:,rolling_horizon]\n",
    "\n",
    "            # Reshape to match (tau+1)-periods rolling horizon\n",
    "            timePeriods = ID_Data_train.sale_yearweek\n",
    "            maxTimePeriod = test_start-1+t-1\n",
    "\n",
    "            id_train = pp.reshape_data(ID_Data_train, timePeriods, maxTimePeriod, tau_t)\n",
    "            X_train = pp.reshape_data(X_Data_train, timePeriods, maxTimePeriod, tau_t)\n",
    "            y_train = pp.reshape_data(Y_Data_train, timePeriods, maxTimePeriod, tau_t)\n",
    "\n",
    "            # Tansfrom data to arrays\n",
    "            X_train, X_test = np.array(X_train), np.array(X_Data_test)\n",
    "            y_train, y_test = np.array(y_train), np.array(Y_Data_test) \n",
    "\n",
    "            if tau_t == 0: y_train, y_test = y_train.flatten(), y_test.flatten() \n",
    "\n",
    "            # Check if fit already exists, due to the adjusted look-ahead tau_t:\n",
    "            if tau_t in weightfunctions.keys():\n",
    "                if t in weightfunctions[tau_t].keys():\n",
    "                    if SKU in weightfunctions[tau_t][t].keys():\n",
    "\n",
    "                        # Set weightsmodel to fitted weightsmodel already existing\n",
    "                        weightsmodel = weightfunctions[tau_t][t][SKU]\n",
    "                        weightfunctions[tau][t][SKU] = weightfunctions[tau_t][t][SKU]\n",
    "                        exec_time_sec[tau][t][SKU]['fit'] = exec_time_sec[tau_t][t][SKU]['fit']\n",
    "                        cpu_time_sec[tau][t][SKU]['fit'] = cpu_time_sec[tau_t][t][SKU]['fit']\n",
    "\n",
    "                    else: \n",
    "\n",
    "                        # Timer start\n",
    "                        st_exec = time.time()\n",
    "                        st_cpu = time.process_time() \n",
    "\n",
    "                        # Initialize weights model\n",
    "                        weightsmodel = RandomForestWeightsModel()\n",
    "\n",
    "                        # Load cv results\n",
    "                        weightsmodel.load_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau_t)+'.joblib', SKU=SKU)\n",
    "\n",
    "                        # Fit random forest weights model\n",
    "                        weightfunctions[tau][t][SKU] = weightsmodel.fit(X=X_train, y=y_train, model_params={'n_jobs': 32, 'verbose': 0})\n",
    "\n",
    "                        # Timer end\n",
    "                        exec_time_sec[tau][t][SKU]['fit'] = time.time()-st_exec\n",
    "                        cpu_time_sec[tau][t][SKU]['fit'] = time.process_time()-st_cpu\n",
    "\n",
    "            # Check if weights already exists due to the adjusted look-ahead tau_t:\n",
    "            if tau_t in weights.keys():\n",
    "                if t in weights[tau_t].keys():\n",
    "                    if SKU in weights[tau_t][t].keys():\n",
    "                        \n",
    "                        # Set weights to weights already existing\n",
    "                        weights[tau][t][SKU] = weights[tau_t][t][SKU]\n",
    "                        exec_time_sec[tau][t][SKU]['weights'] = exec_time_sec[tau_t][t][SKU]['weights']\n",
    "                        cpu_time_sec[tau][t][SKU]['weights'] = cpu_time_sec[tau_t][t][SKU]['weights']\n",
    "\n",
    "                    else: \n",
    "\n",
    "                        # Timer start\n",
    "                        st_exec = time.time()\n",
    "                        st_cpu = time.process_time()  \n",
    "\n",
    "                        # Get weights\n",
    "                        weights[tau][t][SKU] = weightsmodel.apply(X=X_train, x=X_test, model_params={'n_jobs': 32, 'verbose': 0}).flatten()\n",
    "      \n",
    "                        # Timer end\n",
    "                        exec_time_sec[tau][t][SKU]['weights'] = time.time()-st_exec\n",
    "                        cpu_time_sec[tau][t][SKU]['weights'] = time.process_time()-st_cpu\n",
    "\n",
    "\n",
    "            # Check if samples already exists due to the adjusted look-ahead tau_t:\n",
    "            if tau_t in samples.keys():\n",
    "                if t in samples[tau_t].keys():\n",
    "                    if SKU in samples[tau_t][t].keys():\n",
    "\n",
    "                        # Set samples to samples already existing\n",
    "                        samples[tau][t][SKU] = samples[tau_t][t][SKU]\n",
    "\n",
    "                    else: \n",
    "\n",
    "                        # Store unscaled samples for each test product separately\n",
    "                        samples[tau][t][SKU] = {'y_train': y_train,\n",
    "                                                'y_test': y_test, \n",
    "                                                'id_train': id_train,\n",
    "                                                'id_test': ID_Data_test}\n",
    "\n",
    "        # Status\n",
    "        print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "\n",
    "# Save results\n",
    "joblib.dump(samples, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples.joblib')    \n",
    "joblib.dump(weightfunctions, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions.joblib')    \n",
    "joblib.dump(weights, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights.joblib')    \n",
    "joblib.dump(exec_time_sec, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightsmodel_exec_time_sec.joblib')  \n",
    "joblib.dump(cpu_time_sec, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightsmodel_cpu_time_sec.joblib')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab5e23-21d6-47c1-9e08-f92e129df0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f1a5d-7d57-42cd-a679-c9e2e34f32c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4aa74b-eeb8-43c4-b6a6-56b5add95f97",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c4b64-a7ff-4b64-8e95-ddcd851acca5",
   "metadata": {},
   "source": [
    "### Rolling Horizon Local Weighted SAA (wSAA)\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3ff83-6de0-4fad-8b99-a50eb9f3dd11",
   "metadata": {},
   "source": [
    "### Rolling Horizon Local Robust Weighted SAA (wSAA-R)\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650cf8f6-4fd6-4258-9498-759b49e4cbc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Baseline model: Rolling Horizon Local Weighted SAA (SAA)\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4a736-d792-468a-86a1-8c95976293ed",
   "metadata": {},
   "source": [
    "# Ex-post optimal model\n",
    "\n",
    "Description ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac601b03-e908-4458-aa86-1a5564c1f7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4d87f-b912-48bc-b7be-ef142b8c0478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49b4a4-51a2-4f0f-93ce-9c0e10d84277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08ce33-6328-49c9-a5ec-0a00cf7c0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b49f5c-5c40-4d02-8cc5-c68995eed6f6",
   "metadata": {},
   "source": [
    "# ARCHIVE >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a847e9-c739-49b3-9db4-73395c5e5eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca49fba-6106-4448-a755-b7c22849d0a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580d78a-d684-45ab-9ae3-f8125f284206",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We first define a function that prepares the data needed for an experiment (depending on the model/approach). \n",
    "\n",
    "If no sampling strategy is provided via the optional argument 'sampling', no weights are retrieved, else 'global' or 'local' weights are retrieved and historical demands are prepared for 'global' or 'local' sampling, respectively. \n",
    "    \n",
    "If the optional argument 'e' is provided, the function additionally outputs 'epsilon' which is the uncertainty set threshold for robust optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c77ade-d552-4ebd-94d8-2ffcd075dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data to a run an experiment over the full planning horizon\n",
    "def prep_data(SKU, tau, T, sale_yearweek, path_data, path_samples, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    This function prepares the data needed for (weighted, robust) optimization. If no sampling strategy is\n",
    "    provided via the optional argument 'sampling', no weights are retrieved, else 'global' or 'local' weights\n",
    "    are retrieved and historical demands are prepared for 'global' or 'local' sampling, respectively. If the\n",
    "    optional argument 'e' is provided, the function additionally outputs 'epsilon' which is the uncertainty\n",
    "    set threshold for robust optimization.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        SKU: product (SKU) identifier\n",
    "        tau: length of rolling look-ahead horizon\n",
    "        T: Length T of the test horizon\n",
    "        sale_yearweek: Last sale_yearweek of training data\n",
    "        path_data: path of data\n",
    "        path_samples: path of samples\n",
    "\n",
    "    Optional arguments: \n",
    "\n",
    "        sampling: Sampling strategy (either 'global', 'local'), with\n",
    "            - 'global': uses weights generated with global training\n",
    "            - 'local': uses weights generated with local training\n",
    "        e: Robust uncertainty set threshold multiplier, with\n",
    "            - int: uses e as multiplier for product's in sample standard deviation as the uncertainty set threshold \n",
    "\n",
    "    Output:\n",
    "\n",
    "        y: demand data - np.array of shape (n_samples, n_periods)\n",
    "        ids_train: list of selector series (True/False of length n_samples) - list with lengths of the test horizon\n",
    "        ids_test: list of selector series (True/False of length n_samples) - list with lengths of the test horizon\n",
    "\n",
    "        weights (optional): list of weights (flat np.array of length ids_train of t'th test period) - list \n",
    "        with length of test horizon\n",
    "        epsilons (optional): list of epsilons - list with length of the test horizon\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Demand samples\n",
    "    robj = pyreadr.read_r(path_data+'/Y_Data_mv_NEW.RData')\n",
    "    y_samples = np.array(robj['Y_Data_mv'])\n",
    "\n",
    "    # IDs of local demand samples\n",
    "    robj = pyreadr.read_r(path_data+'/ID_Data_NEW.RData')\n",
    "    ID_samples = robj['ID_Data']\n",
    "\n",
    "    # IDs of local demand samples\n",
    "    robj = pyreadr.read_r(path_samples+'/SKU'+str(SKU)+'/Static/TmpFiles'+\n",
    "                          str(tau)+'/ID_samples_k.RDS')\n",
    "    ID_samples_SKU = robj[None]\n",
    "\n",
    "    # If sampling strategy is provided\n",
    "    if 'sampling' in kwargs:\n",
    "\n",
    "        # Weights\n",
    "        with open(path_samples+'/SKU'+str(SKU)+'/Static/Weights'+\n",
    "                  str(tau)+'/weights_'+kwargs['sampling']+'_ij.p', 'rb') as f:\n",
    "            weighty_ij = pickle.load(f)\n",
    "        del f\n",
    "\n",
    "        # Demand samples for global sampling\n",
    "        if kwargs['sampling'] == 'global':\n",
    "            y = y_samples\n",
    "\n",
    "        # Demand samples for local sampling\n",
    "        if kwargs['sampling'] == 'local':\n",
    "            y = y_samples[ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]]\n",
    "\n",
    "    # Default: local demand samples\n",
    "    else:\n",
    "        y = y_samples[ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]]\n",
    "\n",
    "\n",
    "    # Reshape data for each t=1...T (i.e., each period of the test horizon)\n",
    "    ids_train = []\n",
    "    ids_test = []\n",
    "\n",
    "    weights = [] if 'sampling' in kwargs else None\n",
    "    epsilons = [] if 'e' in kwargs else None\n",
    "\n",
    "    # Iterate over t\n",
    "    for t in range(T):\n",
    "\n",
    "        # If sampling strategy is provided\n",
    "        if 'sampling' in kwargs:\n",
    "\n",
    "            # IDs of demand samples for global sampling\n",
    "            if kwargs['sampling'] == 'global':\n",
    "                ids_train = ids_train + [ID_samples.sale_yearweek < sale_yearweek+t]\n",
    "\n",
    "            # IDs of demand samples for local sampling\n",
    "            if kwargs['sampling'] == 'local':\n",
    "                ids_train = ids_train + [(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                         (ID_samples.sale_yearweek < sale_yearweek+t)]                   \n",
    "\n",
    "            # Weights for global/local\n",
    "            weights = weights + [weighty_ij[t+1]]\n",
    "\n",
    "        # Default: IDs of demand samples for local sampling\n",
    "        else:\n",
    "            ids_train = ids_train + [(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                         (ID_samples.sale_yearweek < sale_yearweek+t)]\n",
    "\n",
    "\n",
    "\n",
    "        # IDs of demand samples for testing \n",
    "        ids_test = ids_test + [(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                         (ID_samples.sale_yearweek == sale_yearweek+t)]\n",
    "\n",
    "\n",
    "        # If e is provided, calculate robust optimization parameter epsilon\n",
    "        if 'e' in kwargs:\n",
    "            epsilons = epsilons + [kwargs['e']*np.std(y_samples[(ID_samples.SKU_API == ID_samples_SKU.SKU_API[0]) &\n",
    "                                                                (ID_samples.sale_yearweek < sale_yearweek+t),0])]\n",
    "\n",
    "\n",
    "    # Return\n",
    "    return y, ids_train, ids_test, weights, epsilons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3eae89-6882-4d16-a921-5da051806bf7",
   "metadata": {},
   "source": [
    "## Experiment wrapper\n",
    "\n",
    "We now define a 'wrapper' function that iterates the experiment for a given SKU over differen cost parameter settins and lengths of the rolling look-ahead horizon tau.\n",
    "\n",
    "If the parameter sampling is provided (either 'global' or 'local), the function uses the specified sampling strategy. Else, SAA is performed. If the multiplier 'e' for the uncertainty set threshold epsilon is provided, the function performs the robust extension.\n",
    " \n",
    "Where: epsilon[t] = e *  in-sample standard deviation of the current product (SKU).\n",
    "\n",
    "The function prepares and calls the experiment over t=1...T for each cost paramater setting and look-ahead horizon tau and then summarises the results including performance and performance meta inormation. It also saves the results in CSV format to the specified path and the function can also be used in parallel processing environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f255241-2ab8-4370-90e6-a2a2a48dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(SKU, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Description ...\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        SKU: product (SKU) identifier\n",
    "        sale_yearweek: Last sale_yearweek of training data\n",
    "        T: Length T of the test horizon\n",
    "        tau: List of lengths of rolling look-ahead horizons\n",
    "        cost_params: dictionary/dictionary of dictionaries of cost parameters {'CR', 'K', 'u', 'h', 'b'}\n",
    "        gurobi_params: dictionary of gurobi meta params {'LogToConsole', 'Threads', 'NonConvex' \n",
    "                                                         'PSDTol', 'MIPGap', 'NumericFocus',\n",
    "                                                         'obj_improvement', obj_timeout_sec'}\n",
    "        path: directory where results should be saved\n",
    "        model_name: model name for the file to save results\n",
    "        \n",
    "    Optional arguments:\n",
    "    \n",
    "        sampling: sampling strategy (either 'global' or 'local'); performs SAA if not provided\n",
    "        e: robust uncertainty set threshold multiplier; performs no robust extension if not provided\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    st_exec = time.time()\n",
    "    st_cpu = time.process_time()\n",
    "    \n",
    "    # Print progress\n",
    "    if kwargs['print_progress']: \n",
    "        print('SKU:', SKU)\n",
    "    \n",
    "    # Initialize\n",
    "    rhopt = RollingHorizonOptimization()\n",
    "    results = pd.DataFrame()\n",
    "   \n",
    "    # For each cost param setting\n",
    "    for cost_params in kwargs['cost_params'].values():\n",
    "        \n",
    "        # Print progress\n",
    "        if kwargs['print_progress']: \n",
    "            print('... cost param setting:', cost_params)\n",
    "    \n",
    "        # For each rolling look-ahead horizon\n",
    "        for tau in kwargs['tau']:\n",
    "            \n",
    "            # Print progress\n",
    "            if kwargs['print_progress']: \n",
    "                print('...... look-ahead horizon:', tau)\n",
    "    \n",
    "            ## Weighted (Robust) SAA\n",
    "            if 'sampling' in kwargs:\n",
    "    \n",
    "                ## Weighted Robust SAA  \n",
    "                if 'e' in kwargs:\n",
    "\n",
    "                    # Prepare data\n",
    "                    data = prep_data(SKU, tau, kwargs['T'], kwargs['sale_yearweek'], PATH_DATA, PATH_SAMPLES, sampling=kwargs['sampling'], e=kwargs['e'])\n",
    "                    y, ids_train, ids_test, weights, epsilons = data\n",
    "                    \n",
    "                    # Create empty model\n",
    "                    wsaamodel = RobustWeightedSAA(**kwargs['gurobi_params'])\n",
    "\n",
    "                    # Run rolling horizon model over t=1...T\n",
    "                    result = rhopt.run(y, ids_train, ids_test, tau, wsaamodel, weights=weights, epsilons=epsilons, **cost_params)\n",
    "\n",
    "                ## Weighted SAA\n",
    "                else: \n",
    "                    \n",
    "                    # Prepare data\n",
    "                    data = prep_data(SKU, tau, kwargs['T'], kwargs['sale_yearweek'], PATH_DATA, PATH_SAMPLES, sampling=kwargs['sampling'])\n",
    "                    y, ids_train, ids_test, weights, _ = data\n",
    "                    \n",
    "                    # Create empty model\n",
    "                    wsaamodel = WeightedSAA(**kwargs['gurobi_params'])\n",
    "\n",
    "                    # Run rolling horizon model over t=1...T\n",
    "                    result = rhopt.run(y, ids_train, ids_test, tau, wsaamodel, weights=weights, **cost_params)\n",
    "\n",
    "\n",
    "            ## SAA\n",
    "            else:\n",
    "                \n",
    "                # Prepare data\n",
    "                data = prep_data(SKU, tau, kwargs['T'], kwargs['sale_yearweek'], PATH_DATA, PATH_SAMPLES)\n",
    "                y, ids_train, ids_test, _, _ = data\n",
    "\n",
    "                # Create empty model\n",
    "                wsaamodel = WeightedSAA(**kwargs['gurobi_params'])\n",
    "\n",
    "                # Run rolling horizon model over t=1...T\n",
    "                result = rhopt.run(y, ids_train, ids_test, tau, wsaamodel, **cost_params)\n",
    "\n",
    "            \n",
    "            ## ToDo: ExPost\n",
    "            \n",
    "            # Store result\n",
    "            meta = pd.DataFrame({\n",
    "\n",
    "                'SKU': np.repeat(SKU,kwargs['T']),\n",
    "                'n_periods': np.repeat(kwargs['T'],kwargs['T']),\n",
    "                'tau': np.repeat(tau,kwargs['T']),\n",
    "                'CR': np.repeat(cost_params['CR'],kwargs['T']),\n",
    "                'LogToConsole': np.repeat(kwargs['gurobi_params']['LogToConsole'],kwargs['T']),\n",
    "                'Threads': np.repeat(kwargs['gurobi_params']['Threads'],kwargs['T']),\n",
    "                'NonConvex': np.repeat(kwargs['gurobi_params']['NonConvex'],kwargs['T']),\n",
    "                'PSDTol': np.repeat(kwargs['gurobi_params']['PSDTol'],kwargs['T']),\n",
    "                'MIPGap': np.repeat(kwargs['gurobi_params']['MIPGap'],kwargs['T']),\n",
    "                'NumericFocus': np.repeat(kwargs['gurobi_params']['NumericFocus'],kwargs['T']),\n",
    "                'obj_improvement': np.repeat(kwargs['gurobi_params']['obj_improvement'],kwargs['T']),\n",
    "                'obj_timeout_sec': np.repeat(kwargs['gurobi_params']['obj_timeout_sec'],kwargs['T']),\n",
    "                'e': np.repeat(kwargs['e'],kwargs['T']) if 'e' in kwargs else np.repeat(0,kwargs['T']),\n",
    "                'epsilon': [epsilon for epsilon in epsilons] if 'e' in kwargs else np.repeat(0,kwargs['T'])\n",
    "            })\n",
    "\n",
    "            result = pd.concat([meta, result], axis=1)\n",
    "\n",
    "            # Store\n",
    "            if not results.empty:\n",
    "                results = results.append(result)   \n",
    "            else:\n",
    "                results = pd.DataFrame(result) \n",
    "\n",
    "    # Save result\n",
    "    save_log = results.to_csv(\n",
    "        path_or_buf=kwargs['path']+'/'+kwargs['model_name']+'_SKU'+str(SKU)+(('_e'+str(kwargs['e'])) if 'e' in kwargs else '')+'.csv', \n",
    "        sep=',', index=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Time\n",
    "    exec_time_sec = time.time() - st_exec\n",
    "    cpu_time_sec = time.process_time() - st_cpu\n",
    "    \n",
    "    # Print progress\n",
    "    if kwargs['print_progress']: \n",
    "        print('>>>> Done:',str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    \n",
    "    # Returns results \n",
    "    if (kwargs['return_results'] if 'return_results' in kwargs else False):\n",
    "        return results\n",
    "    \n",
    "    # Returns a log\n",
    "    else:\n",
    "        return  {'SKU': SKU, 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532f938-6661-4734-98cd-5b7aa7d4bf57",
   "metadata": {},
   "source": [
    "**Context Manager**\n",
    "\n",
    "This is a context manager for parellel execution with the purpose of reporting progress. \n",
    "\n",
    "Credits: https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2fc69-9614-41f2-ad27-0b6078b490c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7e458-5bba-4046-bf24-ffc7549a60a2",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e1f45-1946-49c5-a349-b2fa61f84038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder names as global variables\n",
    "os.chdir('/home/fesc/')\n",
    "global PATH_DATA, PATH_PARAMS, PATH_SAMPLES, PATH_RESULTS\n",
    "\n",
    "PATH_DATA = '/home/fesc/MM/Data'\n",
    "PATH_PARAMS  = '/home/fesc/MM/Data/Params'\n",
    "PATH_SAMPLES = '/home/fesc/MM/Data/Samples'\n",
    "PATH_RESULTS = '/home/fesc/MM/Data/Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d2716-7e73-44b0-8638-495396631708",
   "metadata": {},
   "source": [
    "For the models specified, the code below runs the experiment for all given products (SKUs) and over parameter settings (e.g., cost parameters, horizon parameters, etc.). In total, we have 460 products (SKUs) with each 3 different cost parameter settings varying the critical ratio (CR) of holding and backlogging cost being {CR=0.50, CR=0.75, CR=0.90) and each 5 different lengths of the rolling look-ahead horizon tau being {1,2,3,4,5}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de95eb-8805-4058-9ade-cf44df063ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf16ec-85e8-463c-8412-e799ecc98abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd0fb0-24f8-4d8d-898c-708c78596074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d24684-ce5c-4aca-994f-7fa83a6e4d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f0fa2-8eb3-43b1-b4cb-1d91bc87f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfb295-09e1-4ccd-a160-2728f2763b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98c011-52ad-42d2-9913-85ce0d2c8db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6220d9-9f02-48de-b71f-dfb706c1efc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8514c0d-973c-4356-9dd0-1d2beb0f0b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3f1e4-6fb1-4789-8938-8c460062007d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511dade-b55b-40ab-b480-632d4a8d3772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280ad54-a22b-4f76-94d6-3542400ff7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb96850-14f2-4cd0-a08c-0d54018b13c6",
   "metadata": {},
   "source": [
    "## (a) Rolling Horizon Global Weighted SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc2baf-dc67-4cea-a8bd-3c9c9575c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paramaters\n",
    "params = {\n",
    "            \n",
    "    # Sampling strategy\n",
    "    'sampling': 'global',\n",
    "\n",
    "    # Last sale_yearweek of training data\n",
    "    'sale_yearweek': 114,\n",
    "\n",
    "    # Length T of the test horizon\n",
    "    'T': 13,\n",
    "\n",
    "    # Lengths of rolling look-ahead horizons\n",
    "    'tau': [1,2,3,4,5],\n",
    "\n",
    "    # Cost param settings\n",
    "    'cost_params': {\n",
    "\n",
    "        1: {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        2: {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        3: {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "\n",
    "    },\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'gurobi_params': {\n",
    "\n",
    "        'LogToConsole': 1, \n",
    "        'Threads': 1, \n",
    "        'NonConvex': 2, \n",
    "        'PSDTol': 1e-3, # 0.1%\n",
    "        'MIPGap': 1e-3, # 0.1%\n",
    "        'NumericFocus': 0, \n",
    "        'obj_improvement': 1e-3, # 0.1%\n",
    "        'obj_timeout_sec': 3*60, # 3 min\n",
    "        'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    },\n",
    "    \n",
    "    'path': PATH_RESULTS+'/GwSAA',\n",
    "    'model_name': 'GwSAA',\n",
    "    \n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa9fdc-cdc9-45d9-9ec7-ea46b8c65d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "#os.mkdir(params['path'])\n",
    "       \n",
    "# Specify number of cores to use for parallel execution\n",
    "n_jobs = 32\n",
    "\n",
    "# Specify range of products (SKUs) to iterate over\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Run for each product (SKU) in parallel\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKU_range))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiments)(SKU, **params)\n",
    "                                         for SKU in SKU_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9e3ff-505a-4d5c-b8ce-d06c3c434725",
   "metadata": {},
   "source": [
    "## (b) Rolling Horizon Global Robust Weighted SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68ab7b-c496-4866-bc95-bf1393796064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paramaters\n",
    "params = {\n",
    "            \n",
    "    # Sampling strategy\n",
    "    'sampling': 'global',\n",
    "\n",
    "    # Robust uncertainty set threshold multiplier\n",
    "    'e': None,\n",
    "\n",
    "    # Last sale_yearweek of training data\n",
    "    'sale_yearweek': 114,\n",
    "\n",
    "    # Length T of the test horizon\n",
    "    'T': 13,\n",
    "\n",
    "    # Lengths of rolling look-ahead horizons\n",
    "    'tau': [1,2,3,4,5],\n",
    "\n",
    "    # Cost param settings\n",
    "    'cost_params': {\n",
    "\n",
    "        1: {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        2: {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        3: {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "\n",
    "    },\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'gurobi_params': {\n",
    "\n",
    "        'LogToConsole': 1, \n",
    "        'Threads': 1, \n",
    "        'NonConvex': 2, \n",
    "        'PSDTol': 1e-3, # 0.1%\n",
    "        'MIPGap': 1e-3, # 0.1%\n",
    "        'NumericFocus': 3, \n",
    "        'obj_improvement': 1e-3, # 0.1%\n",
    "        'obj_timeout_sec': 3*60, # 3 min\n",
    "        'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    },\n",
    "    \n",
    "    'path': PATH_RESULTS+'/GwSAAR',\n",
    "    'model_name': 'GwSAAR',\n",
    "    \n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3174e9-5624-4d33-a3f8-0e8d9b9239bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "#os.mkdir(params['path'])\n",
    "\n",
    "# Specify number of cores to use for parallel execution\n",
    "n_jobs = 32\n",
    "\n",
    "# Specify range of products (SKUs) to iterate over\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Uncertainty set\n",
    "params['e'] = 1\n",
    "\n",
    "# Run for each product (SKU) in parallel\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKU_range))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiments)(SKU, **params)\n",
    "                                         for SKU in SKU_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660891c5-6ae7-497f-b8e3-d1c7ca4bf334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "#os.mkdir(params['path'])\n",
    "\n",
    "# Specify number of cores to use for parallel execution\n",
    "n_jobs = 32\n",
    "\n",
    "# Specify range of products (SKUs) to iterate over\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Uncertainty set\n",
    "params['e'] = 3\n",
    "\n",
    "# Run for each product (SKU) in parallel\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKU_range))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiments)(SKU, **params)\n",
    "                                         for SKU in SKU_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9ffa8-b71b-42d0-948f-261c2cf43e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "#os.mkdir(params['path'])\n",
    "\n",
    "# Specify number of cores to use for parallel execution\n",
    "n_jobs = 32\n",
    "\n",
    "# Specify range of products (SKUs) to iterate over\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Uncertainty set\n",
    "params['e'] = 6\n",
    "\n",
    "# Run for each product (SKU) in parallel\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKU_range))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiments)(SKU, **params)\n",
    "                                         for SKU in SKU_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d333fd7-2205-41f6-ba02-8b9ad3423dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "#os.mkdir(params['path'])\n",
    "\n",
    "# Specify number of cores to use for parallel execution\n",
    "n_jobs = 32\n",
    "\n",
    "# Specify range of products (SKUs) to iterate over\n",
    "SKU_range = range(1,460+1)\n",
    "\n",
    "# Uncertainty set\n",
    "params['e'] = 12\n",
    "\n",
    "# Run for each product (SKU) in parallel\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKU_range))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(run_experiments)(SKU, **params)\n",
    "                                         for SKU in SKU_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247fb17-e319-4f22-89d0-d501cf7fd57e",
   "metadata": {},
   "source": [
    "## (c) Rolling Horizon Local Weighted SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cd40e-3f31-4165-8e4d-732d4c80fc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b2681-9963-42bc-a4ba-ef81a7617a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aadfac-9be4-46fa-ac39-d7600af7efae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eefca2-a19e-4ca2-863e-9fb4e78ea492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2dff6-c250-4548-bd73-eb86f1759d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d3c93-2990-4d3c-80a1-bb61b72e675f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87da40-1aba-4860-84a1-50bd73e3987e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29a301-92f0-4277-a631-04ec06bbf0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc7ab0-f9d9-4b81-a2d7-84422fa8e921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d3ea2-cc93-43a6-8c89-13e0a6c1ba41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea453d-c9c7-4788-8ac5-ac68336a9fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d698200-1983-4b19-91ba-444707927b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e8d69-7455-4776-8790-20889c536400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd429843-0d10-47ff-b51d-7a7f589cea63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242b746-5f4d-4b7a-a742-331aa3ce7fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e87157c-6ca5-4a32-8167-58ef8157cf63",
   "metadata": {},
   "source": [
    "## (d) Rolling Horizon Local Robust Weighted SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a285dcf-336f-4e50-895e-b40674d97403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c5c2bc-09ac-45b9-beaf-5cf53add027d",
   "metadata": {},
   "source": [
    "## (e) Rolling Horizon SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b590a7-b6d4-4ae0-8959-f6ce47c0d719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ef9b42-8020-4edc-bd0d-90572f4fd299",
   "metadata": {},
   "source": [
    "## (f) Ex-post optimal, deterministic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc32fa-8587-4786-9c27-c03abb1be851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd44032-c175-4372-91b1-2af794f7cea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bec223-6f24-4171-8d1b-c404b872182a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c712f6-467c-4ede-8136-6e8caabac31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e18e40-563b-47ea-a061-c193cdc124cc",
   "metadata": {},
   "source": [
    "# Aggregate all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56455c70-3bf0-418c-be2d-cd054a8173a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a70779-bbd4-41fe-805d-20a51e1536d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c87923-e2e1-4da5-bf9f-366e20184ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a9898-a7d8-43e2-aded-0a5102f9d40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a48de9-f65f-4db5-950c-0be1ed49d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9daf8-66fe-46ff-bbe5-daa14d847001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a8a38-1144-49c5-84e8-e4a11c6bebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiPeriodInv",
   "language": "python",
   "name": "multiperiodinv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
