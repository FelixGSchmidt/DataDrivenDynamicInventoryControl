{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3d67cd-e230-495a-a031-1e5a7ebc4dd7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b518e25-3378-4cde-9fca-4ea5cccd902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Weights Model\n",
    "from WeightsModel import PreProcessing\n",
    "from WeightsModel import MaxQFeatureScaler\n",
    "from WeightsModel import MaxQDemandScaler\n",
    "from WeightsModel import RandomForestWeightsModel\n",
    "\n",
    "# Import Weighted SAA Model\n",
    "from WeightedSAA import WeightedSAA\n",
    "from WeightedSAA import RobustWeightedSAA\n",
    "from WeightedSAA import RollingHorizonOptimization\n",
    "\n",
    "# Import Experiment\n",
    "from Experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8cb12-853b-457f-91ff-8e41687c3862",
   "metadata": {},
   "source": [
    "# Setup the experiment\n",
    "\n",
    "**Experiment data** consists of four data sets.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ of product $j$ at time $n$\n",
    "- **Y_Data** (pd.DataFrame) is the demand time series data, i.e., each row is a demand observations $d_{j,n}$ of product $j$ at time $n$\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets\n",
    "\n",
    "Data is loaded before each experiment using **load_data()**.\n",
    "\n",
    "We run an experiment for all given products (SKUs) $k=1,...,M$ over a test planning horizon $t=1,...,T$ with $T=13$ for three different cost parameter settings $\\{K, u, h, b\\}$ that vary the critical ratio ($CR=\\frac{b}{b+h}$) of holding and backlogging yielding\n",
    "- $CR=0.50$: $\\{K=100, u=0.5, h=1, b=1\\}$\n",
    "- $CR=0.75$: $\\{K=100, u=0.5, h=1, b=3\\}$\n",
    "- $CR=0.90$: $\\{K=100, u=0.5, h=1, b=9\\}$\n",
    "\n",
    "Experiments are run for different choices of the look-ahead $\\tau=0,...,4$. For robust models, we vary the parameter $e=\\{1,3,6,9,12\\}$ (multiplier of the in-sample standard deviation of demand) defining product and sample sepcific uncertainty sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0bd36-7aed-491c-a705-cd2c4812a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the experiment\n",
    "experiment_setup = dict(\n",
    "\n",
    "    # Set paths\n",
    "    path_data = '/home/fesc/DDDInventoryControl/Data',\n",
    "    path_weightsmodel = '/home/fesc/DDDInventoryControl/Data/WeightsModel/OutOfSample',\n",
    "    path_results = '/home/fesc/DDDInventoryControl/Data/Results/OutOfSample',\n",
    "    \n",
    "    # Weights models\n",
    "    global_weightsmodel = 'rfwm_global', \n",
    "    local_weightsmodel = 'rfwm_local', \n",
    "\n",
    "    # Optimization models\n",
    "    GwSAA = 'GwSAA',\n",
    "    GwSAAR = 'GwSAAR',\n",
    "    wSAA = 'wSAA',\n",
    "    wSAAR = 'wSAAR',\n",
    "    SAA = 'SAA',\n",
    "    ExPost = 'ExPost',\n",
    "    \n",
    "    # Set identifier of start period of testing horizon\n",
    "    timePeriodsTestStart = 114,\n",
    "\n",
    "    # Set product identifiers\n",
    "    products = range(1,460+1),   # Products (SKUs) k=1,...,M\n",
    "    \n",
    "    # Set problem params\n",
    "    T = 13,             # Planning horizon T\n",
    "    ts = range(1,13+1), # Periods t=1,...,T of the planning horizon\n",
    "    taus = [0,1,2,3,4], # Look-aheads tau=0,...,4\n",
    "    #es = [1,3,6,9,12],  # Uncertainty set specifications e=1,...,12\n",
    "    es = [0.33, 0.67, 1.00, 1.33, 1.67, 2.00, 2.33, 2.67, 3.00], # Uncertainty set specifications e\n",
    "    \n",
    "    maxnorm = True, # Use max norm to specify uncertainty set \n",
    "       \n",
    "    # Set cost params\n",
    "    cost_params = [\n",
    "        {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "        {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "        {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "    ],\n",
    "    \n",
    "    # Set gurobi params\n",
    "    gurobi_params = {\n",
    "    \n",
    "        'LogToConsole': 0, \n",
    "        'Threads': 1, \n",
    "        'NonConvex': 2, \n",
    "        'PSDTol': 1e-3, # 0.1%\n",
    "        'MIPGap': 1e-3, # 0.1%\n",
    "        'NumericFocus': 0, \n",
    "        'obj_improvement': 1e-3, # 0.1%\n",
    "        'obj_timeout_sec': 3*60, # 3 min\n",
    "        'obj_timeout_max_sec': 10*60, # 10 min\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Global weights model params\n",
    "    global_weightsmodel_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'oob_score': True,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 4,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'n_estimators': [500, 1000],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [x for x in range(20, 100, 20)],  \n",
    "            'min_samples_leaf': [x for x in range(10, 100, 10)],  \n",
    "            'max_features': [x for x in range(8, 136, 8)],   \n",
    "            'max_leaf_nodes': [None],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "        },    \n",
    "\n",
    "        # Tuning params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 100,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 8,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "        # Tuning using random search\n",
    "        random_search = True\n",
    "    ),\n",
    "    \n",
    "    # Local weights model params\n",
    "    local_weightsmodel_params = dict(\n",
    "    \n",
    "        # Meta parameters\n",
    "        model_params = {\n",
    "            'oob_score': True,\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 1,\n",
    "            'verbose': 0\n",
    "        },\n",
    "\n",
    "        # Hyper params search grid\n",
    "        hyper_params_grid = {\n",
    "            'n_estimators': [25, 50],\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [x for x in range(2, 20, 2)],  \n",
    "            'min_samples_leaf': [x for x in range(2, 10, 2)],  \n",
    "            'max_features': [x for x in range(4, 96, 4)],   \n",
    "            'max_leaf_nodes': [None],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "        },    \n",
    "\n",
    "        # Tuning params\n",
    "        tuning_params = {     \n",
    "            'n_iter': 100,\n",
    "            'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "            'return_train_score': True,\n",
    "            'refit': 'MSE',\n",
    "            'random_state': 12345,\n",
    "            'n_jobs': 32,\n",
    "            'verbose': 0\n",
    "        },    \n",
    "\n",
    "        # Tuning using random search\n",
    "        random_search = True\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)\n",
    "\n",
    "# Initialize Experiment\n",
    "experiment = Experiment(**experiment_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137c0ad-6243-4390-bd1c-6aff2a2fca74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Training and Samping\n",
    "\n",
    "The two global models (using 'Global Training and Sampling') are **Rolling Horizon Global Weighted SAA (GwSAA)**, which is our model, and **Rolling Horizon Global Robust Weighted SAA (GwSAA-R)**, which is the analogous model with robust extension. Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'global' distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$.\n",
    "\n",
    "We first load **global** experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline. Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly. Furthermore, features and demands are scaled for training (and later rescaled for sampling). \n",
    "\n",
    "The weights models - and thus the data used, weight functions, and weights per sample - are the same for the two global models **GwSAA** and **GwSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor). Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028da40-43c6-499d-a378-e9d5de6e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, ids_products, ids_timePeriods, features, features_to_scale, features_to_scale_with = experiment.load_data('global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293db730-90be-46a1-a50f-ecacd8b2dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data for out of sample horizon\n",
    "inSample = ids_timePeriods < timePeriodsTestStart + T\n",
    "X, y = X[inSample], y[inSample]\n",
    "ids_products, ids_timePeriods = ids_products[inSample], ids_timePeriods[inSample] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed38d5-839e-4f71-b875-b524656c3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module of the weights model\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7822ac-eb99-4ca4-b702-593f98563c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:  \n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "    # Preprocess data for global models\n",
    "    data = pp.preprocess_weightsmodel_data(X, y, ids_products, ids_timePeriods, timePeriodsTestStart, tau, T, \n",
    "                                           features, features_to_scale, features_to_scale_with, \n",
    "                                           X_scaler=MaxQFeatureScaler(q_outlier=0.975), \n",
    "                                           y_scaler=MaxQDemandScaler(q_outlier=0.975))\n",
    "\n",
    "    # Save\n",
    "    _ = dump(data, path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bcee6-737b-4ac0-b412-837607c2edfb",
   "metadata": {},
   "source": [
    "## Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the global random forest weights model, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b34c5a-0164-4e8e-a20e-78e2d07bb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meta params for tuning global weights model available\n",
    "locals().update(experiment_setup['global_weightsmodel_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9828a1-0293-4bdb-949d-fb5440c0b446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "       \n",
    "    # Using t=1 (i.e, data available before start of testing)\n",
    "    t=1\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Extract preprocessed data\n",
    "    locals().update(data[t])\n",
    "    \n",
    "    # Initialize\n",
    "    pp = PreProcessing()\n",
    "    wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "    # Scale features and demands\n",
    "    X_train = pp.scale(X_train, X_scalers, ids_products_train)\n",
    "    y_train = pp.scale(y_train, y_scalers, ids_products_train)   \n",
    "\n",
    "    # CV time series splits\n",
    "    cv_folds = pp.split_timeseries_cv(n_splits=3, ids_timePeriods=ids_timePeriods_train)\n",
    "    \n",
    "    # CV search\n",
    "    cv_results = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status=True)\n",
    "    wm.save_cv_result(path=path_weightsmodel+'/'+global_weightsmodel+'_cv_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fff30-5122-4776-a507-85b29f06ecae",
   "metadata": {},
   "source": [
    "## Fit weight functions and generate weights\n",
    "\n",
    "We now fit the global random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$ and over periods $t=1,...,T$. This is done across all products at once (global training). Then, for each $\\tau=0,...,4$ and over periods $t=1,...,T$, we generate for each product (SKU) $k=1,...,M$ the weights given the test feature $x_{k,t}$. This is done *jointly* across products (by using $x_{t}=(x_{1,t},...,x_{M,t})^{\\top}$) for computational efficiency - the weights for each individual product can be extracted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac397b5-6cd1-4b43-82c6-92cc48459b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b67c0f-7c50-4b19-a292-9fcdfc8de6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "\n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "        \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "        \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "        \n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[t])\n",
    "            \n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # Scale features and demands\n",
    "        X_train, X_test = pp.scale(X_train, X_scalers, ids_products_train), pp.scale(X_test, X_scalers, ids_products_test)\n",
    "        y_train, y_test = pp.scale(y_train, y_scalers, ids_products_train), pp.scale(y_test, y_scalers, ids_products_test)           \n",
    "            \n",
    "        # Load tuned weights model\n",
    "        wm.load_cv_result(path=path_weightsmodel+'/'+global_weightsmodel+'_cv_tau'+str(tau_)+'.joblib')\n",
    "        \n",
    "        # Fit weight functions  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        wm.fit(X_train, y_train)\n",
    "        weightfunctions_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "      \n",
    "        # Generate weights  \n",
    "        st_exec, st_cpu = time.time(), time.process_time() \n",
    "        weights[t] = wm.apply(X_train, X_test)\n",
    "        weights_times[t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "        \n",
    "    # Status\n",
    "    print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+global_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+global_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d357c25-392d-4686-be11-9fd8a580f007",
   "metadata": {},
   "source": [
    "# Local Training and Sampling\n",
    "\n",
    "The two local models (using 'Local Training and Sampling') are **Rolling Horizon Local Weighted SAA (wSAA)**, and **Rolling Horizon Local Robust Weighted SAA (wSAA-R)**, which is the analogous model with robust extension. Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'local' distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$ for each product $k=1,...,M$ separately.\n",
    "\n",
    "We first load **local** experiment data and then preprocess the data for all look-aheads $\\tau=1,...,4$ and periods $t=1,...,T$ upfront. With this, we can later easily load and reuse the data which is needed for several steps along the experiment pipeline. Preprocessing includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences and mapping corresponding features accordingly.\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two local models **wSAA** and **wSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor) and for each product (SKU) $k=1,...,M$ separately. Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) for each product (SKU) $k=1,...,M$ separately and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$ separatey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a7aac-8290-4df0-b5ed-162e92009e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack experiment data\n",
    "X, y, ids_products, ids_timePeriods, features = experiment.load_data('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e0b0-5096-4c39-afb3-5752dcc680e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data for out of sample horizon\n",
    "inSample = ids_timePeriods < timePeriodsTestStart + T\n",
    "X, y = X[inSample], y[inSample]\n",
    "ids_products, ids_timePeriods = ids_products[inSample], ids_timePeriods[inSample] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e7a4-9bee-40f2-ad65-084f2b229d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing module of the weights model\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc86c4d-52ea-4c13-ad2a-3a21be3f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:  \n",
    "    \n",
    "    # Status\n",
    "    print('#### Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "    # Preprocess data for local models\n",
    "    data = pp.preprocess_weightsmodel_data(X, y, ids_products, ids_timePeriods, timePeriodsTestStart, tau, T, products=products)\n",
    "\n",
    "    # Save\n",
    "    _ = dump(data, path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0442f-cdaa-47ad-a691-4b2aac018b30",
   "metadata": {},
   "source": [
    "## Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the local random forest weights model for each product (SKU) $k=1,...,M$, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe32a6c-8a55-4342-9017-32c908399737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make meta params for tuning global weights model available\n",
    "locals().update(experiment_setup['local_weightsmodel_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710edda0-23b8-4e8e-8c19-7163d9456b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "        \n",
    "    # Initialize\n",
    "    cv_results = {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for product in products:\n",
    "\n",
    "        # Using t=1 (i.e, data available before start of testing)\n",
    "        t=1\n",
    "\n",
    "        # Extract preprocessed data\n",
    "        locals().update(data[product][t])\n",
    "\n",
    "        # Initialize\n",
    "        pp = PreProcessing()\n",
    "        wm = RandomForestWeightsModel(model_params)\n",
    "        \n",
    "        # CV time series splits\n",
    "        cv_folds = pp.split_timeseries_cv(n_splits=3, ids_timePeriods=ids_timePeriods_train)\n",
    "        \n",
    "        # CV search\n",
    "        cv_results[product] = wm.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status=False)\n",
    "\n",
    "        # Status\n",
    "        print('Product '+str(product)+' of '+str(len(products))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "\n",
    "    # Save\n",
    "    _ = dump(cv_results, path_weightsmodel+'/'+local_weightsmodel+'_cv_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1c7a-4494-4191-858a-231dc1ba9fd0",
   "metadata": {},
   "source": [
    "## Fit weight functions and generate weights\n",
    "\n",
    "We now fit a local random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately (local training). Then, for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately, we generate the weights given the test feature $x_{k,t}$. This is done *separately* for each product (SKU) $k=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410060e-ec5e-4f5d-8da2-7a81da076ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meta params\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c21564-d205-4df7-a645-3cb408b54692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    weights, weightfunctions_times, weights_times = {}, {}, {}\n",
    "    \n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here) \n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for product in products:\n",
    "        \n",
    "        # Initialize\n",
    "        weights[product], weightfunctions_times[product], weights_times[product] = {}, {}, {}\n",
    "    \n",
    "        # For each period t=1,...,T\n",
    "        for t in ts:\n",
    "\n",
    "            # Extract preprocessed data\n",
    "            locals().update(data[product][t])\n",
    "                    \n",
    "            # Adjust look-ahead tau to account for end of horizon\n",
    "            tau_ = min(tau,T-t)\n",
    "            \n",
    "            # Initialize\n",
    "            pp = PreProcessing()\n",
    "            wm = RandomForestWeightsModel(model_params)\n",
    "\n",
    "            # Load tuned weights model\n",
    "            wm.load_cv_result(path=path_weightsmodel+'/'+local_weightsmodel+'_cv_tau'+str(tau_)+'.joblib', product=product)\n",
    "\n",
    "            \n",
    "            # Fit weight functions  \n",
    "            st_exec, st_cpu = time.time(), time.process_time() \n",
    "            wm.fit(X_train, y_train)\n",
    "            weightfunctions_times[product][t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "            # Generate weights  \n",
    "            st_exec, st_cpu = time.time(), time.process_time() \n",
    "            weights[product][t] = wm.apply(X_train, X_test)\n",
    "            weights_times[product][t] = {'exec_time_sec': time.time()-st_exec, 'cpu_time_sec': time.process_time()-st_cpu}\n",
    "\n",
    "        # Status\n",
    "        print('Product '+str(product)+' of '+str(len(products))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True) \n",
    "        \n",
    "    # Save\n",
    "    _ = dump(weights, path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib')   \n",
    "    _ = dump(weightfunctions_times, path_weightsmodel+'/'+local_weightsmodel+'_weightfunctions_times_tau'+str(tau)+'.joblib') \n",
    "    _ = dump(weights_times, path_weightsmodel+'/'+local_weightsmodel+'_weights_times_tau'+str(tau)+'.joblib')    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9986-993c-4b9a-96e5-22444655a258",
   "metadata": {},
   "source": [
    "# Rolling Horizon Optimization\n",
    "\n",
    "The rolling horizon optimization approach is best illustrated for the application to GwSAA below. Analogously, this approach is applied for the other models, where the data used for training and sampling, fitting the weights model, and the optimization the model solved, are adjusted accordingly.\n",
    "\n",
    "**Input:** look-ahead $\\tau$, cost parameters $\\{K_{k,t},u_{k,t},h_{k,t},b_{k,t}\\}_{t=1}^{T}$ for products $k=1,....M$\n",
    "\n",
    "**For** $t=1,...,T$\n",
    "    \n",
    "* $\\tau \\gets \\min\\{T-t,\\tau\\}$     Truncate look-ahead $\\tau$ given remaining planning horizon $T-t$\n",
    " \n",
    "* **w.fit**     Train weight functions $w_{j,t,\\tau}^{\\,i}(\\,\\cdot\\,)$ on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$\n",
    "    \n",
    "* **For** $k=1,...,M$ \n",
    "    \n",
    "    * $x_{k,t} \\gets X_{k,t}$     Observe new realization $x_{k,t}$ of the feature vector $X_{k,t}$ for product $k$\n",
    "        \n",
    "    * **w.predict**     Apply $x_{k,t}$ to weight functions $w_{j,t,\\tau}^{\\,i}(\\,\\cdot\\,)$ to retrieve $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$\n",
    "    \n",
    "    * Get decision $\\hat{q}_{k,t}(I_{k,t},x_{k,t})$ by solving **GwSAA**\n",
    "        \\begin{equation*}\n",
    "            \\hat{q}_{k,t}(I_{k,t},x_{k,t}) \\in \\arg\\min_{\\substack{q_{t} \\in Q_{t} \\\\ ... \\\\ q_{t+\\tau} \\in Q_{t+\\tau}}}\\sum_{j=1}^{M}\\sum_{i=1}^{N_{j,t,\\tau}}w_{j,t,\\tau}^{\\,i}(x_{k,t})c_{k,t,\\tau}(q_{t},...,q_{t+\\tau},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i},I_{k,t})\n",
    "        \\end{equation*}\n",
    "        \n",
    "    * $I_{k,t+1} \\gets I_{k,t}+\\hat{q}_{k,t}(I_{k,t},x_{k,t})-d_{k,t}$     Apply decision $\\hat{q}_{k,t}(I_{k,t},x_{k,t})$, observe $d_{k,t} \\gets D_{k,t}$\n",
    "    \n",
    "    **EndFor**\n",
    "**EndFor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b5a1c-c253-41b3-be40-866598943f7e",
   "metadata": {},
   "source": [
    "## (a) Rolling Horizon Global Weighted SAA (GwSAA)\n",
    "\n",
    "Weighted SAA over \"global\" distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{j,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$. Data is scaled per product for training and rescaled with scaler of the current product applied to all products for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736f80d-524b-4165-8993-b59070a1a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea232ec-5727-401f-a60c-37e82bad7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f251620-54eb-412b-9cb3-2e7772622e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+GwSAA): os.mkdir(path_results+'/'+GwSAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(global_weightsmodel, GwSAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), \n",
    "                                                                     weights=weights_[product], samples=samples_[product], \n",
    "                                                                     actuals=actuals_[product]) for product in products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f990e6a-3366-45d5-8ccf-bdc88876c5fc",
   "metadata": {},
   "source": [
    "## (b) Rolling Horizon Global Robust Weighted SAA (GwSAA-R)\n",
    "\n",
    "Weighted SAA as for GwSAA and minimizing worst case cost over product-specific \"global\" uncertainty sets $\\boldsymbol{\\mathcal{U}}_{j,t,\\tau}^{\\,i}(k)=\\{\\boldsymbol{\\tilde{d}}\\in \\mathcal{D}_{k,t} \\times ... \\times \\mathcal{D}_{k,t+\\tau}: ||\\boldsymbol{\\tilde{d}}-\\boldsymbol{d}_{j}^{\\,i}|| \\leq \\epsilon_{k}\\}$, with $\\boldsymbol{\\tilde{d}}=(\\tilde{d}_{t},...,\\tilde{d}_{t+\\tau})^\\top$ and $\\boldsymbol{d}_{j}^{\\,i}=(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})^\\top$, for each pair $(j,i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22733a9b-4016-41d9-8449-087af93b3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5e177-550f-4940-b097-dcaf8819fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de89b1-5104-403d-9fe7-7cad8690c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+GwSAAR): os.mkdir(path_results+'/'+GwSAAR)\n",
    "\n",
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "        \n",
    "        # Initialize Experiment\n",
    "        experiment = Experiment(global_weightsmodel, GwSAAR, **experiment_setup)\n",
    "\n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/global_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "        # Load weights\n",
    "        weights = load(path_weightsmodel+'/'+global_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Preprocess experiment data\n",
    "        weights_, samples_, actuals_, epsilons_ = experiment.preprocess_data(data, weights, e, maxnorm=maxnorm)\n",
    "        \n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, e=e, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         weights=weights_[product], samples=samples_[product], \n",
    "                                                                         actuals=actuals_[product], epsilons=epsilons_[product]) \n",
    "                                                 for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96a45e-8a1e-4675-abb6-60edd81932e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327a9f4-0207-4e81-b682-83fefe85935e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177c4b64-a7ff-4b64-8e95-ddcd851acca5",
   "metadata": {},
   "source": [
    "## (c) Rolling Horizon Local Weighted SAA (wSAA)\n",
    "\n",
    "Weighted SAA over \"local\" distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained (for each product) on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6bda0-2542-436d-a100-79b78d590893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e508dfe-6860-48f0-8747-e7a7242092c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2309b-bd18-4044-ba60-0c619372421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+wSAA): os.mkdir(path_results+'/'+wSAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(local_weightsmodel, wSAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    # Load weights\n",
    "    weights = load(path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "    \n",
    "    # Preprocess experiment data\n",
    "    weights_, samples_, actuals_ = experiment.preprocess_data(data, weights)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), \n",
    "                                                                     weights=weights_[product], samples=samples_[product], \n",
    "                                                                     actuals=actuals_[product]) for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d2e6d-cbc8-4156-9f6d-8db11f19889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a3ff83-6de0-4fad-8b99-a50eb9f3dd11",
   "metadata": {},
   "source": [
    "## (d) Rolling Horizon Local Robust Weighted SAA (wSAA-R)\n",
    "\n",
    "Weighted SAA as for wSAA and minimizing worst case cost over product-specific \"local\" uncertainty sets $\\boldsymbol{\\mathcal{U}}_{k,t,\\tau}^{\\,i}=\\{\\boldsymbol{\\tilde{d}}\\in \\mathcal{D}_{k,t} \\times ... \\times \\mathcal{D}_{k,t+\\tau}: ||\\boldsymbol{\\tilde{d}}-\\boldsymbol{d}_{k}^{\\,i}|| \\leq \\epsilon_{k}\\}$, with $\\boldsymbol{\\tilde{d}}=(\\tilde{d}_{t},...,\\tilde{d}_{t+\\tau})^\\top$ and $\\boldsymbol{d}_{k}^{\\,i}=(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})^\\top$, for each pair $(j,i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd9d8d-cc36-4fa8-b8b8-f0f4a6f5039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce133eb4-1175-4171-8eda-9952a2ab91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3ad4d-7fd9-4c5f-9009-913759315e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+wSAAR): os.mkdir(path_results+'/'+wSAAR)\n",
    "\n",
    "# For each uncertainty set specification\n",
    "for e in es:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "        \n",
    "        # Initialize Experiment\n",
    "        experiment = Experiment(local_weightsmodel, wSAAR, **experiment_setup)\n",
    "\n",
    "        # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "        data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "        # Load weights\n",
    "        weights = load(path_weightsmodel+'/'+local_weightsmodel+'_weights_tau'+str(tau)+'.joblib') \n",
    "\n",
    "        # Preprocess experiment data\n",
    "        weights_, samples_, actuals_, epsilons_ = experiment.preprocess_data(data, weights, e, maxnorm=maxnorm)\n",
    "        \n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, e=e, wsaamodel=RobustWeightedSAA(), \n",
    "                                                                         weights=weights_[product], samples=samples_[product], \n",
    "                                                                         actuals=actuals_[product], epsilons=epsilons_[product]) \n",
    "                                                 for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356cc89-3dc4-45e1-ab7f-7bcf8c64274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650cf8f6-4fd6-4258-9498-759b49e4cbc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (e) Baseline model: Rolling Horizon Local Weighted SAA (SAA)\n",
    "\n",
    "SAA over the distribution $\\{\\frac{1}{N_{k,t,\\tau}},(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca7166-36c7-483f-a9c0-f517b4f377c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ab893-5528-47c7-ad40-386a703f94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c23da1-f4a1-4b59-81ea-49c512307d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+SAA): os.mkdir(path_results+'/'+SAA)\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Initialize Experiment\n",
    "    experiment = Experiment(local_weightsmodel, SAA, **experiment_setup)\n",
    "\n",
    "    # Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "    data = load(path_weightsmodel+'/local_data_tau'+str(tau)+'.joblib')\n",
    "\n",
    "    # Preprocess experiment data\n",
    "    samples_, actuals_ = experiment.preprocess_data(data)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(tau=tau, product=product, wsaamodel=WeightedSAA(), \n",
    "                                                                     samples=samples_[product], actuals=actuals_[product]) \n",
    "                                             for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3ac71-3ae7-4a72-a56f-433ec92bfb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280f481-c0e0-4e07-86bc-c7e2b45fa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b4a736-d792-468a-86a1-8c95976293ed",
   "metadata": {},
   "source": [
    "## (f) Ex-post optimal model with deterministic demand\n",
    "\n",
    "Claircoyant optimal model where demand is known upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430def9f-db7e-455e-8f30-d17c836acb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores\n",
    "n_jobs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e64087-863d-4172-9ce7-1b2f234f651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all experiment variables visible locally\n",
    "locals().update(experiment_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834f8da-ddce-4e34-a632-a87681a2079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(path_results+'/'+ExPost): os.mkdir(path_results+'/'+ExPost)\n",
    "\n",
    "\n",
    "# Initialize Experiment\n",
    "experiment = Experiment(local_weightsmodel, ExPost, **experiment_setup)\n",
    "\n",
    "# Load preprocessed data (alternatively, data can be preprocessed here)\n",
    "data = load(path_weightsmodel+'/local_data_tau0.joblib')\n",
    "\n",
    "# Preprocess experiment data\n",
    "_, actuals = experiment.preprocess_data(data)\n",
    "\n",
    "# For ex-post clairvoyant, we can directly solve over the full horizon\n",
    "actuals_ = {}\n",
    "for product in products:\n",
    "    d = []\n",
    "    for t in ts:\n",
    "        d = d + [actuals[product][t].item()]\n",
    "    actuals_[product] = np.array(d).reshape(1,len(d))\n",
    "\n",
    "# For each product (SKU) k=1,...,M\n",
    "with experiment.tqdm_joblib(tqdm(desc='Progress', total=len(products))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=n_jobs)(delayed(experiment.run)(product=product, wsaamodel=WeightedSAA(), actuals=actuals_[product]) \n",
    "                                         for product in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fde765-4351-4417-8e92-9615f138712a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a024b-e382-45e9-a686-474e7d263560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDDInventoryControl",
   "language": "python",
   "name": "dddinventorycontrol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
