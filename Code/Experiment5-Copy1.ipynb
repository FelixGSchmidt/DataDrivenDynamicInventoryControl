{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa32c966-eb6a-49ab-8484-fdd0b7e22e0a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b518e25-3378-4cde-9fca-4ea5cccd902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "import os\n",
    "import itertools\n",
    "import contextlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import Weights Model\n",
    "import WeightsModel3\n",
    "from WeightsModel3 import PreProcessing\n",
    "from WeightsModel3 import RandomForestWeightsModel\n",
    "from WeightsModel3 import RollingHorizonGlobalTrainingAndSampling\n",
    "\n",
    "# Import (Rolling Horizon) Weighted SAA models\n",
    "from WeightedSAA3 import WeightedSAA\n",
    "from WeightedSAA3 import RobustWeightedSAA\n",
    "from WeightedSAA3 import RobustWeightedSAA2\n",
    "from WeightedSAA3 import RollingHorizonOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d60c56-dbb7-42e2-a8b7-f478cb4dc818",
   "metadata": {},
   "source": [
    "# General paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "331e5ab6-1320-4122-be2b-84b7bb7f89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folder names as global variables\n",
    "os.chdir('/home/fesc/MM/')\n",
    "global PATH_DATA, PATH_PARAMS, PATH_KERNELS, PATH_SAMPLES, PATH_RESULTS\n",
    "\n",
    "PATH_DATA = '/home/fesc/MM/Data'\n",
    "PATH_PARAMS  = '/home/fesc/MM/Data/Params'\n",
    "PATH_WEIGHTSMODEL = '/home/fesc/MM/Data/WeightsModel'\n",
    "PATH_SAMPLES = '/home/fesc/MM/Data/Samples'\n",
    "PATH_RESULTS = '/home/fesc/MM/Data/Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6948b9d8-5514-4c0c-b4fa-8f5e4bd9e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period and SKU ranges\n",
    "T = 13                  # Planning horizon T\n",
    "ts = range(1,13+1)      # Periods t=1,...,T of the planning horizon\n",
    "taus = range(0,4+1)     # Look-aheads tau=0,...,4 to use\n",
    "SKUs = range(1,460+1)   # Products (SKUs) k=1,...,M\n",
    "es = [1,3,6,9,12]\n",
    "# Train/test split (first timePeriods of testing horizon)\n",
    "test_start = 114\n",
    "\n",
    "# Cost param settings\n",
    "cost_params = [\n",
    "\n",
    "    {'CR': 0.50, 'K': 100, 'u': 0.5, 'h': 1, 'b': 1},\n",
    "    {'CR': 0.75, 'K': 100, 'u': 0.5, 'h': 1, 'b': 3},\n",
    "    {'CR': 0.90, 'K': 100, 'u': 0.5, 'h': 1, 'b': 9}\n",
    "\n",
    "]\n",
    "\n",
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618b512-5f0f-4c54-811f-c2ee523d5a17",
   "metadata": {},
   "source": [
    "# Training and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137c0ad-6243-4390-bd1c-6aff2a2fca74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Global Training and Samping\n",
    "\n",
    "The two global models (using 'Global Training and Sampling') are **Rolling Horizon Global Weighted SAA (GwSAA)**, which is our model, and **Rolling Horizon Global Robust Weighted SAA (GwSAA-R)**, which is the analogous model with robust extension.\n",
    "\n",
    "Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'global' distribution $\\{\\{w_{j,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$, with weight functions $w_{j,t,\\tau}(\\,\\cdot\\,)$ trained (once for all products) on data $S_{t,\\tau}^{\\,\\text{Global}}=\\{\\{(x_{j,t}^{\\,i},d_{j,t}^{\\,i},...,d_{j,t+\\tau}^{\\,i})\\}_{i=1}^{N_{j,t,\\tau}}\\}_{j=1}^{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6c9b9-f722-44f1-8f73-dd8c964a2bbf",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "We first load and pre-process the data. This includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ where n is the number of training observations (rows) in the data\n",
    "- **Y_Data** (pd.DataFrame) is the demand data $d_{j,n}$ (a times series per product)\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "807bbbf8-251d-4807-8915-3d90955a53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_global_z'\n",
    "weightsmodel_name = 'rfwm_global_z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3cd9a08d-e0c8-4d6d-b26f-44eef6d7633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns2.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ec22262b-07c2-4fc0-b32f-518b0163ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_Data_Columns = X_Data_Columns.loc[X_Data_Columns.Global == 'YES']\n",
    "X_Data = X_Data[X_Data_Columns.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "53591cfc-0c82-4c57-b808-e5cb7a5346ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "X_Data_train = X_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "\n",
    "# Prepare\n",
    "vars_to_scale_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'Feature'].values\n",
    "vars_to_scale_with_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'ScaleWith'].values\n",
    "\n",
    "vars_to_scale = np.array(X_Data[vars_to_scale_names])\n",
    "vars_to_scale_with = np.array(X_Data_train[vars_to_scale_with_names])\n",
    "\n",
    "vars_to_scale_groups = np.array(ID_Data.SKU)\n",
    "vars_to_scale_with_groups = np.array(ID_Data_train.SKU)\n",
    "\n",
    "# Fit and transform\n",
    "scaler = MinMaxScaler()\n",
    "vars_scaled, scaler_fitted = pp.scale_variables(vars_to_scale, vars_to_scale_with, vars_to_scale_groups, vars_to_scale_with_groups, scaler)\n",
    "\n",
    "# Save fitted feature scaler\n",
    "_ = joblib.dump(scaler_fitted, PATH_RESULTS+'/'+weightsmodel_name+'_feature_scaler.joblib')\n",
    "\n",
    "# Reshape to original data\n",
    "vars_scaled = pd.concat([pd.DataFrame(vars_scaled[i], columns=vars_to_scale_names) for i in vars_scaled]).reset_index(drop=True)\n",
    "X_Data_z = copy.deepcopy(X_Data)\n",
    "for col in vars_scaled.columns:\n",
    "    X_Data_z[col] = vars_scaled[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f4fbbf11-dc0a-4833-9873-b414f5720900",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dcb17317-d7f2-474b-b8bd-3cae76f87a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "Y_Data_train = Y_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "\n",
    "# Prepare\n",
    "vars_to_scale = np.array(Y_Data)\n",
    "vars_to_scale_with = np.array(Y_Data_train)\n",
    "\n",
    "vars_to_scale_groups = np.array(ID_Data.SKU)\n",
    "vars_to_scale_with_groups = np.array(ID_Data_train.SKU)\n",
    "\n",
    "# Fit and transform\n",
    "scaler = MinMaxScaler()\n",
    "vars_scaled, scaler_fitted = pp.scale_variables(vars_to_scale, vars_to_scale_with, vars_to_scale_groups, vars_to_scale_with_groups, scaler)\n",
    "\n",
    "# Save fitted feature scaler\n",
    "_ = joblib.dump(scaler_fitted, PATH_RESULTS+'/'+weightsmodel_name+'_demand_scaler.joblib')\n",
    "\n",
    "# Reshape to original data\n",
    "Y_Data_z = pd.concat([pd.DataFrame(vars_scaled[i], columns=['Y']) for i in vars_scaled]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8601d502-a1bf-42c0-8c45-efa8ad4ed6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3974b08c-ddfd-4dbf-8e32-79e85b235366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-period demand vectors\n",
    "data = pd.concat([ID_Data, Y_Data_z], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data_z = pd.DataFrame(Y)\n",
    "\n",
    "data = pd.concat([ID_Data, Y_Data], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd893890-6849-48fe-8568-5887a176f1c8",
   "metadata": {},
   "source": [
    "### Weights model\n",
    "\n",
    "The weights models - and thus the data used, weight functions, and weights per sample - are the same for the two global models **GwSAA** and **GwSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor). Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bcee6-737b-4ac0-b412-837607c2edfb",
   "metadata": {},
   "source": [
    "#### Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the global random forest weights model, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b187aa7-4edd-4288-88ea-fe5a6ca96a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(20, 1000, 20)],  \n",
    "    'min_samples_leaf': [x for x in range(10, 1000, 10)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 8,\n",
    "    'verbose': 2\n",
    "}    \n",
    "\n",
    "random_search = True\n",
    "print_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9828a1-0293-4bdb-949d-fb5440c0b446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Initialize preprocessing module\n",
    "    pp = PreProcessing()\n",
    "        \n",
    "    # Select and reshape training and test data\n",
    "    args = {'train': (ID_Data.sale_yearweek < test_start), \n",
    "            'timePeriods': ID_Data.loc[(ID_Data.sale_yearweek < test_start)].sale_yearweek, \n",
    "            'maxTimePeriod': test_start-1, 'tau': tau}\n",
    "    \n",
    "    id_train = pp.train_test_split(ID_Data, **args)\n",
    "    X_train = pp.train_test_split(X_Data_z, **args, to_array=True)\n",
    "    y_train = pp.train_test_split(Y_Data_z, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)\n",
    "   \n",
    "    # Initialize\n",
    "    weightsmodel = RandomForestWeightsModel(model_params)\n",
    "\n",
    "    # CV search\n",
    "    cv_folds = pp.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "    cv_results = weightsmodel.tune(X_train, y_train, cv_folds, hyper_params_grid, tuning_params, random_search, print_status)\n",
    "    weightsmodel.save_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fff30-5122-4776-a507-85b29f06ecae",
   "metadata": {},
   "source": [
    "#### Fit weight functions and generate weights\n",
    "\n",
    "We now fit the global random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$ and over periods $t=1,...,T$. This is done across all products at once (global training). Then, for each $\\tau=0,...,4$ and over periods $t=1,...,T$, we generate for each product (SKU) $k=1,...,M$ the weights given the test feature $x_{k,t}$. This is done *jointly* across products (by using $x_{t}=(x_{1,t},...,x_{M,t})^{\\top}$) for computational efficiency - the weights for each individual product can be extracted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bac397b5-6cd1-4b43-82c6-92cc48459b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e4020-8b86-47b7-92b7-55861cb27f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Initialize\n",
    "    samples, weightfunctions, weightfunctions_times, weights, weights_times = {}, {}, {}, {}, {}\n",
    "        \n",
    "    # For each period t=1,...,T\n",
    "    for t in ts:\n",
    "\n",
    "        # Adjust look-ahead tau to account for end of horizon\n",
    "        tau_ = min(tau,T-t)\n",
    "        \n",
    "        # Status\n",
    "        print('#### Look-ahead tau='+str(tau)+' (tau\\'='+str(tau_)+'), period t='+str(t)+'...')\n",
    "        start_time = dt.datetime.now().replace(microsecond=0)\n",
    "                \n",
    "        # Generate samples, fit weight functions, and generate weights (based on tuned weights model)\n",
    "        weightsmodel = RandomForestWeightsModel()\n",
    "        weightsmodel.load_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau_)+'.joblib')\n",
    "        res = weightsmodel.training_and_sampling(ID_Data, X_Data_z, Y_Data_z, tau=tau_, timePeriods=ID_Data.sale_yearweek,\n",
    "                                                 timePeriodsTestStart=test_start+t-1, model_params=model_params)\n",
    "        samples[t], weightfunctions[t], weightfunctions_times[t], weights[t], weights_times[t] = res\n",
    "        \n",
    "        # Status\n",
    "        print('...done in', dt.datetime.now().replace(microsecond=0) - start_time)    \n",
    "        \n",
    "    # Save\n",
    "    _ = joblib.dump(samples, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')  \n",
    "    _ = joblib.dump(weightfunctions, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weightfunctions_times, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_times_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weights, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weights_times, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_times_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849aa53f-c4c4-4c07-938a-2eff4099ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ae19d125-6a77-4139-8bcf-5f9bedf9e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=0\n",
    "t=1\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params = {\n",
    "    'n_estimators': 500,\n",
    "    'min_samples_split': 20,\n",
    "    'min_samples_leaf': 10,\n",
    "    'max_features': 3,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'max_samples': 0.90\n",
    "}\n",
    "\n",
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "X = np.array(X_Data_z.loc[ID_Data.sale_yearweek < 114][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']])\n",
    "y = np.array(Y_Data_z.loc[ID_Data.sale_yearweek < 114]['Y0'])\n",
    "wm_z = weightsmodel.fit(X, y)\n",
    "\n",
    "\n",
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "X = np.array(X_Data.loc[ID_Data.sale_yearweek < 114][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']])\n",
    "y = np.array(Y_Data.loc[ID_Data.sale_yearweek < 114]['Y0'])\n",
    "wm = weightsmodel.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "605c3ef8-9397-45e1-bdd6-8c38b1d49bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24990111206413546"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm_z.weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0819bf81-5690-4438-811b-98c53e1d0fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082096963101499"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm.weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ae9b98a7-b898-443c-b4ef-5cccbdb338c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Also when applied for one SKU, i.e., a local model with scaled values, the performance is very bad ...\n",
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "X = np.array(X_Data_z.loc[(ID_Data.sale_yearweek < 114) & (ID_Data.SKU == 5)][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']])\n",
    "y = np.array(Y_Data_z.loc[(ID_Data.sale_yearweek < 114) & (ID_Data.SKU == 5)]['Y0'])\n",
    "wm_z = weightsmodel.fit(X, y)\n",
    "y_pred_z = weightsmodel.weightsmodel.predict(X)\n",
    "mse_z = np.mean((scaler_fitted[5].inverse_transform(y_pred_z.reshape(-1,1)).flatten() - scaler_fitted[5].inverse_transform(y.reshape(-1,1)).flatten())**2)\n",
    "\n",
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "X = np.array(X_Data.loc[(ID_Data.sale_yearweek < 114) & (ID_Data.SKU == 5)][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']])\n",
    "y = np.array(Y_Data.loc[(ID_Data.sale_yearweek < 114) & (ID_Data.SKU == 5)]['Y0'])\n",
    "wm = weightsmodel.fit(X, y)\n",
    "y_pred = weightsmodel.weightsmodel.predict(X)\n",
    "mse = np.mean((y_pred-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "746b05d7-b414-40df-91b8-13ac7b4b4c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18939986330527836"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm_z.weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b37aea4c-82e7-4422-aa70-2ce95d39ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7209491946930937"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm.weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a1bd2c52-27f9-437c-9472-60da37dbdc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372970.9181515987, 147819.47787353792)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_z, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9004dd2c-9421-46a3-8de5-2f8027913069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test if scaaling is wrong by performin manual scaling\n",
    "X = X_Data.loc[(ID_Data.sale_yearweek < 114) & (ID_Data.SKU == 5)][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']]\n",
    "y = Y_Data.loc[(ID_Data.sale_yearweek < 114) & (ID_Data.SKU == 5)]['Y0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7bfb0124-0063-48fa-9272-a8a913dbd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2f9833d4-9992-4e02-8a54-6e67c73e02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_scaler_X = scaler.fit(np.array(pd.DataFrame({'y1': y, 'y2': y, 'y3': y, 'y4': y, 'y5': y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2661cc14-813c-43cf-ace7-d77b2b28e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_z = fitted_scaler_X.transform(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "763e4ba5-73e7-4045-8c52-bf52991aca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_scaler_y = scaler.fit(np.array(y).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "0871c9ef-9a75-48b5-a36d-18097b997601",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_z = fitted_scaler_y.transform(np.array(y).reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b3b861e5-c792-4b18-a940-90da6a212d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "wm_z = weightsmodel.fit(X_z, y_z)\n",
    "y_pred_z = weightsmodel.weightsmodel.predict(X_z)\n",
    "mse_z = np.mean((fitted_scaler_y.inverse_transform(y_pred_z.reshape(-1,1)).flatten() - np.array(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "33967717-3541-4af4-9315-f62425cd863e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148001.45258743598"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f88e0d14-bc13-415b-99b4-147bf3e34d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "wm = weightsmodel.fit(np.array(X), np.array(y))\n",
    "y_pred = weightsmodel.weightsmodel.predict(np.array(X))\n",
    "mse = np.mean((y_pred-np.array(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "250d7446-5166-4c60-a1e8-ccd82c944b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147819.4778735379"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6def6410-2432-4572-8e48-8586ae517c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7202067750194299"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm_z.weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f0289ba2-2630-4114-8aeb-f07fadc6b058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7209491946930937"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm.weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "aaf8c2d9-05d1-4109-b4ec-6134124069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... this seems to work now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "84f5c241-4c63-43f4-abf1-a47e42907aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's try for a global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4aa7231f-251f-4b6c-a179-4f2496994f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test if scaling is wrong by performin manual scaling\n",
    "X_Data_z = pd.DataFrame()\n",
    "Y_Data_z = pd.DataFrame()\n",
    "fitted_scalers_X = {}\n",
    "fitted_scalers_y = {}\n",
    "for SKU in SKUs:\n",
    "    X = X_Data.loc[(ID_Data.sale_yearweek<114) & (ID_Data.SKU==SKU)][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']]\n",
    "    y = Y_Data.loc[(ID_Data.sale_yearweek<114) & (ID_Data.SKU==SKU)]['Y0']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    fitted_scaler_X = scaler.fit(np.array(pd.DataFrame({'y1': y, 'y2': y, 'y3': y, 'y4': y, 'y5': y})))\n",
    "    X_z = fitted_scaler_X.transform(np.array(X))\n",
    "    fitted_scalers_X[SKU] = copy.deepcopy(fitted_scaler_X)\n",
    "    X_Data_z = pd.concat([X_Data_z, pd.DataFrame(X_z, columns = ['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5'])])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    fitted_scaler_y = scaler.fit(np.array(y).reshape(-1,1))\n",
    "    y_z = fitted_scaler_y.transform(np.array(y).reshape(-1,1))\n",
    "    fitted_scalers_y[SKU] = copy.deepcopy(fitted_scaler_y)\n",
    "    Y_Data_z = pd.concat([Y_Data_z, pd.DataFrame(y_z, columns = ['Y0'])])   \n",
    "    \n",
    "X_Data_z = X_Data_z.reset_index(drop=True)\n",
    "Y_Data_z = Y_Data_z.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7bbfd386-9fee-4792-9ba8-835aac7ac797",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "wm_z = weightsmodel.fit(np.array(X_Data_z), np.array(Y_Data_z).flatten())\n",
    "y_pred_z = weightsmodel.weightsmodel.predict(np.array(X_Data_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c2f277aa-4865-4837-bc5e-5a686edc6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel = RandomForestWeightsModel(model_params=model_params, hyper_params=hyper_params)\n",
    "wm = weightsmodel.fit(np.array(X_Data.loc[ID_Data.sale_yearweek<114][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']]), \n",
    "                      np.array(Y_Data.loc[ID_Data.sale_yearweek<114]['Y0']).flatten())\n",
    "y_pred = weightsmodel.weightsmodel.predict(np.array(X_Data.loc[ID_Data.sale_yearweek<114][['SKU_wqs_lag1','SKU_wqs_lag2','SKU_wqs_lag3','SKU_wqs_lag4','SKU_wqs_lag5']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082b4b2-e13a-42e2-a6ff-5e7799768fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Data_pred_z = pd.DataFrame()\n",
    "for SKU in SKUs:\n",
    "    y_pred_zz = fitted_scalers_y[SKU].inverse_transform(np.array(y_pred_z[ID_Data.loc[ID_Data.sale_yearweek<114].SKU==SKU]).reshape(-1,1))\n",
    "    Y_Data_pred_z = pd.concat([Y_Data_pred_z, pd.DataFrame(y_pred_zz, columns = ['Y0'])])   \n",
    "    \n",
    "Y_Data_pred_z = Y_Data_pred_z.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf6c0b-0276-44c2-a801-23563180f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Data_pred = pd.DataFrame(y_pred, columns = ['Y0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "387caa89-bd0e-4a3a-8f53-3fec568e279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Data_true = pd.DataFrame()\n",
    "for SKU in SKUs:\n",
    "    Y_Data_true = pd.concat([Y_Data_true, pd.DataFrame(Y_Data.loc[(ID_Data.sale_yearweek<114) & (ID_Data.SKU==SKU)]['Y0'], columns = ['Y0'])])    \n",
    "    \n",
    "Y_Data_true = Y_Data_true.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "acfcd602-eb55-41f7-89fb-e2b5bda364b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_z = []\n",
    "mse = []\n",
    "for SKU in SKUs:\n",
    "    y_pred_zz_SKU = fitted_scalers_y[SKU].inverse_transform(np.array(y_pred_z[ID_Data.loc[ID_Data.sale_yearweek<114].SKU==SKU]).reshape(-1,1)).flatten()\n",
    "    y_pred_SKU = y_pred[ID_Data.loc[ID_Data.sale_yearweek<114].SKU==SKU].flatten()\n",
    "    y_true_SKU = np.array(Y_Data.loc[(ID_Data.sale_yearweek<114) & (ID_Data.SKU==SKU)]['Y0']).flatten()\n",
    "    \n",
    "    mse_z += [np.mean((y_pred_zz_SKU-y_true_SKU)**2)]\n",
    "    mse += [np.mean((y_pred_SKU-y_true_SKU)**2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "2d848702-e197-4cf5-90f8-2cb6ec0f7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'SKU': SKUs, 'mse_z': mse_z, 'mse': mse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "56dae2bf-b57b-4df1-91a2-bf851852adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['diffs'] = results.mse_z / results.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b85e3364-6956-4a4b-bc90-cfbf7039d5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>mse_z</th>\n",
       "      <th>mse</th>\n",
       "      <th>diffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>460.000000</td>\n",
       "      <td>4.600000e+02</td>\n",
       "      <td>4.600000e+02</td>\n",
       "      <td>460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>230.500000</td>\n",
       "      <td>2.058256e+07</td>\n",
       "      <td>1.823364e+06</td>\n",
       "      <td>5.838786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>132.934821</td>\n",
       "      <td>2.764980e+08</td>\n",
       "      <td>2.047753e+07</td>\n",
       "      <td>6.716479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.823240e-02</td>\n",
       "      <td>7.274181e-01</td>\n",
       "      <td>0.083095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>115.750000</td>\n",
       "      <td>7.406972e+01</td>\n",
       "      <td>2.794942e+01</td>\n",
       "      <td>1.808917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>230.500000</td>\n",
       "      <td>1.267851e+03</td>\n",
       "      <td>3.775251e+02</td>\n",
       "      <td>3.132171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>345.250000</td>\n",
       "      <td>4.813500e+04</td>\n",
       "      <td>6.458503e+03</td>\n",
       "      <td>7.291291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>460.000000</td>\n",
       "      <td>5.567112e+09</td>\n",
       "      <td>3.401364e+08</td>\n",
       "      <td>46.511272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SKU         mse_z           mse       diffs\n",
       "count  460.000000  4.600000e+02  4.600000e+02  460.000000\n",
       "mean   230.500000  2.058256e+07  1.823364e+06    5.838786\n",
       "std    132.934821  2.764980e+08  2.047753e+07    6.716479\n",
       "min      1.000000  6.823240e-02  7.274181e-01    0.083095\n",
       "25%    115.750000  7.406972e+01  2.794942e+01    1.808917\n",
       "50%    230.500000  1.267851e+03  3.775251e+02    3.132171\n",
       "75%    345.250000  4.813500e+04  6.458503e+03    7.291291\n",
       "max    460.000000  5.567112e+09  3.401364e+08   46.511272"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "88088457-0793-4fca-8ad6-be0d5d37c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seems that on global level, still MSE is really poor ... At least with these limited features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01ca9d-35ae-43f3-95a1-c4eb312207a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c03bce-102d-483d-88c5-0bdfca1cb25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a9ff1-6498-40d8-b3a4-7d772f0f22af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e2d91-c0ec-4aa2-b898-75767ffeb262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230f9bb-3c3a-45cf-a4be-5c4166fbb18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da62d23-0a6d-4d8c-82be-a15d850d15b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76c597-09f7-4693-a6d6-833915d7365a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ddb69-6b92-4331-a35d-b113d6d848b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec993a-4e7d-4276-b4be-2ae2a1c12aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ac88c-8ba3-480c-be2c-580f0a688274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b8b2f-4ec5-4cee-900f-167d25c5749f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f9c1b-c0b0-4108-b836-146ad89fe984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c614b-b36f-487e-8f7f-7946c3630371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53a357-a03d-4319-8114-98d3fc905fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09999dc-ae45-449c-a7c4-b358694d10f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b7fe8-3b63-4a12-a941-4db5b03179ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ef3d4-5620-4717-a881-2c1203b85e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c338f4c-b3c6-46e8-ba72-a9371abeaaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e6c0e-b267-44dd-beed-613b70a28ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ef925-ed7c-401f-bb83-0924cfd1810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd8c16-0ba2-42f1-bb23-248114443d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightfunctions = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0eccf-7304-4b3d-87a0-3fa0f69b60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_ = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_samples_tau'+str(tau)+'.joblib') \n",
    "weightfunctions_ = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_weightfunctions_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55eb10f-23d0-4b2f-b891-42a5a819a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_z = weightfunctions[1].weightsmodel.predict(samples[1]['X_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060aa36-69f2-42db-acf7-7c520327ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = weightfunctions_[1].weightsmodel.predict(samples_[1]['X_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735823c-e00b-4c2d-8920-e47a495385b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_zz = []\n",
    "for SKU in SKUs:\n",
    "    y_pred_zz += [scaler_fitted[SKU].inverse_transform(y_pred_z[samples_[1]['id_test'].SKU == SKU].reshape(1,1)).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddc659-3cc4-4cfa-9173-b2092e8e1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'SKU': samples_[1]['id_test'].SKU, 'y': samples_[1]['y_test'], 'y_pred_z': np.array(y_pred_zz), 'y_pred': y_pred}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5a830-5103-4ae8-b643-229dcfd82152",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac88e0-2417-4f86-a727-a6e4aab268a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['error_z'] = abs(res.y-res.y_pred_z)\n",
    "res['error'] =  abs(res.y-res.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d270b2f-3300-46b8-89e1-79060cb78b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['delta'] = res.error_z / res.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a565a3-7871-4d54-bf56-981796620b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43782887-9f5a-4b9d-9b1b-d974cd61cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.665111e+04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14f016-46f0-4d53-9172-4b1f10befa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(weightfunctions_[1].weightsmodel.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b642e-e55e-4ed3-9815-f4cd473d04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature': X_Data.columns, 'importance': weightfunctions[1].weightsmodel.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934f26c-be51-4d1d-9314-2cf3a44afae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.sort_values(['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddeb2f-522b-4cdb-8916-0bc31550504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(Y_Data.Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f9920-df4f-4992-8175-f31091463cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7955afa-76fb-4fb2-b967-a5f0080c7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKU=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1815ac-6eeb-46b6-b5be-984e5c0e8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.hist(Y_Data_z.Y0.loc[ID_Data.SKU==SKU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc03ff-b0b6-4ef9-a56f-c6c1e1235fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.hist(Y_Data.Y0.loc[ID_Data.SKU==SKU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b51c2-abcc-4636-bad4-0e3c6f7c0ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8605e5-b1eb-44d8-84fe-73cac7e2cdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd50f0-e0a3-44c3-9dcb-c13ee6e0f820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c87b27-c315-40d3-91de-1a7518bafefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe7537-09b8-44c9-9dd7-f1eb5b28bf56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cedc8-44ef-4fc4-8dee-29fe2dbc60a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305334e6-89d5-4110-99b9-3690bb5330b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c7c5d-b80c-4151-bb5f-dcef71f1c8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242dd19-f8a4-4614-8cab-8013a722ae83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326363b4-d449-4730-860e-7c6e306efbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_global'\n",
    "weightsmodel_name = 'rfwm_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d542b5-c741-4efb-bdd9-83e29dfab55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677604d3-ec9c-4237-bc44-0b778d33281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=1\n",
    "SKU=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1c35a-db61-47b4-b3e9-f957cf3c624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessing()\n",
    "\n",
    "\n",
    "timePeriods = ID_Data.sale_yearweek\n",
    "timePeriodsTestStart = test_start+t-1\n",
    "\n",
    "        \n",
    "# Select and reshape training and test data\n",
    "args = {'train': (timePeriods < timePeriodsTestStart), 'test': (timePeriods == timePeriodsTestStart), \n",
    "        'timePeriods': timePeriods[(timePeriods < timePeriodsTestStart)], 'maxTimePeriod': timePeriodsTestStart-1, 'tau': tau}\n",
    "\n",
    "id_train, id_test = pp.train_test_split(ID_Data, **args)\n",
    "X_train, X_test = pp.train_test_split(X_Data, **args, to_array=True)\n",
    "y_train, y_test = pp.train_test_split(Y_Data, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3061c0-53eb-4625-a86b-9ec9c8790952",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[id_train.SKU==SKU].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a87a43-cd11-42a2-a551-947605b8907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Data[(ID_Data.SKU==SKU) & (ID_Data.sale_yearweek < test_start+t-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d413f6-d1e2-4792-8a35-793f9c36ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_Data[(ID_Data.SKU==SKU) & (ID_Data.sale_yearweek < test_start+t-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61795d1-d646-4fbe-b985-5c0b8b37806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train.loc[id_train.SKU==SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a262a-65b2-485b-b1fa-9fe9966cf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Data.SKU_dqs_min_lag1.loc[(ID_Data.SKU==SKU) & (ID_Data.sale_yearweek==99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33de58-8bc3-4bd2-82a5-231fe09459d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[id_train.SKU==SKU,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbe7a3-9312-40b0-9604-b952d271967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234ac19-b855-4c9b-8d64-d0b00e734c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel_name = 'rfwm_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bdbb4-9f82-4eb9-8581-ea0641a98a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel_global = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_tau'+str(tau)+'.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610557d-86e9-47ea-9a69-1712490a7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel_global[1].weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9cd8d-d576-46e8-8e4c-c0afd541a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsmodel_name = 'rfwm_global_z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776f4a4-0dcf-41f8-9262-0eccd221cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel_global_z = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_tau'+str(tau)+'.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06ba42-e874-4c95-9f5d-8a4e87737630",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel_global_z[1].weightsmodel.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fc221-83ce-46b5-8533-12adca150985",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfmodel_global[1].weightsmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ca02d-4cca-4d3e-9d1e-8731d8b441d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acedce-8b54-4cf9-92b1-0413e868565a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b299fe-6fb3-40d2-8e27-7798cf7b6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6f61a-360b-4375-8532-1db7aba25afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessing()\n",
    "\n",
    "\n",
    "timePeriods = ID_Data.sale_yearweek\n",
    "timePeriodsTestStart = test_start+t-1\n",
    "\n",
    "        \n",
    "# Select and reshape training and test data\n",
    "args = {'train': (timePeriods < timePeriodsTestStart), 'test': (timePeriods == timePeriodsTestStart), \n",
    "        'timePeriods': timePeriods[(timePeriods < timePeriodsTestStart)], 'maxTimePeriod': timePeriodsTestStart-1, 'tau': tau}\n",
    "\n",
    "id_train, id_test = pp.train_test_split(ID_Data, **args)\n",
    "X_train_z, X_test_z = pp.train_test_split(X_Data_z, **args, to_array=True)\n",
    "y_train_z, y_test_z = pp.train_test_split(Y_Data_z, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e42b94-6628-40ed-8320-ead2795a8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_z = rfmodel_global_z[1].weightsmodel.predict(X_train_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d9117-7583-47b6-9643-1179b793e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_z.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e4cb3-c253-40c4-82bc-83b1c54d92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55feffc-5a53-4b81-ba2b-8889fb81a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaler_fitted[SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349cdc0-827d-4c19-95f6-0aa8edd6ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_zz = scaler.inverse_transform(y_pred_z.reshape(y_pred_z.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0c430-f459-411d-8d50-a5645a7a4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_pred_zz.flatten().shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339818e-2291-423d-a456-76b5454fb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "mse_z = []\n",
    "\n",
    "for SKU in SKUs:\n",
    "    sel = id_train.SKU == SKU\n",
    "    mse += [np.mean((y_train[sel] - y_pred[sel])**2)]\n",
    "    mse_z += [np.mean((y_train[sel] - y_pred_zz[sel])**2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fb245-ccbe-4a26-9760-e9a3247ba6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'SKU': np.array(SKUs), 'mse': np.array(mse), 'mse_z': np.array(mse_z)})\n",
    "res['diff'] = res.mse_z / res.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f0b-331e-4fcc-be16-388a126df644",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398efb9c-8e98-47ee-80c8-32733215ddd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a7589-508a-4a6e-ad1b-a4780b898181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e833e6-816c-40eb-a2d6-be03bf66f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mv0 = joblib.load(PATH_WEIGHTSMODEL+'/rf_mv_t1.joblib')  \n",
    "rf_mv1 = joblib.load(PATH_WEIGHTSMODEL+'/rf_mv_t2.joblib')  \n",
    "rf_mv2 = joblib.load(PATH_WEIGHTSMODEL+'/rf_mv_t3.joblib')  \n",
    "rf_mv3 = joblib.load(PATH_WEIGHTSMODEL+'/rf_mv_t4.joblib')  \n",
    "rf_mv4 = joblib.load(PATH_WEIGHTSMODEL+'/rf_mv_t5.joblib')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2d4e4-175a-4700-881c-972acfcd33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel_global[1].weightsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07670942-3d8a-4128-9706-989b2d9cdcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mv0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df6c43-20e8-43b1-8867-3bed83fbf5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969d240-2943-480e-8c45-7acdc3237dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b4228-b281-45c1-9678-529b0058130c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ca03f-3e22-4b02-b3ad-c7a70f7b65af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b042d897-c9e2-4a40-8e89-f973b47098dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2b3b4-8c8a-40a9-9851-5ca7980971cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10579d85-d221-45a8-89e7-c28e9b339927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns3.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb1dd4-e9c4-46c3-9f75-8fd96e995ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_Data_Columns = X_Data_Columns.loc[X_Data_Columns.Global == 'YES']\n",
    "X_Data = X_Data[X_Data_Columns.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f4a6d-24f8-43a7-ade4-72873d7df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "X_Data_train = X_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "\n",
    "# Prepare\n",
    "vars_to_scale_names = X_Data_Columns.loc[X_Data_Columns.Scale == 'YES', 'Feature'].values\n",
    "vars_to_scale = np.array(X_Data[vars_to_scale_names])\n",
    "vars_to_scale_with = np.array(Y_Data_train[['Y','Y','Y','Y','Y']])\n",
    "\n",
    "vars_to_scale_groups = np.array(ID_Data.SKU)\n",
    "vars_to_scale_with_groups = np.array(ID_Data_train.SKU)\n",
    "\n",
    "# Fit and transform\n",
    "scaler = MinMaxScaler()\n",
    "vars_scaled, scaler_fitted = pp.scale_variables(vars_to_scale, vars_to_scale_with, vars_to_scale_groups, vars_to_scale_with_groups, scaler)\n",
    "\n",
    "# Reshape to original data\n",
    "vars_scaled = pd.concat([pd.DataFrame(vars_scaled[i], columns=vars_to_scale_names) for i in vars_scaled]).reset_index(drop=True)\n",
    "X_Data_z = copy.deepcopy(X_Data)\n",
    "for col in vars_scaled.columns:\n",
    "    X_Data_z[col] = vars_scaled[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68c924-2e6c-4f93-a703-97299cecb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training data\n",
    "ID_Data_train = ID_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "Y_Data_train = Y_Data.loc[ID_Data.sale_yearweek < test_start]\n",
    "\n",
    "# Prepare\n",
    "vars_to_scale = np.array(Y_Data)\n",
    "vars_to_scale_with = np.array(Y_Data_train)\n",
    "\n",
    "vars_to_scale_groups = np.array(ID_Data.SKU)\n",
    "vars_to_scale_with_groups = np.array(ID_Data_train.SKU)\n",
    "\n",
    "# Fit and transform\n",
    "scaler = MinMaxScaler()\n",
    "vars_scaled, scaler_fitted = pp.scale_variables(vars_to_scale, vars_to_scale_with, vars_to_scale_groups, vars_to_scale_with_groups, scaler)\n",
    "\n",
    "# Reshape to original data\n",
    "Y_Data_z = pd.concat([pd.DataFrame(vars_scaled[i], columns=['Y']) for i in vars_scaled]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc1ded-490f-4ae7-9eee-a2b3d3bd833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-period demand vectors\n",
    "data = pd.concat([ID_Data, Y_Data_z], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data_z = pd.DataFrame(Y)\n",
    "\n",
    "data = pd.concat([ID_Data, Y_Data], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "    \n",
    "Y_Data = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdc007-8d4e-481a-90cd-6350ff9cb499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f51f2-332c-4703-9961-c20789ae0114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59bb5e-422f-44b9-aea3-15fc9a7bb616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71808c31-5b75-4aa9-a33e-23a1d17ad21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518cc220-0297-4556-9731-7279d8792f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5171d1-c0b1-41df-b112-d896d372728c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcbede-d98d-4c57-8ec9-cdb1c6874790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f52a99-effc-4222-93dc-94a8dc4df617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2d493-351f-4dae-ab5d-5221329c4979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479b54d-c328-476e-97f0-48a75615b035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000579-6586-4aff-a6db-20a7e3f9da6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122f6c2-5b68-482a-a73f-d93d6981e1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b0a23-22be-4075-8d00-a5e007b6c235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1f46f-05f9-4a49-8cce-85f4134240e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessing()\n",
    "\n",
    "### Function to generate samples, fit weight functions, and generate weights\n",
    "def training_and_sampling(ID_Data, X_Data, Y_Data, tau, timePeriods, timePeriodsTestStart):\n",
    "\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    \"\"\"           \n",
    "    # Select and reshape training and test data\n",
    "    args = {'train': (timePeriods < timePeriodsTestStart), 'test': (timePeriods == timePeriodsTestStart), \n",
    "            'timePeriods': timePeriods[(timePeriods < timePeriodsTestStart)], 'maxTimePeriod': timePeriodsTestStart-1, 'tau': tau}\n",
    "\n",
    "    id_train, id_test = pp.train_test_split(ID_Data, **args)\n",
    "    X_train, X_test = pp.train_test_split(X_Data, **args, to_array=True)\n",
    "    y_train, y_test = pp.train_test_split(Y_Data, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)\n",
    "\n",
    "    # Store samples of historical demands\n",
    "    samples = {'y_train': y_train, 'y_test': y_test, \n",
    "               'X_train': X_train, 'X_test': X_test, \n",
    "               'id_train': id_train, 'id_test': id_test}\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763526d-0eee-4708-8b20-a61042732b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17dc04-c056-42c0-a3b3-792dbe44cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b92cf7-903d-4e52-93e7-95074fce8dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5023b0-a20f-4290-b9a1-c194bf160271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3e8a4-f810-4c96-b537-16f6fdbadc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc01413-6191-4cac-ab2f-e0405b7f8ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167964dc-6321-4cc8-b80d-cd759ab77987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965d2d2-d8f8-496f-8fab-09a191dbedd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d357c25-392d-4686-be11-9fd8a580f007",
   "metadata": {},
   "source": [
    "## Local Training and Sampling\n",
    "\n",
    "The two local models (using 'Local Training and Sampling') are **Rolling Horizon Local Weighted SAA (wSAA)**, and **Rolling Horizon Local Robust Weighted SAA (wSAA-R)**, which is the analogous model with robust extension.\n",
    "\n",
    "Given product $k$, period $t$, and look-ahead $\\tau$, both models apply Weighted SAA over the 'local' distribution $\\{w_{k,t,\\tau}^{\\,i}(x_{k,t}^{\\,i}),(d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$, with weight functions $w_{k,t,\\tau}(\\,\\cdot\\,)$ trained on data $S_{k,t,\\tau}^{\\,\\text{Local}}=\\{(x_{k,t}^{\\,i},d_{k,t}^{\\,i},...,d_{k,t+\\tau}^{\\,i})\\}_{i=1}^{N_{k,t,\\tau}}$ for each product $k=1,...,M$ separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1793967-c0a7-49f9-afbe-3c2edc82a733",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "We first load and pre-process the data. This includes reshaping demand time series into $(\\tau+1)$-periods rolling look-ahead horizon sequences.\n",
    "\n",
    "- **ID_Data** (pd.DataFrame) stores identifiers (in particular the product (SKU) identifier and the timePeriod (sale_yearweek) identifier)\n",
    "- **X_Data** (pd.DataFrame) is the 'feature matrix', i.e., each row is a feature vector $x_{j,n}$ where n is the number of training observations (rows) in the data\n",
    "- **Y_Data** (pd.DataFrame) is the demand data $d_{j,n}$ (a times series per product)\n",
    "- **X_Data_Columns** (pd.DataFrame) provides 'selectors' for local vs. global feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cde388-4e04-432e-a25f-c7c86ad4a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local'\n",
    "weightsmodel_name = 'rfwm_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c111f-7ccb-4ddb-aa74-41e9c671bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ID_Data = pd.read_csv(PATH_DATA+'/ID_Data.csv')\n",
    "X_Data = pd.read_csv(PATH_DATA+'/X_Data.csv')\n",
    "X_Data_Columns = pd.read_csv(PATH_DATA+'/X_Data_Columns.csv')\n",
    "Y_Data = pd.read_csv(PATH_DATA+'/Y_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a27ac-fb86-4586-97e1-04fb33da5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_Data_Columns = X_Data_Columns.loc[X_Data_Columns.Local == 'YES']\n",
    "X_Data = X_Data[X_Data_Columns.Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badda3cf-e799-4d46-9fcf-af3ec6dac26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by SKU and sale_yearweek for preprocessing\n",
    "data = pd.concat([ID_Data, X_Data, Y_Data], axis=1).sort_values(by=['SKU', 'sale_yearweek']).reset_index(drop=True)\n",
    "\n",
    "ID_Data = data[ID_Data.columns]\n",
    "X_Data = data[X_Data.columns]\n",
    "Y_Data = data[Y_Data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2044675-b66d-42d9-ab1d-24cd56e81dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-period demand vectors\n",
    "data = pd.concat([ID_Data, Y_Data], axis=1)\n",
    "Y = {}\n",
    "for tau in taus:\n",
    "    Y['Y'+str(tau)] = data.groupby(['SKU']).shift(-tau)['Y']\n",
    "Y_Data = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be213da-406b-4e42-8699-2f5973a9c767",
   "metadata": {},
   "source": [
    "### Weights model\n",
    "\n",
    "The weights model - and thus the data used, weight functions, and weights per sample - are the same for the two local models **wSAA** and **wSAA-R**. First, we tune the hyper parameters of the random forest weights model for each given look-ahead $\\tau$ (as for each look-ahead $\\tau$ we have a different response for the multi-output random forest regressor) and for each product (SKU) $k=1,...,M$ separately. Second, we fit all weight functions (for each look-ahead $\\tau=0,...,4$ and over periods $t=1,...,T$) for each product (SKU) $k=1,...,M$ separately and generate all weights (for each look-ahead $\\tau=0,...,4$, over periods $t=1,...,T$, and for each product (SKU) $k=1,...,M$ separatey)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0442f-cdaa-47ad-a691-4b2aac018b30",
   "metadata": {},
   "source": [
    "#### Tune weights model\n",
    "\n",
    "To tune the hyper parameters of the local random forest weights model for each product (SKU) $k=1,...,M$, we use 3-fold rolling timeseries cross-validation on the training data and perform random search with 100 iterations over the specified hyper parameter search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653420-33b5-4569-8397-2da1590e53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to tune random forest weights kernels\n",
    "model_params = {\n",
    "    'oob_score': True,\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [x for x in range(2, 20, 1)],  \n",
    "    'min_samples_leaf': [x for x in range(2, 10, 1)],  \n",
    "    'max_features': [x for x in range(8, 256, 8)],   \n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "}    \n",
    "\n",
    "\n",
    "tuning_params = {     \n",
    "    'n_iter': 100,\n",
    "    'scoring': {'MSE': 'neg_mean_squared_error'},\n",
    "    'return_train_score': True,\n",
    "    'refit': 'MSE',\n",
    "    'random_state': 12345,\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}    \n",
    "\n",
    "random_search = True\n",
    "print_status = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381d48c-ccee-44df-9335-7595ae2c48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    cv_results = {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for SKU in SKUs:\n",
    "\n",
    "        # Initialize preprocessing module\n",
    "        pp = PreProcessing()\n",
    "\n",
    "        # Select and reshape training and test data\n",
    "        args = {'train': (ID_Data.SKU == SKU) & (ID_Data.sale_yearweek < test_start), \n",
    "                'timePeriods': ID_Data.loc[(ID_Data.SKU == SKU) & (ID_Data.sale_yearweek < test_start)].sale_yearweek, \n",
    "                'maxTimePeriod': test_start-1, 'tau': tau}\n",
    "\n",
    "        id_train = pp.train_test_split(ID_Data, **args)\n",
    "        X_train = pp.train_test_split(X_Data, **args, to_array=True)\n",
    "        y_train = pp.train_test_split(Y_Data, **args, rolling_horizon=[l for l in range(0,tau+1)], to_array=True)\n",
    "\n",
    "        # Initialize\n",
    "        weightsmodel = RandomForestWeightsModel(model_params)\n",
    "\n",
    "        # CV search\n",
    "        cv_folds = pp.split_timeseries_cv(n_splits=3, timePeriods=id_train.sale_yearweek)\n",
    "        cv_results[SKU] = weightsmodel.tune(X_train, y_train, cv_folds, hyper_params_grid, \n",
    "                                            tuning_params, random_search, print_status)\n",
    "        \n",
    "        # Status\n",
    "        print('SKU '+str(SKU)+' of '+str(len(SKUs))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "\n",
    "    # Save\n",
    "    _ = joblib.dump(cv_results, PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1c7a-4494-4191-858a-231dc1ba9fd0",
   "metadata": {},
   "source": [
    "#### Fit weight functions and generate weights\n",
    "\n",
    "We now fit a local random forest weights model (i.e., the weight functions) for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately (local training). Then, for each $\\tau=0,...,4$, period $t=1,...,T$, and product (SKU) $k=1,...,M$ separately, we generate the weights given the test feature $x_{k,t}$. This is done *separately* for each product (SKU) $k=1,...,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410060e-ec5e-4f5d-8da2-7a81da076ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_params = {\n",
    "    'n_jobs': 32,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print_status = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806721b-465b-4edf-b312-a88245d9b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Status\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    start_time = dt.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    # Initialize\n",
    "    samples, weightfunctions, weightfunctions_times, weights, weights_times = {}, {}, {}, {}, {}\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    for SKU in SKUs:\n",
    "        \n",
    "        # Initialize\n",
    "        samples[SKU], weightfunctions[SKU], weightfunctions_times[SKU], weights[SKU], weights_times[SKU] = {}, {}, {}, {}, {}\n",
    "        \n",
    "        # For each period t=1,...,T\n",
    "        for t in ts:\n",
    "        \n",
    "            # Adjust look-ahead tau to account for end of horizon\n",
    "            tau_ = min(tau,T-t)\n",
    "\n",
    "            # Generate samples, fit weight functions, and generate weights (based on tuned weights model)\n",
    "            weightsmodel = RandomForestWeightsModel()\n",
    "            weightsmodel.load_cv_result(path=PATH_WEIGHTSMODEL+'/'+weightsmodel_cv_name+'_tau'+str(tau_)+'.joblib', SKU=SKU)\n",
    "            res = weightsmodel.training_and_sampling(ID_Data.loc[ID_Data.SKU==SKU], X_Data.loc[ID_Data.SKU==SKU], Y_Data.loc[ID_Data.SKU==SKU], \n",
    "                                                     tau=tau_, timePeriods=ID_Data.loc[ID_Data.SKU==SKU].sale_yearweek, \n",
    "                                                     timePeriodsTestStart=test_start+t-1, model_params=model_params)\n",
    "            samples[SKU][t], weightfunctions[SKU][t], weightfunctions_times[SKU][t], weights[SKU][t], weights_times[SKU][t] = res\n",
    "\n",
    "        # Status\n",
    "        print('SKU '+str(SKU)+' of '+str(len(SKUs))+' in', dt.datetime.now().replace(microsecond=0) - start_time, end='\\r', flush=True)\n",
    "        \n",
    "    # Save\n",
    "    _ = joblib.dump(samples, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')  \n",
    "    _ = joblib.dump(weightfunctions, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weightfunctions_times, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weightfunctions_times_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weights, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')    \n",
    "    _ = joblib.dump(weights_times, PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_times_tau'+str(tau)+'.joblib')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9986-993c-4b9a-96e5-22444655a258",
   "metadata": {},
   "source": [
    "# Rolling Horizon Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8e28f-3903-447f-b538-a0b937c26688",
   "metadata": {},
   "source": [
    "The code below runs an experiment for all given products (SKUs) $k=1,...,M$ over a test planning horizon $t=1,...,T$ with $T=13$ for three different cost parameter settings $\\{K, u, h, b\\}$ that vary the critical ratio ($CR=\\frac{b}{b+h}$) of holding and backlogging yielding\n",
    "- $CR=0.50$: $\\{K=100, u=0.5, h=1, b=1\\}$\n",
    "- $CR=0.75$: $\\{K=100, u=0.5, h=1, b=3\\}$\n",
    "- $CR=0.90$: $\\{K=100, u=0.5, h=1, b=9\\}$\n",
    "\n",
    "We run experiments for different choices of the look-ahead $\\tau=0,...,4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d2455-3d8d-48f6-bee1-386b8cd57dbf",
   "metadata": {},
   "source": [
    "## Experiment functions\n",
    "\n",
    "We first define several functions for experiments over different choices for the look-ahead $\\tau=0,...,4$, cost parameter settings $\\{K,u,h,b\\}$, products (SKUs) $k=1,...,M$, and periods $t=1,...,T$ of the planning horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f00834-9b52-4df8-8889-2bacbf8ad5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to extract global historical demand samples, weights, and actuals for each product and test period\n",
    "def prep_samples_and_weights(samples, weights=None, e=None, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    SKUs = kwargs.get('SKUs', range(1,460+1))\n",
    "    ts = kwargs.get('ts', range(1,13+1))\n",
    "    \n",
    "    # Local\n",
    "    if len(samples) == len(SKUs):\n",
    "        \n",
    "        # Samples\n",
    "        samples_ = {}\n",
    "        for SKU in SKUs:\n",
    "            samples_[SKU] = {}\n",
    "            for t in ts:\n",
    "                samples_[SKU][t] = samples[SKU][t]['y_train']\n",
    "                \n",
    "        # Actuals\n",
    "        actuals_ = {}\n",
    "        for SKU in SKUs:\n",
    "            actuals_[SKU] = {}\n",
    "            for t in ts:\n",
    "                actuals_[SKU][t] = samples[SKU][t]['y_test'].flatten()\n",
    "                \n",
    "        # Weights   \n",
    "        if not weights is None:\n",
    "            weights_ = {}\n",
    "            for SKU in SKUs:\n",
    "                weights_[SKU] = {}\n",
    "                for t in ts:\n",
    "                    weights_[SKU][t] = weights[SKU][t].flatten()\n",
    "                    \n",
    "        # Epsilons\n",
    "        if not e is None:\n",
    "            epsilons_ = {}\n",
    "            for SKU in SKUs:\n",
    "                epsilons_[SKU] = {}\n",
    "                for t in ts:\n",
    "                    epsilons_[SKU][t] = e*np.std(samples[SKU][t]['y_train'], axis=0).flatten()[0]\n",
    "                 \n",
    "    # Global\n",
    "    else:\n",
    "        \n",
    "        # Samples\n",
    "        samples_ = {}\n",
    "        for t in ts:\n",
    "            samples_[t] = samples[t]['y_train']\n",
    "\n",
    "        # Actuals\n",
    "        actuals_ = {}\n",
    "        for SKU in SKUs:\n",
    "            actuals_[SKU] = {}\n",
    "            for t in ts:\n",
    "                actuals_[SKU][t] = samples[t]['y_test'][samples[t]['id_test'].SKU==SKU].flatten()\n",
    "\n",
    "        # Weights   \n",
    "        if not weights is None:\n",
    "            weights_ = {}\n",
    "            for SKU in SKUs:\n",
    "                weights_[SKU] = {}\n",
    "                for t in ts:\n",
    "                    weights_[SKU][t] = weights[t][samples[t]['id_test'].SKU==SKU].flatten()\n",
    "                    \n",
    "        # Epsilons\n",
    "        if not e is None:\n",
    "            epsilons_ = {}\n",
    "            for SKU in SKUs:\n",
    "                epsilons_[SKU] = {}\n",
    "                for t in ts:\n",
    "                    epsilons_[SKU][t] = e*np.std(samples[t]['y_train'][samples[t]['id_train'].SKU == SKU], axis=0).flatten()[0]\n",
    "\n",
    "    # Return\n",
    "    if not weights is None:\n",
    "        if not e is None:\n",
    "            return samples_, actuals_, weights_, epsilons_\n",
    "        else:\n",
    "            return samples_, actuals_, weights_\n",
    "    else:\n",
    "        if not e is None:\n",
    "            return samples_, actuals_, epsilons_\n",
    "        else:\n",
    "            return samples_, actuals_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a21211e6-0f49-4a89-9b95-6f99fe23a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to run an experiment over a list of given cost parameter settings and the specified model\n",
    "def run_experiment(wsaamodel, cost_params, samples, actuals, weights=None, epsilons=None, print_progress=False,\n",
    "                   path_to_save=None, name_to_save=None, return_results=True, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Raise error if cost_params is not a list of dict(s)\n",
    "    if not type(cost_params)==list:\n",
    "        raise ValueError('Argument cost_params has to be a list of at least one dict with keys {K, u, h, b}')  \n",
    "            \n",
    "    # Timer\n",
    "    st_exec, st_cpu = time.time(), time.process_time()\n",
    "\n",
    "    # Status\n",
    "    if print_progress and 'SKU' in kwargs: print('SKU:', kwargs['SKU'])\n",
    "    \n",
    "    # Initialize\n",
    "    ropt, results = RollingHorizonOptimization(), pd.DataFrame()\n",
    "\n",
    "    # For each cost param setting\n",
    "    for cost_params_ in cost_params:\n",
    "\n",
    "        # Print progress\n",
    "        if print_progress: print('...cost param setting:', cost_params_)\n",
    "\n",
    "        # Apply (Weighted) SAA  model\n",
    "        wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "        result = ropt.run(wsaamodel, samples, actuals, weights, epsilons)\n",
    "\n",
    "        # Store results\n",
    "        meta = pd.DataFrame({'CR': cost_params_['CR'], **kwargs}, index=list(range(len(samples))))\n",
    "        results = pd.concat([results, pd.concat([meta, result], axis=1)], axis=0)\n",
    "\n",
    "    # Save result as csv file\n",
    "    if not path_to_save is None and not name_to_save is None:\n",
    "        results.to_csv(path_or_buf=(path_to_save+'/'+name_to_save+'_SKU'+str(kwargs.get('SKU', None))+\n",
    "                                    '_tau'+str(kwargs.get('tau', None))+'.csv'), sep=',', index=False)\n",
    "\n",
    "    # Timer\n",
    "    exec_time_sec, cpu_time_sec = time.time() - st_exec, time.process_time() - st_cpu\n",
    "    \n",
    "    # Status\n",
    "    if print_progress: print('>>>> Done:', str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    # Return  \n",
    "    return results if return_results else {'SKU': kwargs.get('SKU', None), 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a30c8ba-f50f-4693-9fa0-01c5ec8818ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to run an experiment over a list of given cost parameter settings and the specified model\n",
    "def run_experiment(wsaamodel, cost_params, actuals, samples=None, weights=None, epsilons=None, print_progress=False,\n",
    "                   path_to_save=None, name_to_save=None, return_results=True, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Raise error if cost_params is not a list of dict(s)\n",
    "    if not type(cost_params)==list:\n",
    "        raise ValueError('Argument cost_params has to be a list of at least one dict with keys {K, u, h, b}')  \n",
    "    \n",
    "    # Timer\n",
    "    st_exec, st_cpu = time.time(), time.process_time()\n",
    "\n",
    "    # Status\n",
    "    if print_progress and 'SKU' in kwargs: print('SKU:', kwargs['SKU'])\n",
    "    \n",
    "    # Initialize\n",
    "    ropt, results = RollingHorizonOptimization(), pd.DataFrame()\n",
    "\n",
    "    # For each cost param setting\n",
    "    for cost_params_ in cost_params:\n",
    "\n",
    "        # Print progress\n",
    "        if print_progress: print('...cost param setting:', cost_params_)\n",
    "        \n",
    "        # Check if samples provided\n",
    "        if not samples is None:\n",
    "            \n",
    "            # Apply (Weighted) SAA  model\n",
    "            wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "            result = ropt.run(wsaamodel, samples, actuals, weights, epsilons, q_ub=False)\n",
    "             \n",
    "            # Get T\n",
    "            T = len(samples)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Apply ex-post clairvoyant model\n",
    "            wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "            result = ropt.run_expost(wsaamodel, actuals)\n",
    "            \n",
    "            # Get T\n",
    "            T = actuals.shape[1]\n",
    "\n",
    "        # Store results\n",
    "        meta = pd.DataFrame({'CR': cost_params_['CR'], **kwargs}, index=list(range(T)))\n",
    "        results = pd.concat([results, pd.concat([meta, result], axis=1)], axis=0)\n",
    "\n",
    "    # Save result as csv file\n",
    "    if not path_to_save is None and not name_to_save is None:\n",
    "        results.to_csv(path_or_buf=(path_to_save+'/'+name_to_save+'_SKU'+str(kwargs.get('SKU', None))+\n",
    "                                    '_tau'+str(kwargs.get('tau', None))+'.csv'), sep=',', index=False)\n",
    "\n",
    "    # Timer\n",
    "    exec_time_sec, cpu_time_sec = time.time() - st_exec, time.process_time() - st_cpu\n",
    "    \n",
    "    # Status\n",
    "    if print_progress: print('>>>> Done:', str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    # Return  \n",
    "    return results if return_results else {'SKU': kwargs.get('SKU', None), 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70d3c27-3e96-4f24-8e37-98a4299bf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to run ex-post clairvoyant experiment over a list of given cost parameter settings\n",
    "def run_experiment_expost(wsaamodel, cost_params, actuals, print_progress=False, path_to_save=None, name_to_save=None, return_results=True, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Raise error if cost_params is not a list of dict(s)\n",
    "    if not type(cost_params)==list:\n",
    "        raise ValueError('Argument cost_params has to be a list of at least one dict with keys {K, u, h, b}')  \n",
    "            \n",
    "    # Timer\n",
    "    st_exec, st_cpu = time.time(), time.process_time()\n",
    "\n",
    "    # Status\n",
    "    if print_progress and 'SKU' in kwargs: print('SKU:', kwargs['SKU'])\n",
    "    \n",
    "    # Initialize\n",
    "    ropt, results = RollingHorizonOptimization(), pd.DataFrame()\n",
    "\n",
    "    # For each cost param setting\n",
    "    for cost_params_ in cost_params:\n",
    "\n",
    "        # Print progress\n",
    "        if print_progress: print('...cost param setting:', cost_params_)\n",
    "\n",
    "        # Apply (Weighted) SAA  model\n",
    "        wsaamodel.set_params(**{**kwargs, **cost_params_})\n",
    "        result = ropt.run_expost(wsaamodel, actuals, q_ub=False)\n",
    "\n",
    "        # Store results\n",
    "        meta = pd.DataFrame({'CR': cost_params_['CR'], **kwargs}, index=list(range(actuals.shape[1])))\n",
    "        results = pd.concat([results, pd.concat([meta, result], axis=1)], axis=0)\n",
    "\n",
    "    # Save result as csv file\n",
    "    if not path_to_save is None and not name_to_save is None:\n",
    "        results.to_csv(path_or_buf=(path_to_save+'/'+name_to_save+'_SKU'+str(kwargs.get('SKU', None))+'.csv'), sep=',', index=False)\n",
    "\n",
    "    # Timer\n",
    "    exec_time_sec, cpu_time_sec = time.time() - st_exec, time.process_time() - st_cpu\n",
    "    \n",
    "    # Status\n",
    "    if print_progress: print('>>>> Done:', str(np.around(exec_time_sec/60,1)), 'minutes')\n",
    "\n",
    "    # Return  \n",
    "    return results if return_results else {'SKU': kwargs.get('SKU', None), 'exec_time_sec': exec_time_sec, 'cpu_time_sec': cpu_time_sec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047a2caa-f987-43d1-9670-6f8ef2dcf863",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to aggregate all results\n",
    "def aggregateResults(taus, SKUs, path_to_save, name_to_save):\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        for SKU in SKUs:\n",
    "            file_name = path_to_save+'/'+name_to_save+'_SKU'+str(SKU)+'_tau'+str(tau)+'.csv'\n",
    "            # Check if results exist   \n",
    "            if os.path.exists(file_name):\n",
    "                results = pd.concat([results, pd.read_csv(file_name)])\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1938aee-dc52-4442-a1dc-34aee4f4db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Context manager (Credits: 'https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution')\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b5a1c-c253-41b3-be40-866598943f7e",
   "metadata": {},
   "source": [
    "## (a) Rolling Horizon Global Weighted SAA (GwSAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f258a74-8ab3-4a35-9243-5abaaa22b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_global_z'\n",
    "weightsmodel_name = 'rfwm_global_z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ea232ec-5727-401f-a60c-37e82bad7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/GwSAA_NEW_z',\n",
    "    'name_to_save': 'GwSAA_NEW_z',\n",
    "    'print_progress': True,\n",
    "    'return_results': True\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5248f73a-8f0f-4b52-9d43-8f15a0aa0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    \n",
    "    # Prepare data\n",
    "    samples_z = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "    weights_z = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "    samples_z, _, weights_z = prep_samples_and_weights(samples_z, weights_z, SKUs=SKUs, ts=ts)\n",
    "\n",
    "    # Prepare data - unscaled\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_samples_tau'+str(tau)+'.joblib')\n",
    "    _, actuals = prep_samples_and_weights(samples, SKUs=SKUs, ts=ts)\n",
    "\n",
    "    samples_ = {}\n",
    "    for SKU in SKUs:\n",
    "        samples_[SKU] = {}\n",
    "        for t in ts:\n",
    "            d = samples_z[t].reshape(-1,1) if samples_z[t].ndim == 1 else copy.deepcopy(samples_z[t])\n",
    "            d_z = np.around(scaler_fitted[SKU].inverse_transform(d))\n",
    "            samples_[SKU][t] = d_z.flatten() if samples_z[t].ndim == 1 else copy.deepcopy(d_z)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples_[SKU], weights=weights_z[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a2396-c864-46cb-a662-f8b0539bdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b852a-9fe4-4ae7-a5f9-4170757ccc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d8fec-c851-4844-89f8-fdde75028a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c370c3-1d42-4b19-8dc9-c3c29750b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17081c-0230-489e-a940-9c3824fcfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    \n",
    "    # Prepare data\n",
    "    samples_z = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "    samples_z, actuals_z = prep_samples_and_weights(samples_z, SKUs=SKUs, ts=ts)\n",
    "\n",
    "    # Prepare data - unscaled\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_samples_tau'+str(tau)+'.joblib')\n",
    "    samples, actuals = prep_samples_and_weights(samples, SKUs=SKUs, ts=ts)\n",
    "\n",
    "    samples_ = {}\n",
    "    for SKU in SKUs:\n",
    "        samples_[SKU] = {}\n",
    "        for t in ts:\n",
    "            d = samples_z[t].reshape(-1,1) if samples_z[t].ndim == 1 else copy.deepcopy(samples_z[t])\n",
    "            d_z = np.around(scaler_fitted[SKU].inverse_transform(d))\n",
    "            samples_[SKU][t] = d_z.flatten() if samples_z[t].ndim == 1 else copy.deepcopy(d_z)\n",
    "            \n",
    "            weightfunctions_z[t].weightsmodel.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bbe230a8-44db-4817-b4d4-d8f8ce6e4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_z = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_z_samples_tau'+str(tau)+'.joblib')\n",
    "weightfunctions_z = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_z_weightfunctions_tau'+str(tau)+'.joblib')\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_samples_tau'+str(tau)+'.joblib')\n",
    "weightfunctions = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_global_weightfunctions_tau'+str(tau)+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "29b5d89f-f7dc-46e7-87e2-1417c5a6dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ec187ae1-ede3-4d7d-9d68-55c602f9056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_z = weightfunctions_z[t].weightsmodel.predict(samples_z[t]['X_test'])\n",
    "y_pred = weightfunctions[t].weightsmodel.predict(samples[t]['X_test'])\n",
    "y = copy.deepcopy(samples[t]['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "53ba293f-0f7d-4253-bbb4-e1f1147346d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_z = weightfunctions_z[t].weightsmodel.predict(samples_z[t]['X_test'])\n",
    "y_pred = weightfunctions[t].weightsmodel.predict(samples[t]['X_test'])\n",
    "y = copy.deepcopy(samples[t]['y_test'])\n",
    "\n",
    "y_pred_zz = copy.deepcopy(y_pred_z)*(-999)\n",
    "error_zz = copy.deepcopy(y_pred_z)*0\n",
    "error = copy.deepcopy(y_pred)*0\n",
    "for SKU in SKUs:\n",
    "    y_pred_zz[samples_z[t]['id_test'].SKU==SKU] = (\n",
    "        scaler_fitted[SKU].inverse_transform(y_pred_z[samples_z[t]['id_test'].SKU==SKU].reshape(-1,1)).flatten()\n",
    "    )\n",
    "    \n",
    "    error_zz[samples_z[t]['id_test'].SKU==SKU] = y_pred_zz[samples_z[t]['id_test'].SKU==SKU] - y[samples_z[t]['id_test'].SKU==SKU]\n",
    "    error[samples[t]['id_test'].SKU==SKU] = y_pred[samples[t]['id_test'].SKU==SKU] - y[samples[t]['id_test'].SKU==SKU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "450bbd66-8d32-4b19-bd2d-c7d88fdf3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d02a9776-a736-4800-bfd7-57f89fda4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for t in ts:\n",
    "    \n",
    "    y_pred_z = weightfunctions_z[t].weightsmodel.predict(samples_z[t]['X_test'])\n",
    "    y_pred = weightfunctions[t].weightsmodel.predict(samples[t]['X_test'])\n",
    "    y = copy.deepcopy(samples[t]['y_test'])\n",
    "\n",
    "    y_pred_zz = copy.deepcopy(y_pred_z)*(-999)\n",
    "    error_zz = copy.deepcopy(y_pred_z)*0\n",
    "    error = copy.deepcopy(y_pred)*0\n",
    "    for SKU in SKUs:\n",
    "        y_pred_zz[samples_z[t]['id_test'].SKU==SKU] = (\n",
    "            scaler_fitted[SKU].inverse_transform(y_pred_z[samples_z[t]['id_test'].SKU==SKU].reshape(-1,1)).flatten()\n",
    "        )\n",
    "\n",
    "        error_zz[samples_z[t]['id_test'].SKU==SKU] = y_pred_zz[samples_z[t]['id_test'].SKU==SKU] - y[samples_z[t]['id_test'].SKU==SKU]\n",
    "        error[samples[t]['id_test'].SKU==SKU] = y_pred[samples[t]['id_test'].SKU==SKU] - y[samples[t]['id_test'].SKU==SKU]\n",
    "        \n",
    "        \n",
    "    res[t] = {'error_zz': error_zz, 'error': error, 'y_pred_zz': y_pred_zz, 'y_pred': y_pred, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1d72afc0-2afc-4731-98c6-f3447c554c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zz = {}\n",
    "test = {}\n",
    "for SKU in SKUs:\n",
    "    test_zz[SKU] = np.array([])\n",
    "    test[SKU] = np.array([])\n",
    "    for t in ts:\n",
    "        test_zz[SKU] = np.append(test_zz[SKU], res[t]['error_zz'][samples_z[t]['id_test'].SKU==SKU].flatten())\n",
    "        test[SKU] = np.append(test[SKU], res[t]['error'][samples[t]['id_test'].SKU==SKU].flatten())\n",
    "    test_zz[SKU] = np.mean(test_zz[SKU]**2)\n",
    "    test[SKU] = np.mean(test[SKU]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ba6121c6-d624-497b-8ecf-9ce21abb6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'mse_zz': test_zz, 'mse': test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "09835147-c0d7-411b-969f-f148e5afa1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['diffs'] = results.mse_zz / results.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7efc4387-c9f5-4e6e-9548-8f4c6f53bede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse_zz</th>\n",
       "      <th>mse</th>\n",
       "      <th>diffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.600000e+02</td>\n",
       "      <td>4.600000e+02</td>\n",
       "      <td>4.600000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.581221e+08</td>\n",
       "      <td>1.046270e+07</td>\n",
       "      <td>1.057154e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.661554e+09</td>\n",
       "      <td>1.911136e+08</td>\n",
       "      <td>1.768808e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.937060e-02</td>\n",
       "      <td>5.364695e-02</td>\n",
       "      <td>3.467115e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.402088e+02</td>\n",
       "      <td>2.758633e+01</td>\n",
       "      <td>1.750013e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.360877e+04</td>\n",
       "      <td>6.951627e+02</td>\n",
       "      <td>1.517072e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.646551e+05</td>\n",
       "      <td>1.020189e+04</td>\n",
       "      <td>3.863075e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.115637e+10</td>\n",
       "      <td>4.091510e+09</td>\n",
       "      <td>3.770655e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse_zz           mse         diffs\n",
       "count  4.600000e+02  4.600000e+02  4.600000e+02\n",
       "mean   1.581221e+08  1.046270e+07  1.057154e+06\n",
       "std    1.661554e+09  1.911136e+08  1.768808e+07\n",
       "min    1.937060e-02  5.364695e-02  3.467115e-05\n",
       "25%    9.402088e+02  2.758633e+01  1.750013e+00\n",
       "50%    2.360877e+04  6.951627e+02  1.517072e+01\n",
       "75%    4.646551e+05  1.020189e+04  3.863075e+02\n",
       "max    3.115637e+10  4.091510e+09  3.770655e+08"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "75456055-48ef-487c-925a-808404d35535",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=range(1,13+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a287505f-69c3-446b-a7cc-621666c96273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.17072"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.517072e+01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cb1ac9e8-3145-4094-a77d-f68d95469be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1258.]), array([8.]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_fitted[1].data_max_, scaler_fitted[33].data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4bad1-d219-449c-a2b8-c1dc053d56f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b2494-0f05-4718-859e-26beddcec0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b352fcd-e02e-4def-bd0d-bb6ed4f2bb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4038fe-44af-4680-8514-4b2a78e1dfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f990e6a-3366-45d5-8ccf-bdc88876c5fc",
   "metadata": {},
   "source": [
    "## (b) Rolling Horizon Global Robust Weighted SAA (GwSAA-R)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a233e-c6ef-4377-b8c6-a18de2a2666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_global'\n",
    "weightsmodel_name = 'rfwm_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc3496-a393-41bc-bedd-069d126aad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "\n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/GwSAAR_NEW',\n",
    "    'name_to_save_prefix': 'GwSAAR_NEW',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df713a-ded7-4e2a-ad4a-3b0604bb666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "    # CHECK IF DUE TO INFINITY NORM I SHOULD CHOOSE EPSILON SIGNIFICANTLY HIGHER!\n",
    "        # Check convergence...\n",
    "        # Maybe use rather:\n",
    "            # e = 1,32,64,128,256,512,1024 ?\n",
    "            \n",
    "    # delete uptter bound in all functions - this was wrong or did at least not help... also for Weighted SAA remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff7d42-139d-4b72-85bf-0fbed2a1448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in [1,3,6,9,12]:\n",
    "    \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('Look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        samples, actuals, weights, epsilons = prep_samples_and_weights(samples, weights, e=e, SKUs=SKUs, ts=ts)\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=RobustWeightedSAA2(), \n",
    "                                                                     samples=samples, weights=weights[SKU], epsilons=epsilons[SKU],\n",
    "                                                                     actuals=actuals[SKU], e=e, **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2697b8c-238c-4c2d-8e4a-349605cc65e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537da801-62e8-4a39-be41-1b7b3c8f802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a0bca-cc34-4050-9b0f-d65de82d5641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223f0c5-47f9-496d-897c-13ed69e7536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24de5bb-a882-46bb-be04-6b8609a8d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3b32b-f50b-4a25-8ac4-f250bb755dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7c739-3536-4f0c-be6b-a2b48663ac45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c876ec2-021d-4faf-a276-3f5dfb61a59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18608c56-5725-4a91-9b2a-a11bc8af240d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851c186-aead-4cd6-832f-d40da419f85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528959ec-63f1-47d8-a9d9-8a787c97b079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177c4b64-a7ff-4b64-8e95-ddcd851acca5",
   "metadata": {},
   "source": [
    "## (c) Rolling Horizon Local Weighted SAA (wSAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c6aec-1d0e-4064-8b35-45ef1c641401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local'\n",
    "weightsmodel_name = 'rfwm_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86228ea-cc1f-447a-a3f1-ff95887b38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/wSAA_NEW',\n",
    "    'name_to_save': 'wSAA_NEW',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e54514-7c62-4791-a34a-5f404b7dc986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "    weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "    samples, actuals, weights = prep_samples_and_weights(samples, weights, SKUs=SKUs, ts=ts)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples[SKU], weights=weights[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d2e6d-cbc8-4156-9f6d-8db11f19889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c38723-58d1-4eed-90bc-72d34ff86cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "results = aggregateResults(taus, SKUs, experiment_params['path_to_save'], experiment_params['name_to_save'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36151426-7056-4fd2-89b9-5f0c5490f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregate results\n",
    "file_name = experiment_params['path_to_save']+'/'+experiment_params['name_to_save']+'_results.csv'\n",
    "results.to_csv(file_name, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3ae23-fecf-40c4-ab82-42d6f1d04b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df1134-01fb-4015-86a6-8137d3fa9e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490dc65-a248-49fc-a291-c70530ec4501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35ca8e-b483-4cc4-97e1-30f3266dcfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f4f6d-e9c1-4981-b1c4-6536b0d81d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a3ff83-6de0-4fad-8b99-a50eb9f3dd11",
   "metadata": {},
   "source": [
    "## (d) Rolling Horizon Local Robust Weighted SAA (wSAA-R)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189defe-7b4b-433f-8bab-150e4c5250d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights model names\n",
    "weightsmodel_cv_name = 'cv_rfwm_local'\n",
    "weightsmodel_name = 'rfwm_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83e339-7ec6-4a30-8315-3bdfd9616e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/wSAAR_NEW',\n",
    "    'name_to_save_prefix': 'wSAAR_NEW',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b6457-abd5-4d2d-ac5d-4d74f6da08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each uncertainty set specification\n",
    "for e in [1,3,6,9,12]:\n",
    "    \n",
    "    # Print:\n",
    "    print('Uncertainty set parameter e='+str(e)+'...')\n",
    "        \n",
    "    # Update params\n",
    "    experiment_params['name_to_save'] = experiment_params['name_to_save_prefix']+'_e'+str(e).replace('.', '')\n",
    "    \n",
    "    # Set path\n",
    "    if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "    # For each look-ahead tau=0,...,4\n",
    "    for tau in taus:\n",
    "\n",
    "        # Print:\n",
    "        print('...look-ahead tau='+str(tau)+'...')\n",
    "\n",
    "        # Prepare data\n",
    "        samples = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_samples_tau'+str(tau)+'.joblib')\n",
    "        weights = joblib.load(PATH_WEIGHTSMODEL+'/'+weightsmodel_name+'_weights_tau'+str(tau)+'.joblib')\n",
    "\n",
    "        samples, actuals, weights, epsilons = prep_samples_and_weights(samples, weights, e=e, SKUs=SKUs, ts=ts)\n",
    "\n",
    "        # For each product (SKU) k=1,...,M\n",
    "        with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "            resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=RobustWeightedSAA2(), \n",
    "                                                                     samples=samples[SKU], weights=weights[SKU], epsilons=epsilons[SKU],\n",
    "                                                                     actuals=actuals[SKU], e=e, **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370dea6-a487-4db2-bade-1f91a6c9975a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f42885-78fc-45fa-93d5-d6ea85789961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602f846-a0d0-4b1f-b046-13dd0feaeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356cc89-3dc4-45e1-ab7f-7bcf8c64274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d980c8-d0f7-4f19-bd17-65100814f1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650cf8f6-4fd6-4258-9498-759b49e4cbc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (e) Baseline model: Rolling Horizon Local Weighted SAA (SAA)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36241d0-6ac1-4fdf-9ead-18bc7d921e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/SAA_NEW',\n",
    "    'name_to_save': 'SAA_NEW',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1c7f7-e1ae-4a9b-b474-19d1d5b10420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each look-ahead tau=0,...,4\n",
    "for tau in taus:\n",
    "    \n",
    "    # Print:\n",
    "    print('Look-ahead tau='+str(tau)+'...')\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_local_samples_tau'+str(tau)+'.joblib')\n",
    "    \n",
    "    samples, actuals = prep_samples_and_weights(samples, SKUs=SKUs, ts=ts)\n",
    "    \n",
    "    # For each product (SKU) k=1,...,M\n",
    "    with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "        resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(tau=tau, SKU=SKU, wsaamodel=WeightedSAA(), \n",
    "                                                                 samples=samples[SKU], actuals=actuals[SKU], \n",
    "                                                                 **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1bc2c-df43-4c08-bd5a-08c91d99b970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554967d-2acd-45c3-8e68-10cd23695bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccb147-9122-4e3f-8435-8c73275d2836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280f481-c0e0-4e07-86bc-c7e2b45fa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b4a736-d792-468a-86a1-8c95976293ed",
   "metadata": {},
   "source": [
    "## (f) Ex-post optimal model with deterministic demand\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696c509-5415-49f7-a94e-b69279244785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment paramaters\n",
    "experiment_params = {\n",
    "            \n",
    "    # Cost param settings\n",
    "    'cost_params': cost_params,\n",
    "    \n",
    "    # Gurobi meta params\n",
    "    'LogToConsole': 0, \n",
    "    'Threads': 1, \n",
    "    'NonConvex': 2, \n",
    "    'PSDTol': 1e-3, # 0.1%\n",
    "    'MIPGap': 1e-3, # 0.1%\n",
    "    'NumericFocus': 0, \n",
    "    'obj_improvement': 1e-3, # 0.1%\n",
    "    'obj_timeout_sec': 3*60, # 3 min\n",
    "    'obj_timeout_max_sec': 10*60, # 10 min\n",
    "\n",
    "    # Program meta params\n",
    "    'path_to_save': PATH_RESULTS+'/ExPost_NEW',\n",
    "    'name_to_save': 'ExPost_NEW',\n",
    "    'print_progress': False,\n",
    "    'return_results': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc93a75-85e3-428a-ac69-a824756d68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "samples = joblib.load(PATH_WEIGHTSMODEL+'/rfwm_local_samples_tau'+str(0)+'.joblib')\n",
    "actuals = {}\n",
    "for SKU in SKUs:\n",
    "    d = []\n",
    "    for t in ts:\n",
    "        d = d + [samples[SKU][t]['y_test'].item()]\n",
    "    actuals[SKU] = np.array(d).reshape(1,len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67975180-c482-416c-9a03-18a97c42677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "if not os.path.exists(experiment_params['path_to_save']): os.mkdir(experiment_params['path_to_save'])\n",
    "\n",
    "# For each product (SKU) k=1,...,M\n",
    "with tqdm_joblib(tqdm(desc='Progress', total=len(SKUs))) as progress_bar:\n",
    "    resultslog = Parallel(n_jobs=32)(delayed(run_experiment)(SKU=SKU, wsaamodel=WeightedSAA(), actuals=actuals[SKU], **experiment_params) for SKU in SKUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b089dda-c51e-4ad8-9246-acbf1729136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d10eb6-98c8-49b9-8335-eac24a9e37c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ec46e-1bd4-4b0e-8a18-0c0caf340dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49b4a4-51a2-4f0f-93ce-9c0e10d84277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08ce33-6328-49c9-a5ec-0a00cf7c0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9daf8-66fe-46ff-bbe5-daa14d847001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a8a38-1144-49c5-84e8-e4a11c6bebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiPeriodInv",
   "language": "python",
   "name": "multiperiodinv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
